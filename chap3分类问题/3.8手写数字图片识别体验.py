# å½“å‰ç‰ˆæœ¬ ï¼š python3.7.11
# å¼€å‘æ—¶é—´ ï¼š 2021/9/13 20:04
"""
æœ¬èŠ‚æˆ‘ä»¬å°†åœ¨æœªä»‹ç» TensorFlow çš„æƒ…å†µä¸‹ï¼Œå…ˆå¸¦å¤§å®¶ä½“éªŒä¸€ä¸‹ç¥ç»ç½‘ç»œçš„ä¹è¶£ã€‚æœ¬èŠ‚çš„ä¸»è¦ç›®çš„å¹¶ä¸æ˜¯æ•™ä¼šæ¯ä¸ªç»†èŠ‚ï¼Œ
è€Œæ˜¯è®©è¯»è€…å¯¹ç¥ç»ç½‘ç»œç®—æ³•æœ‰å…¨é¢ã€ç›´è§‚çš„æ„Ÿå—ï¼Œä¸ºæ¥ä¸‹æ¥ä»‹ç» TensorFlow åŸºç¡€å’Œæ·±åº¦å­¦ä¹ ç†è®ºæ‰“ä¸‹åŸºç¡€ã€‚è®©æˆ‘ä»¬å¼€å§‹ä½“éªŒç¥å¥‡çš„å›¾ç‰‡è¯†åˆ«ç®—æ³•å§ï¼
3.8.1ç½‘ç»œæ­å»º
å¯¹äºç¬¬ä¸€å±‚æ¨¡å‹æ¥è¯´ï¼Œå®ƒæ¥å—çš„è¾“å…¥ğ’™ âˆˆ ğ‘…^784ï¼Œè¾“å‡ºğ’‰1 âˆˆ ğ‘…^256è®¾è®¡ä¸ºé•¿åº¦ä¸º 256 çš„å‘
é‡ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼åœ°ç¼–å†™ğ’‰1 = ReLU(ğ‘¾1ğ’™ + ğ’ƒ1)çš„è®¡ç®—é€»è¾‘ï¼Œåœ¨ TensorFlow ä¸­é€šè¿‡ä¸€è¡Œä»£ç å³å¯å®ç°ï¼š
# åˆ›å»ºä¸€å±‚ç½‘ç»œï¼Œè®¾ç½®è¾“å‡ºèŠ‚ç‚¹æ•°ä¸º 256ï¼Œæ¿€æ´»å‡½æ•°ç±»å‹ä¸º ReLU
layers.Dense(256, activation='relu')
ä½¿ç”¨ TensorFlow çš„ Sequential å®¹å™¨å¯ä»¥éå¸¸æ–¹ä¾¿åœ°æ­å»ºå¤šå±‚çš„ç½‘ç»œã€‚å¯¹äº 3 å±‚ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¿«é€Ÿå®Œæˆ 3 å±‚ç½‘ç»œçš„æ­å»ºã€‚
# åˆ©ç”¨ Sequential å®¹å™¨å°è£… 3 ä¸ªç½‘ç»œå±‚ï¼Œå‰ç½‘ç»œå±‚çš„è¾“å‡ºé»˜è®¤ä½œä¸ºä¸‹ä¸€å±‚çš„è¾“å…¥
model = keras.Sequential([ # 3 ä¸ªéçº¿æ€§å±‚çš„åµŒå¥—æ¨¡å‹
layers.Dense(256, activation='relu'), # éšè—å±‚ 1
layers.Dense(128, activation='relu'), # éšè—å±‚ 2
layers.Dense(10)]) # è¾“å‡ºå±‚ï¼Œè¾“å‡ºèŠ‚ç‚¹æ•°ä¸º 10
ç¬¬ 1 å±‚çš„è¾“å‡ºèŠ‚ç‚¹æ•°è®¾è®¡ä¸º 256ï¼Œç¬¬ 2 å±‚è®¾è®¡ä¸º 128ï¼Œè¾“å‡ºå±‚èŠ‚ç‚¹æ•°è®¾è®¡ä¸º 10ã€‚
ç›´æ¥è°ƒç”¨è¿™ä¸ªæ¨¡å‹å¯¹è±¡ model(x)å°±å¯ä»¥è¿”å›æ¨¡å‹æœ€åä¸€å±‚çš„è¾“å‡ºğ‘œã€‚

3.8.2æ¨¡å‹è®­ç»ƒ
æ­å»ºå®Œæˆ 3 å±‚ç¥ç»ç½‘ç»œçš„å¯¹è±¡åï¼Œç»™å®šè¾“å…¥ğ’™ï¼Œè°ƒç”¨ model(ğ’™)å¾—åˆ°æ¨¡å‹è¾“å‡ºğ‘œåï¼Œé€šè¿‡MSE æŸå¤±å‡½æ•°è®¡ç®—å½“å‰çš„è¯¯å·®â„’ï¼š
with tf.GradientTape() as tape: # æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
    # æ‰“å¹³æ“ä½œï¼Œ[b, 28, 28] => [b, 784]
    x = tf.reshape(x, (-1, 28*28))
    # Step1. å¾—åˆ°æ¨¡å‹è¾“å‡º output [b, 784] => [b, 10]
    out = model(x)
    # [b] => [b, 10]
    y_onehot = tf.one_hot(y, depth=10)
    # è®¡ç®—å·®çš„å¹³æ–¹å’Œï¼Œ[b, 10]
    loss = tf.square(out-y_onehot)
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è¯¯å·®ï¼Œ[b]
    loss = tf.reduce_sum(loss) / x.shape[0]
å†åˆ©ç”¨ TensorFlow æä¾›çš„è‡ªåŠ¨æ±‚å¯¼å‡½æ•° tape.gradient(loss, model.trainable_variables)æ±‚å‡ºæ¨¡å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ä¿¡æ¯ğœ•â„’/ğœ•ğœƒ ,
ğœƒ âˆˆ {ğ‘¾1, ğ’ƒ1,ğ‘¾2, ğ’ƒ2,ğ‘¾3, ğ’ƒ3}ã€‚
    # Step3. è®¡ç®—å‚æ•°çš„æ¢¯åº¦ w1, w2, w3, b1, b2, b3
    grads = tape.gradient(loss, model.trainable_variables)
è®¡ç®—è·å¾—çš„æ¢¯åº¦ç»“æœä½¿ç”¨ grads åˆ—è¡¨å˜é‡ä¿å­˜ã€‚å†ä½¿ç”¨ optimizers å¯¹è±¡è‡ªåŠ¨æŒ‰ç…§æ¢¯åº¦æ›´æ–°æ³•åˆ™å»æ›´æ–°æ¨¡å‹çš„å‚æ•°ğœƒã€‚
         ğœƒâ€² = ğœƒ âˆ’ ğœ‚ âˆ™ ğœ•â„’/ğœ•ğœƒ
å®ç°å¦‚ä¸‹ã€‚
     # è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
     grads = tape.gradient(loss, model.trainable_variables)
     # w' = w - lr * gradï¼Œæ›´æ–°ç½‘ç»œå‚æ•°
     optimizer.apply_gradients(zip(grads, model.trainable_variables))
å¾ªç¯è¿­ä»£å¤šæ¬¡åï¼Œå°±å¯ä»¥åˆ©ç”¨å­¦å¥½çš„æ¨¡å‹ğ‘“ğœƒå»é¢„æµ‹æœªçŸ¥çš„å›¾ç‰‡çš„ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒã€‚æ¨¡å‹çš„æµ‹è¯•éƒ¨åˆ†æš‚ä¸è®¨è®ºã€‚

æ‰‹å†™æ•°å­—å›¾ç‰‡ MNIST æ•°æ®é›†çš„è®­ç»ƒè¯¯å·®æ›²çº¿å¦‚å›¾ 3.10 æ‰€ç¤ºï¼Œç”±äº 3 å±‚çš„ç¥ç»ç½‘ç»œè¡¨è¾¾èƒ½åŠ›è¾ƒå¼ºï¼Œæ‰‹å†™æ•°å­—å›¾ç‰‡è¯†åˆ«ä»»åŠ¡ç›¸å¯¹ç®€å•ï¼Œ
è¯¯å·®å€¼å¯ä»¥è¾ƒå¿«é€Ÿã€ç¨³å®šåœ°ä¸‹é™ï¼Œå…¶ä¸­ï¼Œ æŠŠå¯¹æ•°æ®é›†çš„æ‰€æœ‰æ ·æœ¬è¿­ä»£ä¸€éå«ä½œä¸€ä¸ª Epochï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é—´éš”æ•°ä¸ª Epoch åæµ‹è¯•æ¨¡å‹
çš„å‡†ç¡®ç‡ç­‰æŒ‡æ ‡ï¼Œæ–¹ä¾¿ç›‘æ§æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚
"""