# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/13 19:52
"""
3.4存在的问题
按照上面的方案，手写数字图片识别问题真的得到了完美的解决吗？目前来看，至少存在两大问题：
❑ 线性模型
线性模型是机器学习中间最简单的数学模型之一，参数量少，计算简单，但是只能表达线性关系。即使是简单如数字图片识别任务，
它也是属于图片识别的范畴，人类目前对于复杂大脑的感知和决策的研究尚处于初步探索阶段，如果只使用一个简单的线性模型去
逼近复杂的人脑图片识别模型，很显然不能胜任。
❑ 表达能力
表达能力体现为逼近复杂分布的能力。上面的解决方案只使用了少量神经元组成的一层网络模型，相对于人脑中千亿级别的神经元互联结构，
它的表达能力明显偏弱
模型的表达能力与数据模态之间的示意图如图 3.7 所示，图中绘制了带观测误差的采样点的分布，
人为推测数据的真实分布可能是某二次抛物线模型。如图 3.7(a)所示，如果使用表达能力偏弱的线性模型去学习，很难学习到比较好的模型；
如果使用合适的多项式函数模型去学习，例如二次多项式，则能学到比较合适的模型，如图 3.7(b)所示；
但模型过于复杂，表达能力过强时，例如 10 次多项式，则很有可能会过拟合，伤害模型的泛化能力，如图 3.7(c)所示。
目前我们所采用的多神经元模型仍是线性模型，表达能力偏弱，接下来我们尝试解决这两个问题。

3.5非线性模型
既然线性模型不可行，我们可以给线性模型嵌套一个非线性函数，即可将其转换为非线性模型。
我们把这个非线性函数称为激活函数(Activation Function)，用𝜎表示：
                           o = 𝜎(𝑾𝒙 + 𝒃)
这里的𝜎代表了某个具体的非线性激活函数，如 Sigmoid 函数(图 3.8(a))、ReLU 函数(图 3.8(b))

ReLU 函数非常简单，在𝑦 = 𝑥的基础上面截去了𝑥 < 0的部分，可以直观地理解为ReLU 函数仅保留正的输入部份，清零负的输入，
具有单边抑制特性。虽然简单，ReLU 函数却有优良的非线性特性，而且梯度计算简单，训练稳定，是深度学习模型使用最广泛的
激活函数之一。我们这里通过嵌套 ReLU 函数将模型转换为非线性模型：
                           o = ReLU(𝑾𝒙 + 𝒃)
"""