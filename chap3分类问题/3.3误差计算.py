# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/13 19:45
"""
对于分类问题来说，我们的目标是最大化某个性能指标，比如准确度𝑎𝑐𝑐，但是把准确度当作损失函数去优化时，
会发现𝜕𝑎𝑐𝑐/𝜕𝜃 其实是不可导的，无法利用梯度下降算法优化网络参数𝜃。一般的做法是，设立一个平滑可导的代理目标函数，
比如优化模型的输出 与 One-hot 编码后的真实标签𝒚之间的距离(Distance)，通过优化代理目标函数得到的模型，
一般在测试性能上也能有良好的表现。因此，相对回归问题而言，分类问题的优化目标函数和评价目标函数是不一致的。
模型的训练目标是通过优化损失函数ℒ来找到最优数值解𝑾∗, 𝒃∗:

对于分类问题的误差计算来说，更常见的是采用交叉熵(Cross Entropy)损失函数，较少采用回归问题中介绍的均方差损失函数。
我们将在后续章节介绍交叉熵损失函数，这里仍然使用均方差损失函数来求解手写数字识别问题。
现在我们只需要采用梯度下降算法来优化损失函数得到𝑾, 𝒃的最优解，然后再利用求得的模型去预测未知的手写数字图片𝒙 ∈ 𝔻test。
"""