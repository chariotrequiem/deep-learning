# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/17 15:38
"""
在搭建完模型结构后，下一步就是选择合适的误差函数来计算误差。常见的误差函数有均方差、交叉熵、KL 散度、Hinge Loss 函数等，
其中均方差函数和交叉熵函数在深度学习中比较常见，均方差函数主要用于回归问题，交叉熵函数主要用于分类问题。

6.6.1均方差误差函数
均方差(Mean Squared Error，简称 MSE)误差函数把输出向量和真实向量映射到笛卡尔坐标系的两个点上，
通过计算这两个点之间的欧式距离(准确地说是欧式距离的平方)来衡量两个向量之间的差距：
                            MSE(𝒚,𝒐) ≜ 1/𝑑out ∑(𝑦𝑖 − 𝑜𝑖)2
MSE 误差函数的值总是大于等于 0，当 MSE 函数达到最小值 0 时，输出等于真实标签，此时神经网络的参数达到最优状态。
均方差误差函数广泛应用在回归问题中，实际上，分类问题中也可以应用均方差误差函数。在 TensorFlow 中，
可以通过函数方式或层方式实现 MSE 误差计算。例如，使用函数方式实现 MSE 计算，代码如下：
In [16]:
o = tf.random.normal([2,10]) # 构造网络输出
y_onehot = tf.constant([1,3]) # 构造真实值
y_onehot = tf.one_hot(y_onehot, depth=10)
loss = keras.losses.MSE(y_onehot, o) # 计算均方差
loss
Out[16]:
<tf.Tensor: id=27, shape=(2,), dtype=float32, numpy=array([0.779179 , 1.6585705], dtype=float32)>
特别要注意的是，MSE 函数返回的是每个样本的均方差，需要在样本维度上再次平均来获得平均样本的均方差，实现如下：
In [17]:
loss = tf.reduce_mean(loss) # 计算 batch 均方差
loss
Out[17]:
<tf.Tensor: id=30, shape=(), dtype=float32, numpy=1.2188747>

也可以通过层方式实现，对应的类为 keras.losses.MeanSquaredError()，和其他层的类一样，调用__call__函数即可完成前向计算，
代码如下：
In [18]: # 创建 MSE 类
criteon = keras.losses.MeanSquaredError()
loss = criteon(y_onehot,o) # 计算 batch 均方差
loss
Out[18]:
<tf.Tensor: id=54, shape=(), dtype=float32, numpy=1.2188747>


6.6.2交叉熵误差函数
在介绍交叉熵损失函数之前，我们首先来介绍信息学中熵(Entropy)的概念。1948 年，Claude Shannon 将热力学中熵的概念引入到信息论中，
用来衡量信息的不确定度。熵在信息学科中也叫信息熵，或者香农熵。熵越大，代表不确定性越大，信息量也就越大。
某个分布𝑃(𝑖)的熵定义为：
                        𝐻(𝑃) ≜ −∑𝑖 𝑃(𝑖) log2 𝑃(𝑖)
实际上，𝐻(𝑃)也可以使用其他底数的log函数计算。举个例子，对于 4 分类问题，如果某个样本的真实标签是第 4 类，
那么标签的 One-hot 编码为[0,0,0,1]，即这张图片的分类是唯一确定的，它属于第 4 类的概率𝑃(𝑦为 4|𝒙) = 1，不确定性为 0，
它的熵可以简单的计算为： −0 ∙ log2 0 − 0 ∙ log2 0 − 0 ∙ log2 0 − 1 ∙ log2 1 = 0
也就是说，对于确定的分布，熵为 0，不确定性最低。

如果它预测的概率分布是[0.1,0.1,0.1,0.7]，它的熵可以计算为： −0.1 ∙ log2 0.1 − 0.1 ∙ log2 0.1 − 0.1 ∙ log2 0.1 − 0.7 ∙ log2 0.7 ≈ 1.356
这种情况比前面确定性类别的例子的确定性要稍微大点。

考虑随机分类器，它每个类别的预测概率是均等的：[0.25,0.25,0.25,0.25]，同样的方
法，可以计算它的熵约为 2，这种情况的不确定性略大于上面一种情况。
由于𝑃(𝑖) ∈ [0,1], log2 𝑃(𝑖) ≤ 0，因此熵𝐻(𝑃)总是大于等于 0。当熵取得最小值 0 时，不确定性为 0。
分类问题的 One-hot 编码的分布就是熵为 0 的典型例子。在 TensorFlow 中间，我们可以利用 tf.math.log 来计算熵。

在介绍完熵的概念后，我们基于熵引出交叉熵(Cross Entropy)的定义：
                                𝐻(𝑝||𝑞) ≜ −∑𝑖 𝑝(𝑖) log2 𝑞(𝑖)
通过变换，交叉熵可以分解为𝑝的熵𝐻(𝑝)和𝑝与𝑞的 KL 散度(Kullback-Leibler Divergence)的和：
                                𝐻(𝑝||𝑞) = 𝐻(𝑝) + 𝐷𝐾𝐿(𝑝||𝑞)
其中 KL 定义为
                                𝐷𝐾𝐿(𝑝||𝑞) = ∑𝑝(𝑖)log (𝑝(𝑖) / 𝑞(𝑖))
KL 散度是 Solomon Kullback 和 Richard A. Leibler 在 1951 年提出的用于衡量 2 个分布之间距离的指标。
𝑝 = 𝑞时，𝐷𝐾𝐿(𝑝||𝑞)取得最小值 0，𝑝与𝑞之间的差距越大，𝐷𝐾𝐿(𝑝||𝑞)也越大。
需要注意的是，交叉熵和 KL 散度都不是对称的，即：
                                    𝐻(𝑝||𝑞) ≠ 𝐻(𝑞||𝑝)
                                    𝐷𝐾𝐿(𝑝||𝑞) ≠ 𝐷𝐾𝐿(𝑞||𝑝)
交叉熵可以很好地衡量 2 个分布之间的“距离”。特别地，当分类问题中 y 的编码分布𝑝采用 One-hot 编码𝒚时：
𝐻(𝑝) = 0，此时𝐻(𝑝||𝑞) = 𝐻(𝑝) + 𝐷𝐾𝐿(𝑝||𝑞) = 𝐷𝐾𝐿(𝑝||𝑞)
退化到真实标签分布𝒚与输出概率分布𝒐之间的 KL 散度上。
根据 KL 散度的定义，我们推导分类问题中交叉熵的计算表达式：
                                𝐻(𝑝||𝑞) = 𝐷𝐾𝐿(𝑝||𝑞) = ∑𝑦𝑗 log (𝑦j / 𝑜𝑗)
                                = 1 ∙ log 1 / 𝑜𝑖 +∑0 ∙ log (𝑜 / 0𝑗)
                                = −log𝑜𝑖
其中𝑖为 One-hot 编码中为 1 的索引号，也是当前输入的真实类别。可以看到，ℒ只与真实类别𝑖上的概率𝑜𝑖有关，对应概率𝑜𝑖越大，
𝐻(𝑝||𝑞)越小。当对应类别上的概率为 1 时，交叉熵𝐻(𝑝||𝑞)取得最小值 0，此时网络输出𝒐与真实标签𝒚完全一致，神经网络取得最优状态。

因此最小化交叉熵损失函数的过程也是最大化正确类别的预测概率的过程。从这个角度去理解交叉熵损失函数，非常地直观易懂。
"""