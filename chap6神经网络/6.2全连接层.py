# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/16 14:57
"""
感知机模型的不可导特性严重约束了它的潜力，使得它只能解决极其简单的任务。实际上，现代深度学习动辄数百万甚至上亿的参数规模，
但它的核心结构与感知机并没有多大差别。它在感知机的基础上，将不连续的阶跃激活函数换成了其它平滑连续可导的激活函数，
并通过堆叠多个网络层来增强网络的表达能力。

本节我们通过替换感知机的激活函数，同时并行堆叠多个神经元来实现多输入、多输出的网络层结构。如图 6.4 所示，
并行堆叠了 2 个神经元，即 2 个替换了激活函数的感知机，构成 3 输入节点、2 个输出节点的网络层。其中第一个输出节点的输出为：
                    𝑜1 = 𝜎(𝑤11 ∙ 𝑥1 + 𝑤21 ∙ 𝑥2 + 𝑤 1 ∙ 𝑥 + 𝑏1)
第二个输出节点的输出为：
                    𝑜2 = 𝜎(𝑤12 ∙ 𝑥1 + 𝑤22 ∙ 𝑥2 + 𝑤 2 ∙ 𝑥 + 𝑏2)
输出向量为𝒐 = [𝑜1, 𝑜2]。整个网络层可以通过矩阵关系式表达：
                    [𝑜1 𝑜2] = [𝑥1 𝑥2 𝑥3]@ [𝑤11 𝑤12
                                            𝑤21 𝑤22   + [𝑏1 𝑏2] (6-1)
                                            𝑤31 𝑤32]
即 𝑶 = 𝑿@𝑾 + 𝒃
其中输入矩阵𝑿的 shape 定义为[𝑏, 𝑑in]，𝑏为样本数量，此处只有 1 个样本参与前向运算， 𝑑in为输入节点数；
权值矩阵 W 的 shape 定义为[𝑑in, 𝑑out]，𝑑out为输出节点数，偏置向量 b 的 shape 定义为[𝑑out]。

考虑批量并行计算，例如 2 个样本，𝒙(1) = [𝑥1(1), 𝑥2(1), 𝑥 (1)]，𝒙(2) = [𝑥1(2), 𝑥2(2), 𝑥 (2)]，则可以方便地将式(6-1)推广到批量形式：
                 [𝑜1(1) 𝑜2(1)      [𝑥1(1) 𝑥2(1) 𝑥3(1)    [𝑤11 𝑤12
                  𝑜1(2) 𝑜2(2)] =    𝑥1(2) 𝑥2(2) 𝑥3(2)] @  𝑤21 𝑤22  + [𝑏1 𝑏2]
                                                          𝑤31 𝑤32]

其中输出矩阵𝑶包含了𝑏个样本的输出特征，shape 为[𝑏, 𝑑out]。由于每个输出节点与全部的输入节点相连接，
这种网络层称为全连接层(Fully-connected Layer)，或者稠密连接层(Dense Layer)，𝑾矩阵叫做全连接层的权值矩阵，𝒃向量叫做全连接层的偏置向量。


6.2.1张量实现方式
在 TensorFlow 中，要实现全连接层，只需要定义好权值张量𝑾和偏置张量𝒃，并利用TensorFlow 提供的批量矩阵相乘函数tf.matmul()即可完成网络层的计算。
例如，创建输入𝑿矩阵为𝑏 = 2个样本，每个样本的输入特征长度为𝑑in = 784，输出节点数为𝑑out = 256，故定义权值矩阵𝑾的 shape 为[784,256]，
并采用正态分布初始化𝑾；偏置向量𝒃的 shape 定义为[256]，在计算完𝑿@𝑾后相加即可，最终全连接层的输出𝑶的 shape 为[2,256]，
即 2 个样本的特征，每个特征长度为 256，代码实现如下：
In [1]: # 创建 W,b 张量
x = tf.random.normal([2,784])
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
b1 = tf.Variable(tf.zeros([256]))
o1 = tf.matmul(x,w1) + b1 # 线性变换
o1 = tf.nn.relu(o1) # 激活函数
Out[1]:
<tf.Tensor: id=31, shape=(2, 256), dtype=float32, numpy=array([[ 1.51279330e+00, 2.36286330e+00, 8.16453278e-01,
                                                                 1.80338228e+00, 4.58602428e+00, 2.54454136e+00,…
实际上，我们已经多次使用过上述代码实现网络层。


6.2.2层方式实现
全连接层本质上是矩阵的相乘和相加运算，实现并不复杂。但是作为最常用的网络层之一，
TensorFlow 中有更高层、使用更方便的层实现方式：layers.Dense(units, activation)。
通过 layer.Dense 类，只需要指定输出节点数 Units 和激活函数类型 activation 即可。需要注意的是，
输入节点数会根据第一次运算时的输入 shape 确定，同时根据输入、输出节点数自动创建并初始化权值张量𝑾和偏置张量𝒃，
因此在新建类 Dense 实例时，并不会立即创建权值张量𝑾和偏置张量𝒃，而是需要调用 build 函数或者直接进行一次前向计算，
才能完成网络参数的创建。其中 activation 参数指定当前层的激活函数，可以为常见的激活函数或自定义激活函数，
也可以指定为 None，即无激活函数。例如：
In [2]:
x = tf.random.normal([4,28*28])
from tensorflow.keras import layers # 导入层模块
# 创建全连接层，指定输出节点数和激活函数
fc = layers.Dense(512, activation=tf.nn.relu)
h1 = fc(x) # 通过 fc 类实例完成一次全连接层的计算，返回输出张量
Out[2]:
<tf.Tensor: id=72, shape=(4, 512), dtype=float32, numpy=
array([[0.63339347, 0.21663809, 0. , ..., 1.7361937 , 0.39962345,
 2.4346168 ],…

上述通过一行代码即可以创建一层全连接层 fc，并指定输出节点数为 512，输入的节点数在fc(x)计算时自动获取，
并创建内部权值张量𝑾和偏置张量𝒃。我们可以通过类内部的成员名 kernel 和 bias 来获取权值张量𝑾和偏置张量𝒃对象：
In [3]: fc.kernel # 获取 Dense 类的权值矩阵
Out[3]:
<tf.Variable 'dense_1/kernel:0' shape=(784, 512) dtype=float32, numpy=
array([[-0.04067389, 0.05240148, 0.03931375, ..., -0.01595572,
 -0.01075954, -0.06222073],
In [4]: fc.bias # 获取 Dense 类的偏置向量
Out[4]:
<tf.Variable 'dense_1/bias:0' shape=(512,) dtype=float32, numpy=
array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
可以看到，权值张量𝑾和偏置张量𝒃的 shape 和内容均符合我们的理解

在优化参数时，需要获得网络的所有待优化的张量参数列表，可以通过类的trainable_variables 来返回待优化参数列表，代码如下：
In [5]: fc.trainable_variables
Out[5]: # 返回待优化参数列表
[<tf.Variable 'dense_1/kernel:0' shape=(784, 512) dtype=float32,…,
<tf.Variable 'dense_1/bias:0' shape=(512,) dtype=float32, numpy=…]

实际上，网络层除了保存了待优化张量列表 trainable_variables，还有部分层包含了不参与梯度优化的张量，
如后续介绍的 Batch Normalization 层，可以通过non_trainable_variables 成员返回所有不需要优化的参数列表。
如果希望获得所有参数列表，可以通过类的 variables 返回所有内部张量列表，例如：
In [6]: fc.variables # 返回所有参数列表
Out[6]:
[<tf.Variable 'dense_1/kernel:0' shape=(784, 512) dtype=float32,…,
<tf.Variable 'dense_1/bias:0' shape=(512,) dtype=float32, numpy=…]
对于全连接层，内部张量都参与梯度优化，故 variables 返回的列表与 trainable_variables 相同。

利用网络层类对象进行前向计算时，只需要调用类的__call__方法即可，即写成 fc(x)方式便可，
它会自动调用类的__call__方法，在__call__方法中会自动调用 call 方法，这一设定由 TensorFlow 框架自动完成，
因此用户只需要将网络层的前向计算逻辑实现在 call 方法中即可。对于全连接层类，在 call 方法中实现𝜎(𝑿@𝑾 + 𝒃)的运算逻辑，
非常简单，最后返回全连接层的输出张量即可。
"""
