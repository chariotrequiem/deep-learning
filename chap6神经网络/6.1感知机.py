# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/16 14:48
"""
1943 年，美国神经科学家 Warren Sturgis McCulloch 和数理逻辑学家 Walter Pitts 从生物神经元的结构上得到启发，
提出了人工神经元的数学模型，这进一步被美国神经物理学家 Frank Rosenblatt 发展并提出了感知机(Perceptron)模型。
1957 年，Frank Rosenblatt 在一台 IBM-704 计算机上面模拟实现了他发明的感知机模型，
这个网络模型可以完成一些简单的视觉分类任务，比如区分三角形、圆形、矩形等 [1]。

感知机模型的结构如图 6.1 所示，它接受长度为𝑛的一维向量𝒙 = [𝑥1, 𝑥2, … , 𝑥𝑛]，每个输入节点通过权值为𝑤𝑖, 𝑖𝜖[1, 𝑛]的连接汇集为变量𝑧，即：
                                    𝑧 = 𝑤1 𝑥1 + 𝑤2 𝑥2 + ⋯ + 𝑤𝑛 𝑥𝑛 + 𝑏
其中𝑏称为感知机的偏置(Bias)，一维向量𝒘 = [𝑤1, 𝑤2, … , 𝑤𝑛]称为感知机的权值(Weight)，𝑧称为感知机的净活性值(Net Activation)。
上式写成向量形式：
                                              𝑧 = 𝒘T𝒙 + 𝑏
感知机是线性模型，并不能处理线性不可分问题。通过在线性模型后添加激活函数后得到活性值(Activation) a:
                                          a = 𝜎(𝑧) = 𝜎(𝒘T𝒙 + 𝑏)
其中激活函数可以是阶跃函数(Step function)，如图 6.2 所示，阶跃函数的输出只有 0/1 两 种数值，
当𝑧 < 0时输出 0，代表类别 0；当𝑧 ≥ 0时输出 1，代表类别 1，即：
                                    a = {1 𝒘T𝒙 + 𝑏 ≥ 0
                                         0 𝒘T𝒙 + 𝑏 < 0
也可以是符号函数(Sign function)，如图 6.3 所示，表达式为：
                                    a = { 1 𝒘T𝒙 + 𝑏 ≥ 0
                                         −1 𝒘T𝒙 + 𝑏 < 0
添加激活函数后，感知机可以用来完成二分类任务。阶跃函数和符号函数在𝑧 = 0处是不连续的，其他位置导数为 0，无法利用梯度下降算法进行参数优化。
为了能够让感知机模型能够从数据中间自动学习，Frank Rosenblatt 提出了感知机的学习算法，如算法 1 所示。
算法 1：感知机训练算法
初始化参数𝒘 = 0, 𝑏 = 0
repeat
    从训练集随机采样一个样本(𝒙𝑖, 𝑦𝑖)
    计算感知机的输出 a = sign(𝒘𝐓𝒙𝑖 + 𝑏)
    如果 a ≠ 𝑦𝑖：
        𝒘′ ← 𝒘 + 𝜂 ∙ 𝑦𝑖 ∙ 𝒙𝒊
        𝑏′ ← 𝑏 + 𝜂 ∙ 𝑦𝑖
    until 训练次数达到要求
   输出：分类网络参数𝒘和b
其中𝜂为学习率。
虽然感知机提出之处被寄予了良好的发展潜力，但是 Marvin Lee Minsky 和 Seymour Papert 于 1969 年在《Perceptrons》
书中证明了以感知机为代表的线性模型不能解决异或(XOR)等线性不可分问题，这直接导致了当时新兴的神经网络的研究进入了低谷期。
尽管感知机模型不能解决线性不可分问题，但书中也提到通过嵌套多层神经网络可以解决。
"""