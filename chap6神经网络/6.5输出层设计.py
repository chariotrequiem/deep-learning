# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/17 14:55
"""
我们来特别地讨论网络的最后一层的设计，它除了和所有的隐藏层一样，完成维度变换、特征提取的功能，还作为输出层使用，
需要根据具体的任务场景来决定是否使用激活函数，以及使用什么类型的激活函数等。
我们将根据输出值的区间范围来分类讨论。常见的几种输出类型包括：
❑ 𝑜𝑖 ∈ 𝑅𝑑 输出属于整个实数空间，或者某段普通的实数空间，比如函数值趋势的预测，年龄的预测问题等。
❑ 𝑜𝑖 ∈ [0,1] 输出值特别地落在[0, 1]的区间，如图片生成，图片像素值一般用[0, 1]区间的值表示；
或者二分类问题的概率，如硬币正反面的概率预测问题。
❑ 𝑜𝑖 ∈ [0, 1], Σ𝑖 𝑜𝑖 = 1 输出值落在[0,1]的区间，并且所有输出值之和为 1，常见的如多分类问题，
如 MNIST 手写数字图片识别，图片属于 10 个类别的概率之和应为 1。
❑ 𝑜𝑖 ∈ [−1, 1] 输出值在[-1, 1]之间


6.5.1普通实数空间
这一类问题比较普遍，像正弦函数曲线预测、年龄的预测、股票走势的预测等都属于整个或者部分连续的实数空间，
输出层可以不加激活函数。误差的计算直接基于最后一层的输出𝒐和真实值𝒚进行计算，
如采用均方差误差函数度量输出值𝒐与真实值𝒚之间的距离：
                            ℒ = 𝑔(𝒐,𝒚)
其中𝑔代表了某个具体的误差计算函数，例如 MSE 等


6.5.2[0, 1]区间
输出值属于[0,1]区间也比较常见，比如图片的生成、二分类问题等。在机器学习中，一般会将图片的像素值归一化到[0,1]区间，
如果直接使用输出层的值，像素的值范围会分布在整个实数空间。为了让像素的值范围映射到[0,1]的有效实数空间，需要在输出层后添
加某个合适的激活函数𝜎，其中 Sigmoid 函数刚好具有此功能。
同样地，对于二分类问题，如硬币的正反面的预测，输出层可以只设置一个节点，表示某个事件 A 发生的概率𝑃(A|𝒙)，𝒙为网络输入。
如果我们利用网络的输出标量𝑜表示正 面事件出现的概率，那么反面事件出现的概率即为1 − 𝑜，网络结构如图 6.12 所示。
                        𝑃(正面|𝒙) = 𝑜
                        𝑃(反面|𝒙) = 1 − 𝑜
此时只需要在输出层的净活性值𝑧后添加 Sigmoid 函数即可将输出转译为概率值。
对于二分类问题，除了可以使用单个输出节点表示事件 A 发生的概率𝑃(A|𝒙)外，还可以分别预测𝑃(A|𝒙)和𝑃(A̅|𝒙)，并满足约束
                            𝑃(A|𝒙) + 𝑃(A̅|𝒙) = 1
其中，A̅表示事件 A 的对立事件。如图 6.13 所示，二分类网络的输出层设置为 2 个节点，第一个节点的输出值表示为事件 A 发生的概率𝑃(A|𝒙)，
第二个节点的输出值表示对立事件发生的概率𝑃(A̅|𝒙)，考虑到 Sigmoid 函数只能将单个值压缩到(0,1)区间，并不会考虑 2 个节点值之间的关系。
我们希望除了满足𝑜𝑖 ∈ [0,1]之外，还希望他们能满足概率之和为 1 的约束：
                                ∑𝑜𝑖 𝑖 = 1
这种情况就是下一节要介绍的问题设定。



6.5.3[0,1]区间，和为 1
输出值𝑜𝑖 ∈ [0,1]，且所有输出值之和为 1，这种设定以多分类问题最为常见。如图 6.15 所示，输出层的每个输出节点代表了一种类别，
图中网络结构用于处理 3 分类任务，3个节点的输出值分布代表了当前样本属于类别 A、类别 B 和类别 C 的概率𝑃(A|𝒙)、𝑃(B|𝒙)、𝑃(C|𝒙)，
考虑多分类问题中的样本只可能属于所有类别中的某一种，因此满足所有类别概率之和为 1 的约束。
如何实现此约束逻辑呢？可以通过在输出层添加 Softmax 函数实现。Softmax 函数定义为
𝑆𝑜𝑓𝑡𝑚 𝑥(𝑧𝑖) ≜ e^𝑧𝑖 / Σj e^𝑧𝑗
Softmax 函数不仅可以将输出值映射到[0,1]区间，还满足所有的输出值之和为 1 的特性。
如图 6.14 中的例子，输出层的输出为[2.0,1.0,0.1]，经过 Softmax 函数计算后，得到输出为[0.7,0.2,0.1]，
每个值代表了当前样本属于每个类别的概率，概率值之和为 1。通过 Softmax函数可以将输出层的输出转译为类别概率，在分类问题中使用的非常频繁。

在 TensorFlow 中，可以通过 tf.nn.softmax 实现 Softmax 函数，代码如下：
In [12]: z = tf.constant([2.,1.,0.1])
tf.nn.softmax(z) # 通过 Softmax 函数
Out[12]:
<tf.Tensor: id=19, shape=(3,), dtype=float32, numpy=array([0.6590012,
0.242433 , 0.0985659], dtype=float32)>
与 Dense 层类似，Softmax 函数也可以作为网络层类使用，通过类 layers.Softmax(axis=-1)可以方便添加 Softmax 层，其中 axis 参数指定需要进行计算的维度。

在 Softmax 函数的数值计算过程中，容易因输入值偏大发生数值溢出现象；在计算交叉熵时，也会出现数值溢出的问题。
为了数值计算的稳定性，TensorFlow 中提供了一个统一的接口，将 Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，
一般推荐使用这些接口函数，避免分开使用 Softmax 函数与交叉熵损失函数。
函数式接口为tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False)，
其中 y_true 代表了One-hot 编码后的真实标签，
     y_pred 表示网络的预测值，
     当 from_logits 设置为 True 时,y_pred 表示须为未经过 Softmax 函数的变量 z；
     当 from_logits 设置为 False 时，y_pred 表示为经过 Softmax 函数的输出。
为了数值计算稳定性，一般设置 from_logits 为 True，此时
tf.keras.losses.categorical_crossentropy 将在内部进行 Softmax 函数计算，所以不需要在模型中显式调用 Softmax 函数，
例如。
In [13]:
z = tf.random.normal([2,10]) # 构造输出层的输出
y_onehot = tf.constant([1,3]) # 构造真实值
y_onehot = tf.one_hot(y_onehot, depth=10) # one-hot 编码
# 输出层未使用 Softmax 函数，故 from_logits 设置为 True
# 这样 categorical_crossentropy 函数在计算损失函数前，会先内部调用 Softmax 函数
loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=True)
loss = tf.reduce_mean(loss) # 计算平均交叉熵损失
loss
Out[13]:
<tf.Tensor: id=210, shape=(), dtype=float32, numpy= 2.4201946>
除了函数式接口，也可以利用 losses.CategoricalCrossentropy(from_logits)类方式同时实 现 Softmax 与交叉熵损失函数的计算，
from_logits 参数的设置方式相同。例如：
In [14]: # 创建 Softmax 与交叉熵计算类，输出层的输出 z 未使用 softmax
criteon = keras.losses.CategoricalCrossentropy(from_logits=True)
loss = criteon(y_onehot,z) # 计算损失
loss
Out[14]:
<tf.Tensor: id=258, shape=(), dtype=float32, numpy= 2.4201946>

6.5.4[-1, 1]
如果希望输出值的范围分布在(−1,1)区间，可以简单地使用 tanh 激活函数，实现如下：
In [15]:
x = tf.linspace(-6.,6.,10)
tf.tanh(x) # tanh 激活函数
Out[15]:
<tf.Tensor: id=264, shape=(10,), dtype=float32, numpy=
array([-0.9999877 , -0.99982315, -0.997458 , -0.9640276 , -0.58278286,
 0.5827831 , 0.9640276 , 0.997458 , 0.99982315, 0.9999877 ], dtype=float32)>


输出层的设计具有一定的灵活性，可以根据实际的应用场景自行设计，充分利用现有激活函数的特性。
"""