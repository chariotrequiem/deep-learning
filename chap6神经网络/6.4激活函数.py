# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/17 14:26
"""
下面介绍神经网络中的常见激活函数，与阶跃函数和符号函数不同，这些函数都是平滑可导的，适合于梯度下降算法。

6.4.1Sigmoid函数
Sigmoid函数也叫Logistic函数，定义为：
                   Sigmoid(x) = 1/(1 + e^(-x))
他的一个优良特性就是能够把𝑥 ∈ 𝑅的输入“压缩”到𝑥 ∈ (0,1)区间，这个区间的数值在机器学习常用来表示以下意义：
❑ 概率分布 (0,1)区间的输出和概率的分布范围[0,1]契合，可以通过 Sigmoid 函数将输出转译为概率输出
❑ 信号强度 一般可以将 0~1 理解为某种信号的强度，如像素的颜色强度，1 代表当前通道颜色最强，0 代表当前通道无颜色；
抑或代表门控值(Gate)的强度，1 代表当前门控全部开放，0 代表门控关闭
Sigmoid 函数连续可导，如图 6.7 所示，可以直接利用梯度下降算法优化网络参数，应用的非常广泛。

在 TensorFlow 中，可以通过 tf.nn.sigmoid 实现 Sigmoid 函数，代码如下：
In [7]:x = tf.linspace(-6.,6.,10) x # 构造-6~6 的输入向量
Out[7]:
<tf.Tensor: id=5, shape=(10,), dtype=float32, numpy=
array([-6. , -4.6666665, -3.3333333, -2. , -0.6666665, 0.666667 , 2. , 3.333334 , 4.666667 , 6. ]…
In [8]:tf.nn.sigmoid(x) # 通过 Sigmoid 函数
Out[8]:
<tf.Tensor: id=7, shape=(10,), dtype=float32, numpy=
array([0.00247264, 0.00931597, 0.03444517, 0.11920291, 0.33924365,
0.6607564 , 0.8807971 , 0.96555483, 0.99068403, 0.9975274 ],dtype=float32)>
可以看到，向量中元素值的范围由[−6,6]映射到(0,1)的区间。

6.4.2RELU函数
在 ReLU(REctified Linear Unit，修正线性单元)激活函数提出之前，Sigmoid 函数通常是神经网络的激活函数首选。
但是 Sigmoid 函数在输入值较大或较小时容易出现梯度值接近于 0 的现象，称为梯度弥散现象。
出现梯度弥散现象时，网络参数长时间得不到更新，导致训练不收敛或停滞不动的现象发生，较深层次的网络模型中更容易出现梯度弥散现象。
2012 年提出的 8 层 AlexNet 模型采用了一种名叫 ReLU 的激活函数，使得网络层数达到了 8 层，自此 ReLU 函数应用的越来越广泛。
ReLU 函数定义为：
                            ReLU(𝑥) ≜ max(0, 𝑥)
函数曲线如图 6.8 所示。可以看到，ReLU 对小于 0 的值全部抑制为 0；对于正数则直接输出，这种单边抑制特性来源于生物学。
2001 年，神经科学家 Dayan 和 Abott 模拟得出更加精确的脑神经元激活模型，如图 6.9 所示，
它具有单侧抑制、相对宽松的兴奋边界等特性，ReLU 函数的设计与之非常类似

在 TensorFlow 中，可以通过 tf.nn.relu 实现 ReLU 函数，代码如下：
In [9]:tf.nn.relu(x) # 通过 ReLU 激活函数
Out[9]:
<tf.Tensor: id=11, shape=(10,), dtype=float32, numpy=
array([0. , 0. , 0. , 0. , 0. , 0.666667,
 2. , 3.333334, 4.666667, 6. ], dtype=float32)>
可以看到，经过 ReLU 激活函数后，负数全部抑制为 0，正数得以保留。除了可以使用函数式接口 tf.nn.relu 实现 ReLU 函数外，
还可以像 Dense 层一样将ReLU 函数作为一个网络层添加到网络中，对应的类为 layers.ReLU()类。一般来说，
激活函数类并不是主要的网络运算层，不计入网络的层数。ReLU 函数的设计源自神经科学，函数值和导数值的计算均十分简单，
同时有着优良的梯度特性，在大量的深度学习应用中被验证非常有效，是应用最广泛的激活函数之一。


6.4.3LeakyReLu
ReLU 函数在𝑥 < 0时导数值恒为 0，也可能会造成梯度弥散现象，为了克服这个问题，LeakyReLU 函数被提出，如图 6.10 所示，
LeakyReLU 的表达式为:
                                LeakyReLU ≜ {𝑥 𝑥 ≥ 0
                                             𝑝𝑥 𝑥 < 0
其中𝑝为用户自行设置的某较小数值的超参数，如 0.02 等。
当𝑝 = 0时，LeayReLU 函数退化为 ReLU 函数；
当𝑝 ≠ 0时，𝑥 < 0处能够获得较小的导数值𝑝，
从而避免出现梯度弥散现象。

在 TensorFlow 中，可以通过 tf.nn.leaky_relu 实现 LeakyReLU 函数，代码如下：
In [10]:tf.nn.leaky_relu(x, alpha=0.1) # 通过 LeakyReLU 激活函数
Out[10]:
<tf.Tensor: id=13, shape=(10,), dtype=float32, numpy=
array([-0.6 , -0.46666667, -0.33333334, -0.2 , -0.06666666,
 0.666667 , 2. , 3.333334 , 4.666667 , 6. ],
 dtype=float32)>
其中 alpha 参数代表𝑝。tf.nn.leaky_relu 对应的类为 layers.LeakyReLU，可以通过LeakyReLU(alpha)创建 LeakyReLU 网络层，
并设置𝑝参数，像 Dense 层一样将 LeakyReLU层放置在网络的合适位置


6.4.4Tanh
Tanh 函数能够将𝑥 ∈ 𝑅的输入“压缩”到(−1,1)区间，定义为：
                 tanh(𝑥) = (e𝑥 − e−𝑥)/(e𝑥 + e−𝑥)
                         = 2 ∙ sigmoid(2𝑥) − 1
可以看到 tanh 激活函数可通过 Sigmoid 函数缩放平移后实现，函数曲线如图 6.11 所示。

在 TensorFlow 中，可以通过 tf.nn.tanh 实现 tanh 函数，代码如下：
In [11]:tf.nn.tanh(x) # 通过 tanh 激活函数
Out[11]:
<tf.Tensor: id=15, shape=(10,), dtype=float32, numpy=
array([-0.9999877 , -0.99982315, -0.997458 , -0.9640276 , -0.58278286,
0.5827831 , 0.9640276 , 0.997458 , 0.99982315, 0.9999877 ], dtype=float32)>
可以看到向量元素值的范围被映射到(-1, 1)之间。
"""