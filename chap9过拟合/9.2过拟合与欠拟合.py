# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/22 10:47
"""
由于真实数据的分布往往是未知而且复杂的，无法推断出其分布函数的类型和相关参数，因此人们在选择学习模型的容量时，
往往会根据经验值选择稍大的模型容量。但模型的容量过大时，有可能出现在训练集上表现较好，但是测试集上表现较差的现象，
如图 9.2 中红色竖线右边区域所示；当模型的容量过小时，有可能出现在训练集和测试集表现皆不佳的现象，如图 9.2 中红色竖线左边区域所示

当模型的容量过大时，网络模型除了学习到训练集数据的模态之外，还把额外的观测误差也学习进来，导致学习的模型在训练集上面表现较好，
但是在未见的样本上表现不佳，也就是模型泛化能力偏弱，我们把这种现象叫作过拟合(Overfitting)。当模型的容量过小时，
模型不能够很好地学习到训练集数据的模态，导致训练集上表现不佳，同时在未见的样本上表现也不佳，我们把这种现象叫作欠拟合(Underfitting)。

这里用一个简单的例子来解释模型的容量与数据的分布之间的关系。图 9.3 绘制了某种数据的分布图，可以大致推测数据可能属于某 2 次多项式分布。
如果我们用简单的线性函数去学习时，会发现很难学习到一个较好的函数，从而出现训练集和测试集表现都不理想的现象，如图 9.3(a)所示，
这种现象叫做欠拟合。但如果用较复杂的函数模型去学习时，有可能学习到的函数会过度地“拟合”训练集样本，从而导致在测试集上表现不佳，
如图 9.3(c)所示，这种现象叫做过拟合。只有学习的模型和真实模型容量大致匹配时，模型才能具有较好地泛化能力，如图 9.3(b)所示。

考虑数据点(𝑥, 𝑦)的分布𝑝data，其中
                                𝑦 = sin(1.2 ∙ 𝜋 ∙ 𝑥)
在采样时，添加随机高斯噪声𝒩(0,1)，共获得 120 个点的数据集，如图 9.4 所示，图中曲线为真实模型函数的曲线，黑色圆形点为训练样本，绿色矩阵点为测试样本。

在已知真实模型的情况下，自然可以设计容量合适的函数假设空间，从而获得不错的学习模型，如图 9.5 所示，我们将模型假设为 2 次多项式模型，
学习得到的函数曲线较好地逼近真实模型的函数曲线。但是在实际场景中，真实模型往往是无法得知的，因此设计的假设空间如果过小，
导致无法搜索到合适的学习模型；设计的假设空间过大，导致模型泛化能力过差。

那么如何去选择模型的容量？统计学习理论给我们提供了一些思路，其中 VC 维度(Vapnik-Chervonenkis 维度)是一个应用比较广泛的度量函数容量的方法。
尽管这些方法给机器学习提供了一定程度的理论保证，但是这些方法却很少应用到深度学习中去，一部分原因是神经网络过于复杂，
很难去确定网络结构背后的数学模型的 VC 维度。尽管统计学习理论很难给出神经网络所需要的最小容量，但是却可以根据奥卡姆剃刀原理(Occam’s razor)
来指导神经网络的设计和训练。奥卡姆剃刀原理是由 14 世纪逻辑学家、圣方济各会修士奥卡姆的威廉(William of Occam)提出的一个解决问题的法则，
他在《箴言书注》2 卷 15 题说“切勿浪费较多东西，去做‘用较少的东西，同样可以做好的事情’。”①。也就是说，
如果两层的神经网络结构能够很好的表达真实模型，那么三层的神经网络也能够很好的表达，但是我们应该优先选择使用更简单的两层神经网络，
因为它的参数量更少，更容易训练，也更容易通过较少的训练样本获得不错的泛化误差。


9.2.1欠拟合
我们来考虑欠拟合的现象。如图 9.6 中所示，黑色圆点和绿色矩形点均独立采样自某抛物线函数的分布，在已知数据的真实模型的条件下，
如果用模型容量小于真实模型的线性函数去回归这些数据，会发现很难找到一条线性函数较好地逼近训练集数据的模态，
具体表现为学习到的线性模型在训练集上的误差(如均方误差)较大，同时在测试集上面的误差也较大。

当我们发现当前的模型在训练集上误差一直维持较高的状态，很难优化减少，同时在测试集上也表现不佳时，我们可以考虑是否出现了欠拟合的现象。
这个时候可以通过增加神经网络的层数、增大中间维度的大小等手段，比较好的解决欠拟合的问题。但是由于现代深度神经网络模型可以很轻易达到较深的层数，
用来学习的模型的容量一般来说是足够的，在实际使用过程中，更多的是出现过拟合现象。


9.2.2过拟合
继续来考虑同样的问题，训练集黑色圆点和测试机绿色矩形点均独立采样自同分布的某抛物线模型，当我们设置模型的假设空间为 25 次多项式时，
它远大于真实模型的函数容量，这时发现学到的模型很有可能过分去拟合训练样本，导致学习模型在训练样本上的误差非常小，
甚至比真实模型在训练集上的误差还要小。但是对于测试样本，模型性能急剧下降，泛化能力非常差，如图 9.7 所示。

现代深度神经网络中过拟合现象非常容易出现，主要是因为神经网络的表达能力非常强，训练集样本数不够，很容易就出现了神经网络的容量偏大的现象。
那么如何有效检测并减少过拟合现象呢？

接下来我们将介绍一系列的方法，来帮助检测并抑制过拟合现象。
"""
