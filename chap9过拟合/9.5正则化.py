# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/22 11:10
"""
通过设计不同层数、大小的网络模型可以为优化算法提供初始的函数假设空间，但是模型的实际容量可以随着网络参数的优化更新而产生变化。
以多项式函数模型为例：
                    𝑦 = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + 𝛽3𝑥3 + ⋯ + 𝛽𝑛𝑥𝑛 + 𝜀
上述模型的容量可以通过𝑛简单衡量。在训练的过程中，如果网络参数𝛽𝑘+1, ⋯ , 𝛽𝑛均为 0，那么网络的实际容量退化到𝑘次多项式的函数容量。
因此，通过限制网络参数的稀疏性，可以来约束网络的实际容量。

这种约束一般通过在损失函数上添加额外的参数稀疏性惩罚项实现，在未加约束之前的优化目标是：
                    𝑚𝑖𝑛 ℒ(𝑓𝜃(x), 𝑦), (x,𝑦) ∈ 𝔻train
对模型的参数添加额外的约束后，优化的目标变为
                    𝑚𝑖𝑛 ℒ(𝑓𝜃(x), 𝑦) + 𝜆 ∙ 𝛺(𝜃), (x,𝑦) ∈ 𝔻train
其中𝛺(𝜃)表示对网络参数𝜃的稀疏性约束函数。一般地，参数𝜃的稀疏性约束通过约束参数𝜃的𝐿范数实现，即
                        𝛺(𝜃) = ∑‖𝜃𝑖‖𝑙
其中‖𝜃𝑖‖𝑙表示参数𝜃𝑖的𝑙范数。

新的优化目标除了要最小化原来的损失函数ℒ( , 𝑦)之外，还需要约束网络参数的稀疏性𝛺(𝜃)，优化算法会在降低ℒ(x, 𝑦)的同时，
尽可能地迫使网络参数𝜃𝑖变得稀疏，它们之间的权重关系通过超参数𝜆来平衡。较大的𝜆意味着网络的稀疏性更重要；
较小的𝜆则意味着网络的训练误差更重要。通过选择合适的𝜆超参数，可以获得较好的训练性能，同时保证网络的稀疏性，从而获得不错的泛化能力。
常用的正则化方式有 L0、L1、L2 正则化。

9.5.1L0正则化
L0 正则化是指采用 L0 范数作为稀疏性惩罚项𝛺(𝜃)的正则化计算方式，即
                        𝛺(𝜃) = ∑‖𝜃𝑖‖0
其中 L0 范数‖𝜃𝑖‖0定义为𝜃𝑖中非零元素的个数。通过约束∑𝜃𝑖‖𝜃𝑖‖0的大小可以迫使网络中的连接权值大部分为 0，从而降低网络的实际参数量和网络容量。
但是由于 L0 范数‖𝜃𝑖‖0并不可导，不能利用梯度下降算法进行优化，在神经网络中使用的并不多。


9.5.2L1正则化
采用 L1 范数作为稀疏性惩罚项𝛺(𝜃)的正则化计算方式叫作 L1 正则化，即
                        𝛺(𝜃) = ∑‖𝜃𝑖‖1
其中 L1 范数‖𝜃𝑖‖1定义为张量𝜃𝑖中所有元素的绝对值之和。L1 正则化也叫 Lasso Regularization，它是连续可导的，在神经网络中使用广泛。
L1 正则化可以实现如下：
# 创建网络参数 w1,w2
w1 = tf.random.normal([4,3])
w2 = tf.random.normal([4,2])
# 计算 L1 正则化项
loss_reg = tf.reduce_sum(tf.math.abs(w1))\
         + tf.reduce_sum(tf.math.abs(w2))


9.5.3L2正则化
采用L2范数作为稀疏性惩罚项𝛺(𝜃)的正则化计算方式叫做 L2 正则化，即
                        𝛺(𝜃) = ∑‖𝜃𝑖‖2
其中 L2 范数‖𝜃𝑖‖2定义为张量𝜃𝑖中所有元素的平方和。L2 正则化也叫 Ridge Regularization，它和 L1 正则化一样，也是连续可导的，
在神经网络中使用广泛。L2 正则化项实现如下：
# 创建网络参数 w1,w2
w1 = tf.random.normal([4,3])
w2 = tf.random.normal([4,2])
# 计算 L2 正则化项
loss_reg = tf.reduce_sum(tf.square(w1)) + tf.reduce_sum(tf.square(w2))


9.5.4正则化效果
继续以月牙形的 2 分类数据为例。在维持网络结构等其它超参数不变的条件下，在损失函数上添加 L2 正则化项，
并通过改变不同的正则化超参数𝜆来获得不同程度的正则化效果。

在训练了 500 个 Epoch 后，我们获得学习模型的分类决策边界，如图 9.16、图 9.17、图 9.18、图 9.19 分布代表了正则化系数𝜆 = 0.00001、0.001、0.1、0.13时的分类效果。
可以看到，随着正则化系数𝜆的增加，网络对参数稀疏性的惩罚变大，从而迫使优化算法搜索让网络容量更小的模型。
在𝜆 = 0.00001时，正则化的作用比较微弱，网络出现了过拟合现象；
但是𝜆 = 0.1时，网络已经能够优化到合适的容量，并没有出现明显过拟合或者欠拟合现象。

实际训练时，一般优先尝试较小的正则化系数𝜆，观测网络是否出现过拟合现象。然后尝试逐渐增大𝜆参数来增加网络参数稀疏性，提高泛化能力。
但是，过大的𝜆参数有可能导致网络不收敛，需要根据实际任务调节。

在不同的正则化系数𝜆下，我们统计了网络中每个连接权值的数值范围。考虑网络的第 2 层的权值矩阵𝑾，其 shape 为[256,256]，
即将输入长度为 256 的向量转换为 256 的输出向量。从全连接层权值连接的角度来看，𝑾一共包含了256 ∙ 256根连接线的权值，
我们将它对应到图 9.20、图 9.21、图 9.22、图 9.23 中的 X-Y 网格中，其中 X 轴的范围为[0,255]，Y 轴的范围为[0,255]，
X-Y 网格的所有整数点分别代表了 shape 为[256,256]的权值张量𝑾的每个位置，每个网格点绘制出当前连接上的权值。
从图中可以看到，添加了不同程度的正则化约束对网络权值的影响。在𝜆 = 0.00001时，正则化的作用比较微弱，网络中权值数值相对较大，
分布在[−1.6088,1.1599]区间；在添加较强稀疏性约束𝜆 = 0.13后，网络权值数值约束在[−0.1104,0.0785]较小范围中，具体的权值范围如表格 9.1 所示，
同时也可以观察到正则化后权值的稀疏性变化。
"""