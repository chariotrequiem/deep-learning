# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/22 14:29
"""
2012 年，Hinton 等人在其论文《Improving neural networks by preventing co-adaptation of feature detectors》中使用了 Dropout 方法来提高模型性能。
Dropout 通过随机断开神经网络的连接，减少每次训练时实际参与计算的模型的参数量；
但是在测试时，Dropout 会恢复所有的连接，保证模型测试时获得最好的性能。

图 9.24 是全连接层网络在某次前向计算时连接状况的示意图。图(a)是标准的全连接神经网络，当前节点与前一层的所有输入节点相连。
在添加了 Dropout 功能的网络层中，如图 9.24(b)所示，每条连接是否断开符合某种预设的概率分布，如断开概率为𝑝的伯努利分布。
图 9.24(b)中的显示了某次具体的采样结果，虚线代表了采样结果为断开的连接线，实线代表了采样结果不断开的连接线。

在 TensorFlow 中，可以通过 tf.nn.dropout(x, rate)函数实现某条连接的 Dropout 功能，其中 rate 参数设置断开的概率值𝑝。
例如：
# 添加 dropout 操作，断开概率为 0.5
x = tf.nn.dropout(x, rate=0.5)
也可以将 Dropout 作为一个网络层使用，在网络中间插入一个 Dropout 层。例如：
# 添加 Dropout 层，断开概率为 0.5
model.add(layers.Dropout(rate=0.5))


为了验证 Dropout 层对网络训练的影响，我们在维持网络层数等超参数不变的条件下，通过在 5 层的全连接层中间隔插入不同数量的 Dropout
层来观测 Dropout 对网络训练的影响。如图 9.25、图 9.26、图 9.27、图 9.28 所示，分布绘制了不添加 Dropout 层，
添加 1、2、4 层 Dropout 层网络模型的决策边界效果。可以看到，在不添加 Dropout 层时，网络模型与之前观测的结果一样，
出现了明显的过拟合现象；随着 Dropout 层的增加，网络模型训练时的实际容量减少，泛化能力变强。

"""