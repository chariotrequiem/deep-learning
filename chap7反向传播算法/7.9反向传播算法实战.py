# å½“å‰ç‰ˆæœ¬ ï¼š python3.7.11
# å¼€å‘æ—¶é—´ ï¼š 2021/9/18 17:11
"""
æœ¬èŠ‚æˆ‘ä»¬å°†åˆ©ç”¨å‰é¢ä»‹ç»çš„å¤šå±‚å…¨è¿æ¥ç½‘ç»œçš„æ¢¯åº¦æ¨å¯¼ç»“æœï¼Œç›´æ¥åˆ©ç”¨ Python å¾ªç¯è®¡ç®—æ¯ä¸€å±‚çš„æ¢¯åº¦ï¼Œå¹¶æŒ‰ç€æ¢¯åº¦ä¸‹é™ç®—æ³•æ‰‹åŠ¨æ›´æ–°ã€‚
ç”±äº TensorFlow å…·æœ‰è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œæˆ‘ä»¬é€‰æ‹©æ²¡æœ‰è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½çš„ Numpy å®ç°ç½‘ç»œï¼Œå¹¶åˆ©ç”¨ Numpy æ‰‹åŠ¨è®¡ç®—æ¢¯åº¦å¹¶æ‰‹åŠ¨æ›´æ–°ç½‘ç»œå‚æ•°ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæœ¬ç« æ¨å¯¼çš„æ¢¯åº¦ä¼ æ’­å…¬å¼æ˜¯é’ˆå¯¹äºå¤šå±‚å…¨è¿æ¥å±‚ï¼Œåªæœ‰ Sigmoid ä¸€ç§æ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸”æŸå¤±å‡½æ•°ä¸ºå‡æ–¹è¯¯å·®å‡½æ•°çš„ç½‘ç»œç±»å‹ã€‚
å¯¹äºå…¶å®ƒç±»å‹çš„ç½‘ç»œï¼Œæ¯”å¦‚æ¿€æ´»å‡½æ•°é‡‡ç”¨ ReLUï¼ŒæŸå¤±å‡½æ•°é‡‡ç”¨äº¤å‰ç†µçš„ç½‘ç»œï¼Œéœ€è¦é‡æ–°æ¨å¯¼æ¢¯åº¦ä¼ æ’­è¡¨è¾¾å¼ï¼Œä½†æ˜¯æ–¹æ³•æ˜¯ä¸€æ ·ã€‚
æ­£æ˜¯å› ä¸ºæ‰‹åŠ¨æ¨å¯¼æ¢¯åº¦çš„æ–¹æ³•å±€é™æ€§è¾ƒå¤§ï¼Œåœ¨å®è·µä¸­é‡‡ç”¨æå°‘ï¼Œæ›´å¤šçš„æ˜¯åˆ©ç”¨è‡ªåŠ¨æ±‚å¯¼å·¥å…·è®¡ç®—ã€‚

æˆ‘ä»¬å°†å®ç°ä¸€ä¸ª 4 å±‚çš„å…¨è¿æ¥ç½‘ç»œï¼Œæ¥å®ŒæˆäºŒåˆ†ç±»ä»»åŠ¡ã€‚ç½‘ç»œè¾“å…¥èŠ‚ç‚¹æ•°ä¸º 2ï¼Œéšè—å±‚çš„èŠ‚ç‚¹æ•°è®¾è®¡ä¸ºï¼š25ã€50å’Œ25ï¼Œè¾“å‡ºå±‚ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œ
åˆ†åˆ«è¡¨ç¤ºå±äºç±»åˆ« 1 çš„æ¦‚ç‡å’Œç±»åˆ« 2çš„æ¦‚ç‡ï¼Œå¦‚å›¾ 7.13 æ‰€ç¤ºã€‚è¿™é‡Œå¹¶æ²¡æœ‰é‡‡ç”¨ Softmax å‡½æ•°å°†ç½‘ç»œè¾“å‡ºæ¦‚ç‡å€¼ä¹‹å’Œè¿›è¡Œçº¦æŸï¼Œ
è€Œæ˜¯ç›´æ¥åˆ©ç”¨å‡æ–¹è¯¯å·®å‡½æ•°è®¡ç®—ä¸ One-hot ç¼–ç çš„çœŸå®æ ‡ç­¾ä¹‹é—´çš„è¯¯å·®ï¼Œæ‰€æœ‰çš„ç½‘ç»œæ¿€æ´»å‡½æ•°å…¨éƒ¨é‡‡ç”¨ Sigmoid å‡½æ•°ï¼Œ
è¿™äº›è®¾è®¡éƒ½æ˜¯ä¸ºäº†èƒ½ç›´æ¥åˆ©ç”¨æˆ‘ä»¬çš„æ¢¯åº¦ä¼ æ’­å…¬å¼ã€‚

7.9.1æ•°æ®é›†
è¿™é‡Œé€šè¿‡ scikit-learn åº“æä¾›çš„ä¾¿æ·å·¥å…·ç”Ÿæˆ 2000 ä¸ªçº¿æ€§ä¸å¯åˆ†çš„ 2 åˆ†ç±»æ•°æ®é›†ï¼Œæ•°æ®çš„ç‰¹å¾é•¿åº¦ä¸º 2ï¼Œé‡‡æ ·å‡ºçš„æ•°æ®åˆ†å¸ƒå¦‚å›¾ 7.14 æ‰€ç¤ºï¼Œ
æ‰€æœ‰çš„çº¢è‰²ç‚¹ä¸ºä¸€ç±»ï¼Œæ‰€æœ‰çš„è“è‰²ç‚¹ä¸ºä¸€ç±»ï¼Œå¯ä»¥çœ‹åˆ°æ¯ä¸ªç±»åˆ«æ•°æ®çš„åˆ†å¸ƒå‘ˆæœˆç‰™çŠ¶ï¼Œå¹¶ä¸”æ˜¯æ˜¯çº¿æ€§ä¸å¯åˆ†çš„ï¼Œæ— æ³•ç”¨çº¿æ€§ç½‘ç»œè·å¾—è¾ƒå¥½æ•ˆæœã€‚
ä¸ºäº†æµ‹è¯•ç½‘ç»œçš„æ€§èƒ½ï¼Œæˆ‘ä»¬æŒ‰ç€7: 3æ¯”ä¾‹åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå…¶ä¸­2000 âˆ™ 0.3 = 600ä¸ªæ ·æœ¬ç‚¹ç”¨äºæµ‹è¯•ï¼Œä¸å‚ä¸è®­ç»ƒï¼Œå‰©ä¸‹çš„ 1400 ä¸ªç‚¹ç”¨äºç½‘ç»œçš„è®­ç»ƒã€‚

æ•°æ®é›†çš„é‡‡é›†ç›´æ¥ä½¿ç”¨ scikit-learn æä¾›çš„ make_moons å‡½æ•°ç”Ÿæˆï¼Œè®¾ç½®é‡‡æ ·ç‚¹æ•°å’Œåˆ‡å‰²æ¯”ç‡ï¼Œä»£ç å¦‚ä¸‹ï¼š
N_SAMPLES = 2000 # é‡‡æ ·ç‚¹æ•°
TEST_SIZE = 0.3 # æµ‹è¯•æ•°é‡æ¯”ç‡
# åˆ©ç”¨å·¥å…·å‡½æ•°ç›´æ¥ç”Ÿæˆæ•°æ®é›†
X, y = make_moons(n_samples = N_SAMPLES, noise=0.2, random_state=100)
# å°† 2000 ä¸ªç‚¹æŒ‰ç€ 7:3 åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=TEST_SIZE, random_state=42)
print(X.shape, y.shape)
å¯ä»¥é€šè¿‡å¦‚ä¸‹å¯è§†åŒ–ä»£ç ç»˜åˆ¶æ•°æ®é›†çš„åˆ†å¸ƒï¼Œå¦‚å›¾ 7.14 æ‰€ç¤ºã€‚
# ç»˜åˆ¶æ•°æ®é›†çš„åˆ†å¸ƒï¼ŒX ä¸º 2D åæ ‡ï¼Œy ä¸ºæ•°æ®ç‚¹çš„æ ‡ç­¾
def make_plot(X, y, plot_name, file_name=None, XX=None, YY=None, preds=None,dark=False):
    if (dark):
        plt.style.use('dark_background')
    else:
        sns.set_style("whitegrid")
    plt.figure(figsize=(16,12))
    axes = plt.gca()
    axes.set(xlabel="$x_1$", ylabel="$x_2$")
    plt.title(plot_name, fontsize=30)
    plt.subplots_adjust(left=0.20)
    plt.subplots_adjust(right=0.80)
    if(XX is not None and YY is not None and preds is not None):
        plt.contourf(XX, YY, preds.reshape(XX.shape), 25, alpha = 1,cmap=cm.Spectral)
        plt.contour(XX, YY, preds.reshape(XX.shape), levels=[.5],cmap="Greys", vmin=0, vmax=.6)
    # ç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œæ ¹æ®æ ‡ç­¾åŒºåˆ†é¢œè‰²
    plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral,edgecolors='none')

    plt.savefig('dataset.svg')
    plt.close()
# è°ƒç”¨ make_plot å‡½æ•°ç»˜åˆ¶æ•°æ®çš„åˆ†å¸ƒï¼Œå…¶ä¸­ X ä¸º 2D åæ ‡ï¼Œy ä¸ºæ ‡ç­¾
make_plot(X, y, "Classification Dataset Visualization ")
plt.show()


7.9.2ç½‘ç»œå±‚
é€šè¿‡æ–°å»ºç±» Layer å®ç°ä¸€ä¸ªç½‘ç»œå±‚ï¼Œéœ€è¦ä¼ å…¥ç½‘ç»œå±‚çš„è¾“å…¥èŠ‚ç‚¹æ•°ã€è¾“å‡ºèŠ‚ç‚¹æ•°ã€æ¿€æ´»å‡½æ•°ç±»å‹ç­‰å‚æ•°ï¼Œ
æƒå€¼ weights å’Œåç½®å¼ é‡ bias åœ¨åˆå§‹åŒ–æ—¶æ ¹æ®è¾“å…¥ã€è¾“å‡ºèŠ‚ç‚¹æ•°è‡ªåŠ¨ç”Ÿæˆå¹¶åˆå§‹åŒ–ã€‚ä»£ç å¦‚ä¸‹ï¼š

class Layer:
    # å…¨è¿æ¥ç½‘ç»œå±‚
    def __init__(self, n_input, n_neurons, activation=None, weights=None,
                 bias=None):
        :param int n_input: è¾“å…¥èŠ‚ç‚¹æ•°
        :param int n_neurons: è¾“å‡ºèŠ‚ç‚¹æ•°
        :param str activation: æ¿€æ´»å‡½æ•°ç±»å‹
        :param weights: æƒå€¼å¼ é‡ï¼Œé»˜è®¤ç±»å†…éƒ¨ç”Ÿæˆ
        :param bias: åç½®ï¼Œé»˜è®¤ç±»å†…éƒ¨ç”Ÿæˆ
        # é€šè¿‡æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ç½‘ç»œæƒå€¼ï¼Œåˆå§‹åŒ–éå¸¸é‡è¦ï¼Œä¸åˆé€‚çš„åˆå§‹åŒ–å°†å¯¼è‡´ç½‘ç»œä¸æ”¶æ•›
        self.weights = weights if weights is not None else np.random.randn(n_input, n_neurons) * np.sqrt(1 / n_neurons)
        self.bias = bias if bias is not None else np.random.rand(n_neurons) * 0.1
        self.activation = activation  # æ¿€æ´»å‡½æ•°ç±»å‹ï¼Œå¦‚â€™sigmoidâ€™
        self.last_activation = None  # æ¿€æ´»å‡½æ•°çš„è¾“å‡ºå€¼o
        self.error = None  # ç”¨äºè®¡ç®—å½“å‰å±‚çš„delta å˜é‡çš„ä¸­é—´å˜é‡
        self.delta = None  # è®°å½•å½“å‰å±‚çš„delta å˜é‡ï¼Œç”¨äºè®¡ç®—æ¢¯åº¦

    # ç½‘ç»œå±‚çš„å‰å‘ä¼ æ’­å‡½æ•°å®ç°å¦‚ä¸‹ï¼Œå…¶ä¸­last_activation å˜é‡ç”¨äºä¿å­˜å½“å‰å±‚çš„è¾“å‡ºå€¼ï¼š
    def activate(self, x):
        # å‰å‘ä¼ æ’­å‡½æ•°
        r = np.dot(x, self.weights) + self.bias  # X@W+b
        # é€šè¿‡æ¿€æ´»å‡½æ•°ï¼Œå¾—åˆ°å…¨è¿æ¥å±‚çš„è¾“å‡ºo
        self.last_activation = self._apply_activation(r)
        return self.last_activation

    # ä¸Šè¿°ä»£ç ä¸­çš„self._apply_activation å‡½æ•°å®ç°äº†ä¸åŒç±»å‹çš„æ¿€æ´»å‡½æ•°çš„å‰å‘è®¡ç®—è¿‡ç¨‹ï¼Œ
    # å°½ç®¡æ­¤å¤„æˆ‘ä»¬åªä½¿ç”¨Sigmoid æ¿€æ´»å‡½æ•°ä¸€ç§ã€‚ä»£ç å¦‚ä¸‹ï¼š
    def _apply_activation(self, r):
        # è®¡ç®—æ¿€æ´»å‡½æ•°çš„è¾“å‡º
        if self.activation is None:
            return r  # æ— æ¿€æ´»å‡½æ•°ï¼Œç›´æ¥è¿”å›
        # ReLU æ¿€æ´»å‡½æ•°
        elif self.activation == 'relu':
            return np.maximum(r, 0)
        # tanh æ¿€æ´»å‡½æ•°
        elif self.activation == 'tanh':
            return np.tanh(r)
        # sigmoid æ¿€æ´»å‡½æ•°
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-r))
        return r

    # é’ˆå¯¹äºä¸åŒç±»å‹çš„æ¿€æ´»å‡½æ•°ï¼Œå®ƒä»¬çš„å¯¼æ•°è®¡ç®—å®ç°å¦‚ä¸‹ï¼š
    def apply_activation_derivative(self, r):
        # è®¡ç®—æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
        # æ— æ¿€æ´»å‡½æ•°ï¼Œå¯¼æ•°ä¸º1
        if self.activation is None:
            return np.ones_like(r)
        # ReLU å‡½æ•°çš„å¯¼æ•°å®ç°
        elif self.activation == 'relu':
            grad = np.array(r, copy=True)
            grad[r > 0] = 1.
            grad[r <= 0] = 0.
            return grad
        # tanh å‡½æ•°çš„å¯¼æ•°å®ç°
        elif self.activation == 'tanh':
            return 1 - r ** 2
        # Sigmoid å‡½æ•°çš„å¯¼æ•°å®ç°
        elif self.activation == 'sigmoid':
            return r * (1 - r)
        return r
å¯ä»¥çœ‹åˆ°ï¼ŒSigmoid å‡½æ•°çš„å¯¼æ•°å®ç°ä¸ºğ‘Ÿ (1 âˆ’ ğ‘Ÿ)ï¼Œå…¶ä¸­ğ‘Ÿå³ä¸ºğœ(ğ‘§)



7.9.3ç½‘ç»œæ¨¡å‹
åˆ›å»ºå•å±‚ç½‘ç»œç±»åï¼Œæˆ‘ä»¬å®ç°ç½‘ç»œæ¨¡å‹çš„ NeuralNetwork ç±»ï¼Œå®ƒå†…éƒ¨ç»´æŠ¤å„å±‚çš„ç½‘ç»œå±‚ Layer ç±»å¯¹è±¡ï¼Œå¯ä»¥é€šè¿‡ add_layer å‡½æ•°è¿½åŠ ç½‘ç»œå±‚ï¼Œ
å®ç°åˆ›å»ºä¸åŒç»“æ„çš„ç½‘ç»œæ¨¡å‹ç›®çš„ã€‚ä»£ç å¦‚ä¸‹ï¼š
class NeuralNetwork:
    # ç¥ç»ç½‘ç»œæ¨¡å‹å¤§ç±»
    def __init__(self):
        self._layers = [] # ç½‘ç»œå±‚å¯¹è±¡åˆ—è¡¨
    def add_layer(self, layer):
        # è¿½åŠ ç½‘ç»œå±‚
        self._layers.append(layer)
    ç½‘ç»œçš„å‰å‘ä¼ æ’­åªéœ€è¦å¾ªç¯è°ƒå„ä¸ªç½‘ç»œå±‚å¯¹è±¡çš„å‰å‘è®¡ç®—å‡½æ•°å³å¯ï¼Œä»£ç å¦‚ä¸‹ï¼š
    def feed_forward(self, X):
        # å‰å‘ä¼ æ’­
        for layer in self._layers:
            # ä¾æ¬¡é€šè¿‡å„ä¸ªç½‘ç»œå±‚
            X = layer.activate(X)
        return X
æ ¹æ®å›¾ 7.13 çš„ç½‘ç»œç»“æ„é…ç½®ï¼Œåˆ©ç”¨ NeuralNetwork ç±»åˆ›å»ºç½‘ç»œå¯¹è±¡ï¼Œå¹¶æ·»åŠ  4 å±‚å…¨è¿æ¥å±‚ï¼Œä»£ç å¦‚ä¸‹ï¼š
nn = NeuralNetwork() # å®ä¾‹åŒ–ç½‘ç»œç±»
nn.add_layer(Layer(2, 25, 'sigmoid')) # éšè—å±‚ 1, 2=>25
nn.add_layer(Layer(25, 50, 'sigmoid')) # éšè—å±‚ 2, 25=>50
nn.add_layer(Layer(50, 25, 'sigmoid')) # éšè—å±‚ 3, 50=>25
nn.add_layer(Layer(25, 2, 'sigmoid')) # è¾“å‡ºå±‚, 25=>2


ç½‘ç»œæ¨¡å‹çš„åå‘ä¼ æ’­å®ç°ç¨å¤æ‚ï¼Œéœ€è¦ä»æœ€æœ«å±‚å¼€å§‹ï¼Œè®¡ç®—æ¯å±‚çš„ğ›¿å˜é‡ï¼Œç„¶åæ ¹æ®æ¨å¯¼å‡ºçš„æ¢¯åº¦å…¬å¼ï¼Œ
å°†è®¡ç®—å‡ºçš„ğ›¿å˜é‡å­˜å‚¨åœ¨ Layer ç±»çš„ delta å˜é‡ä¸­ã€‚ä»£ç å¦‚ä¸‹ï¼š
def backpropagation(self, X, y, learning_rate):
    # åå‘ä¼ æ’­ç®—æ³•å®ç°
    # å‰å‘è®¡ç®—ï¼Œå¾—åˆ°è¾“å‡ºå€¼
    output = self.feed_forward(X)
    for i in reversed(range(len(self._layers))): # åå‘å¾ªç¯
        layer = self._layers[i] # å¾—åˆ°å½“å‰å±‚å¯¹è±¡
        # å¦‚æœæ˜¯è¾“å‡ºå±‚
        if layer == self._layers[-1]: # å¯¹äºè¾“å‡ºå±‚
            layer.error = y - output # è®¡ç®— 2 åˆ†ç±»ä»»åŠ¡çš„å‡æ–¹å·®çš„å¯¼æ•°
            # å…³é”®æ­¥éª¤ï¼šè®¡ç®—æœ€åä¸€å±‚çš„ deltaï¼Œå‚è€ƒè¾“å‡ºå±‚çš„æ¢¯åº¦å…¬å¼
            layer.delta = layer.error * layer.apply_activation_derivative(output)
        else: # å¦‚æœæ˜¯éšè—å±‚
            next_layer = self._layers[i + 1] # å¾—åˆ°ä¸‹ä¸€å±‚å¯¹è±¡
            layer.error = np.dot(next_layer.weights, next_layer.delta)
            # å…³é”®æ­¥éª¤ï¼šè®¡ç®—éšè—å±‚çš„ deltaï¼Œå‚è€ƒéšè—å±‚çš„æ¢¯åº¦å…¬å¼
            layer.delta = layer.error * layer.apply_activation_derivative(layer.last_activation)
 â€¦# ä»£ç æ¥ä¸‹é¢
åœ¨åå‘è®¡ç®—å®Œæ¯å±‚çš„ğ›¿å˜é‡åï¼Œåªéœ€è¦æŒ‰ç€ ğœ•â„’ ğœ•ğ‘¤ğ‘–
= ğ‘œğ‘–ğ›¿ ( )å…¬å¼è®¡ç®—æ¯å±‚å‚æ•°çš„æ¢¯åº¦ï¼Œå¹¶
æ›´æ–°ç½‘ç»œå‚æ•°å³å¯ã€‚ç”±äºä»£ç ä¸­çš„ delta è®¡ç®—çš„å…¶å®æ˜¯âˆ’ğ›¿ï¼Œå› æ­¤æ›´æ–°æ—¶ä½¿ç”¨äº†åŠ å·ã€‚ä»£ç å¦‚ä¸‹ï¼š
 def backpropagation(self, X, y, learning_rate):
 â€¦ # ä»£ç æ¥ä¸Šé¢
 # å¾ªç¯æ›´æ–°æƒå€¼
 for i in range(len(self._layers)):
 layer = self._layers[i]
 # o_i ä¸ºä¸Šä¸€ç½‘ç»œå±‚çš„è¾“å‡º
 o_i = np.atleast_2d(X if i == 0 else self._layers[i - 1].last_activation)
 # æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œdelta æ˜¯å…¬å¼ä¸­çš„è´Ÿæ•°ï¼Œæ•…è¿™é‡Œç”¨åŠ å·
 layer.weights += layer.delta * o_i.T * learning_rate
å› æ­¤ï¼Œåœ¨ backpropagation å‡½æ•°ä¸­ï¼Œåå‘è®¡ç®—æ¯å±‚çš„ğ›¿å˜é‡ï¼Œå¹¶æ ¹æ®æ¢¯åº¦å…¬å¼è®¡ç®—æ¯å±‚å‚æ•°çš„æ¢¯åº¦å€¼ï¼ŒæŒ‰ç€æ¢¯åº¦ä¸‹é™ç®—æ³•å®Œæˆä¸€æ¬¡å‚æ•°çš„æ›´æ–°ã€‚


7.9.4ç½‘ç»œè®­ç»ƒ
è¿™é‡Œçš„äºŒåˆ†ç±»ä»»åŠ¡ç½‘ç»œè®¾è®¡ä¸ºä¸¤ä¸ªè¾“å‡ºèŠ‚ç‚¹ï¼Œå› æ­¤éœ€è¦å°†çœŸå®æ ‡ç­¾ğ‘¦è¿›è¡Œ One-hot ç¼–ç ï¼Œä»£ç å¦‚ä¸‹ï¼š
def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):
    # ç½‘ç»œè®­ç»ƒå‡½æ•°
    # one-hot ç¼–ç 
    y_onehot = np.zeros((y_train.shape[0], 2))
    y_onehot[np.arange(y_train.shape[0]), y_train] = 1
å°† One-hot ç¼–ç åçš„çœŸå®æ ‡ç­¾ä¸ç½‘ç»œçš„è¾“å‡ºè®¡ç®—å‡æ–¹è¯¯å·®ï¼Œå¹¶è°ƒç”¨åå‘ä¼ æ’­å‡½æ•°æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œå¾ªç¯è¿­ä»£è®­ç»ƒé›† 1000 éå³å¯ã€‚ä»£ç å¦‚ä¸‹ï¼š
    mses = []
    for i in range(max_epochs): # è®­ç»ƒ 1000 ä¸ª epoch
        for j in range(len(X_train)): # ä¸€æ¬¡è®­ç»ƒä¸€ä¸ªæ ·æœ¬
            self.backpropagation(X_train[j], y_onehot[j], learning_rate)
        if i % 10 == 0:
            # æ‰“å°å‡º MSE Loss
            mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))
            mses.append(mse)
            print('Epoch: #%s, MSE: %f' % (i, float(mse)))

            # ç»Ÿè®¡å¹¶æ‰“å°å‡†ç¡®ç‡
            print('Accuracy: %.2f%%' % (self.accuracy(self.predict(X_test), y_test.flatten()) * 100))
    return mses
"""