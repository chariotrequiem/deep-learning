# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/18 17:11
"""
本节我们将利用前面介绍的多层全连接网络的梯度推导结果，直接利用 Python 循环计算每一层的梯度，并按着梯度下降算法手动更新。
由于 TensorFlow 具有自动求导功能，我们选择没有自动求导功能的 Numpy 实现网络，并利用 Numpy 手动计算梯度并手动更新网络参数。

需要注意的是，本章推导的梯度传播公式是针对于多层全连接层，只有 Sigmoid 一种激活函数，并且损失函数为均方误差函数的网络类型。
对于其它类型的网络，比如激活函数采用 ReLU，损失函数采用交叉熵的网络，需要重新推导梯度传播表达式，但是方法是一样。
正是因为手动推导梯度的方法局限性较大，在实践中采用极少，更多的是利用自动求导工具计算。

我们将实现一个 4 层的全连接网络，来完成二分类任务。网络输入节点数为 2，隐藏层的节点数设计为：25、50和25，输出层两个节点，
分别表示属于类别 1 的概率和类别 2的概率，如图 7.13 所示。这里并没有采用 Softmax 函数将网络输出概率值之和进行约束，
而是直接利用均方误差函数计算与 One-hot 编码的真实标签之间的误差，所有的网络激活函数全部采用 Sigmoid 函数，
这些设计都是为了能直接利用我们的梯度传播公式。

7.9.1数据集
这里通过 scikit-learn 库提供的便捷工具生成 2000 个线性不可分的 2 分类数据集，数据的特征长度为 2，采样出的数据分布如图 7.14 所示，所有的红色点为一类，所有的蓝色点
为一类，可以看到每个类别数据的分布呈月牙状，并且是是线性不可分的，无法用线性网
络获得较好效果。为了测试网络的性能，我们按着7: 3比例切分训练集和测试集，其中
2000 ∙ 0 3 = 600个样本点用于测试，不参与训练，剩下的 1400 个点用于网络的训练。
"""