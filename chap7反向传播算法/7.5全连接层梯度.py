# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/18 14:37
"""
在介绍完梯度的基础知识后，我们正式地进入到神经网络的反向传播算法的推导。实际使用的神经网络的结构多种多样，
不可能一一分析其梯度表达式。我们将以全连接层网络、激活函数采用 Sigmoid 函数、误差函数为 Softmax+MSE 损失函数的神经网络为例，
推导其梯度传播规律。


7.5.1单神经元梯度
对于采用Sigmoid激活函数的神经元模型，他的数学模型可以写为：
                                    𝑜(1) = 𝜎(𝒘(1)T𝒙 + 𝑏(1))
其中变量的上标表示层数，方便与后续推导统一格式，如𝑜(1)表示第一层的输出，𝒙表示网络的输入，
我们以权值参数𝑤j1的偏导数𝜕ℒ/𝜕𝑤1推导为例。为了方便演示，我们将神经元模型绘制如图 7.7 所示，图中未画出偏置𝑏，输入节点数为 J。
其中输入第𝑗个节点到输出𝑜(1)的权值连接记为𝑤(1)𝑗1，上标表示权值参数属于的层数，下标表示当前连接的起始节点号和终止节点号，
如下标𝑗1表示上一层的第𝑗号节点到当前层的第 1 号节点。经过激活函数𝜎之前的变量叫做𝑧1(1)，经过激活函数𝜎之后的变量叫𝑜1(1)，
由于只有一个输出节点，故𝑜1(1) = 𝑜(1) = 𝑜。输出与真实标签之间通过误差函数函数计算误差值，误差值记为ℒ。

如果我们采用均方误差函数，考虑到单个神经元只有一个输出𝑜1(1)，那么损失可以表达为： \
                                ℒ = 1/2 (𝑜1(1) − 𝑡)2 = 1/2 (𝑜1 − 𝑡)2
其中𝑡为真实标签值，添加1/2并不影响梯度的方向，计算更简便。
我们以权值连接的第𝑗 ∈ [1,𝐽]号节点的权值变量𝑤j1为例，考虑损失函数ℒ对其的偏导数𝜕ℒ/𝜕𝑤j1：
                                𝜕ℒ / 𝜕𝑤j1 = (𝑜1 − 𝑡) 𝜕𝑜1 / 𝜕𝑤j1
将𝑜1 = 𝜎(𝑧1)代入，考虑到 Sigmoid 函数的导数𝜎′= 𝜎(1 − 𝜎)，则有：
                                𝜕ℒ / 𝜕𝑤j1 = (𝑜1 − 𝑡) 𝜕𝜎(𝑧1) / 𝜕𝑤j1
                                           = (𝑜1 − 𝑡)𝜎(𝑧1)(1 − 𝜎(𝑧1)) 𝜕𝑧1(1) / 𝜕𝑤j1
𝜎(𝑧1)写成𝑜1，继续推导𝜕𝑧1(1) / 𝜕𝑤j1：
                                𝜕ℒ / 𝜕𝑤j1 = (𝑜1 − 𝑡)𝑜1(1 − 𝑜1) 𝜕𝑧1(1) / 𝜕𝑤j1
考虑𝜕𝑧1(1) / 𝜕𝑤j1 = 𝑥j，可得：
                                𝜕ℒ / 𝜕𝑤j1 = (𝑜1 − 𝑡)𝑜1(1 − 𝑜1)𝑥j
从上式可以看到，误差对权值𝑤 1的偏导数只与输出值𝑜1、真实值𝑡以及当前权值连接的输入𝑥j有关。



7.5.2全连接层梯度
我们把单个神经元模型推广到单层的全连接层的网络上，如图 7.8 所示。输入层通过一个全连接层得到输出向量𝒐(1)，
与真实标签向量𝒕计算均方误差。输入节点数为 J，输出节点数为𝐾。

多输出的全连接网络层模型与单个神经元模型不同之处在于，它多了很多的输出节点𝑜1(1), 𝑜2(1), 𝑜3(1),⋯ , 𝑜𝐾(1)，
每个输出节点分别对应到真实标签𝑡1,𝑡2, … ,𝑡𝐾。𝑤jk是输入第𝑗号节点与输出第𝑘号节点的连接权值。
均方误差可以表达为：
                                    ℒ = 1/2∑(𝑜𝑖(1) − 𝑡𝑖)2
由于𝜕ℒ / 𝜕𝑤jk只与节点𝑜𝑘(1)有关联，上式中的求和符号可以去掉，即𝑖 = 𝑘：
                                𝜕ℒ / 𝜕𝑤j𝑘 = (𝑜𝑘 − 𝑡𝑘) 𝜕𝑜𝑘 / 𝜕𝑤j𝑘
将𝑜𝑘 = 𝜎(𝑧𝑘)代入可得：
                                𝜕ℒ / 𝜕𝑤j𝑘 = (𝑜𝑘 − 𝑡𝑘) 𝜕𝜎(𝑧𝑘) / 𝜕𝑤j𝑘
考虑 Sigmoid 函数的导数𝜎′ = 𝜎(1 − 𝜎)，代入可得：
                            𝜕ℒ / 𝜕𝑤j𝑘 = (𝑜𝑘 − 𝑡𝑘)𝜎(𝑧𝑘)(1 − 𝜎(𝑧𝑘)) 𝜕𝑧𝑘(1) / 𝜕𝑤j𝑘
将𝜎(𝑧𝑘)记为𝑜𝑘：
                            𝜕ℒ / 𝜕𝑤j𝑘 = (𝑜𝑘 − 𝑡𝑘)𝑜𝑘(1 − 𝑜𝑘) 𝜕𝑧𝑘(1) / 𝜕𝑤j𝑘
将𝜕𝑧𝑘(1) / 𝜕𝑤j𝑘 = 𝑥j替换，最终可得：
                            𝜕ℒ / 𝜕𝑤j𝑘  = (𝑜𝑘 − 𝑡𝑘)𝑜𝑘(1 − 𝑜𝑘)𝑥j
由此可以看到，某条连接𝑤j𝑘上面的偏导数，只与当前连接的输出节点𝑜𝑘(1)，对应的真实值节点的标签𝑡𝑘(1)，以及对应的输入节点𝑥j有关。

我们令𝛿𝑘 = (𝑜𝑘 − 𝑡𝑘)𝑜𝑘(1 − 𝑜𝑘)，则𝜕ℒ / 𝜕𝑤j𝑘可以表达为：
                                𝜕ℒ / 𝜕𝑤j𝑘 = 𝛿𝑘 𝑥j
其中𝛿𝑘变量表征连接线的终止节点的误差梯度传播的某种特性，使用𝛿𝑘表示后，𝜕ℒ / 𝜕𝑤j𝑘偏导数只与当前连接的起始节点𝑥 ，终止节点处𝛿𝑘有关，
理解起来比较简洁直观。后续我们将会在看到𝛿𝑘在循环推导梯度中的作用。

现在已经推导完单层神经网络(即输出层)的梯度传播方式，接下来我们尝试推导倒数第二层的梯度传播方式。完成了倒数第二层的传播推导后，
就可以类似地，循环往复推导所有隐藏层的梯度传播方式，从而获得所有层参数的梯度计算表达式。
在介绍反向传播算法之前，我们先学习导数传播的一个核心法则：链式法则。
"""