# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/18 9:37
"""
在高中阶段，我们就接触到导数(Derivative)的概念，它被定义为自变量𝑥产生一个微小扰动∆𝑥后，
函数输出值的增量∆𝑦与自变量增量∆𝑥的比值在∆𝑥趋于 0 时的极限𝑎，如果存在，𝑎即为在𝑥0处的导数：
                         𝑎 = lim ∆𝑦 / ∆𝑥 = lim    𝑓(𝑥 + ∆𝑥) − 𝑓(𝑥) / ∆𝑥
                            ∆𝑥→0          ∆𝑥→0
函数的导数可以记为𝑓′(𝑥)或d𝑦d𝑥。从几何角度来看，一元函数在某处的导数就是函数的切线在此处的斜率，即函数值沿着𝑥方向的变化率。
考虑物理学中例子：自由落体运动的位移函数的表达式𝑦 = 1/2𝑔𝑡^2，位移对时间的导数d𝑦/d𝑡 = d1/2𝑔𝑡^2 /d𝑡 = 𝑔𝑡，
考虑到速度𝑣定义为位移的变化率，因此𝑣 = 𝑔𝑡，位移对时间的导数即为速度。

实际上，导数是一个非常宽泛的概念，只是因为以前接触到的函数大多是一元函数，自变量∆𝑥只有两个方向：𝑥+和𝑥−。
当函数的自变量数大于一个时，函数的导数概念拓展为函数值沿着任意∆𝒙方向的变化率。
导数本身是标量，没有方向，但是导数表征了函数值在某个方向∆𝒙的上变化率。在这些任意∆𝒙方向中，沿着坐标轴的几个方向比较特殊，
此时的导数也叫作偏导数(Partial Derivative)。对于一元函数，导数记为d𝑦d𝑥；对于多元函数的偏导数，
记为𝜕𝑦 / 𝜕𝑥1 , 𝜕𝑦 / 𝜕𝑥2 , ⋯等。偏导数是导数的特例，也没有方向。

考虑本质上为多元函数的神经网络模型，比如 shape 为[784, 256]的权值矩阵𝑾，它包含了784 × 256个连接权值𝑤，
我们需要求出784 × 256个偏导数。需要注意的是，在数学表达习惯中，一般要讨论的自变量记为𝒙，但是在神经网络中，
𝒙一般用来表示输入，比如图片、文本、语音数据等，网络的自变量是网络参数集𝜃 = {𝑤1, 𝑏1, 𝑤2, 𝑏2, ⋯ }。
利用梯度下降算法优化网络时，需要求出网络的所有偏导数。因此，我们关心的也是误差函数输出ℒ沿着自变量𝜃𝑖方向上的导数，
即ℒ对网络参数𝜃𝑖的偏导数 𝜕ℒ / 𝜕𝑤1 , 𝜕ℒ / 𝜕𝑏1 , ⋯等。
把函数所有偏导数写成向量形式：
                ∇𝜃ℒ = (∂ℒ/ ∂𝜃1 , ∂ℒ/ ∂𝜃2 , ∂ℒ/ ∂𝜃3 , ⋯ , ∂ℒ/ ∂𝜃n)
此时梯度下降算法可以按着向量形式进行更新：
                                𝜃′ = 𝜃 − 𝜂 ∙ ∇𝜃ℒ
𝜂为学习率超参数。梯度下降算法一般是寻找函数ℒ的最小值，有时也希望求解函数的最大值，如强化学习中希望最大化回报函数，
则可按着梯度方向更新：
                                𝜃′ = 𝜃 + 𝜂 ∙ ∇𝜃ℒ

这种更新方式称为梯度上升算法。梯度下降算法和梯度上升算法思想上是相同的，一是朝着梯度的反向更新，一是朝着梯度的方向更新，
两者都需要求解偏导数。这里把向量(∂ℒ/ ∂𝜃1 , ∂ℒ/ ∂𝜃2 , ∂ℒ/ ∂𝜃3 , ⋯ , ∂ℒ/ ∂𝜃n)称为函数的梯度(Gradient)，它由所有偏导数组成，
表征方向，梯度的方向表示函数值上升最快的方向，梯度的反向则表示函数值下降最快的方向。

通过梯度下降算法并不能保证得到全局最优解，这主要是目标函数的非凸性造成的。考虑图 7.1 非凸函数，深蓝色区域为极小值区域，
不同的优化轨迹可能得到不同的最优数值解，这些数值解并不一定是全局最优解。

神经网络的模型表达式通常非常复杂，模型参数量可达千万、数亿级别，几乎所有的神经网络的优化问题都是依赖于深度学习框架去自动计算网络参数的梯度，
然后采用梯度下降算法循环迭代优化网络的参数直至性能满足需求。深度学习框架这里主要实现的算法就是反向传播算法和梯度下降算法，
因此理解这两个算法的原理有利于了解深度学习框架的作用。

在介绍多层神经网络的反向传播算法之前，我们先介绍导数的常见属性，常见激活函数、损失函数的梯度推导，然后再推导多层神经网络的梯度传播规律。
"""