# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/13 15:32
"""
现在来小结一下上述方案：我们需要找出最优参数(Optimal Parameter)𝑤∗和𝑏∗，使得输入和输出满足线性关系𝑦(𝑖) = 𝑤𝑥(𝑖) + 𝑏, 𝑖 ∈ [1, 𝑛]。
但是由于观测误差𝜖的存在，需要通过采样足够多组的数据样本组成的数据集(Dataset)：𝔻 = {(𝑥(1),𝑦(1)), (𝑥(2),𝑦(2)),… , (𝑥(𝑛), 𝑦(𝑛))}，
找到一组最优的参数 𝑤∗和𝑏∗使得均方误差ℒ最小。
对于单输入的神经元模型，只需要两个样本，就能通过消元法求出方程组的精确解，这种通过严格的公式推导出的精确解称为解析解(Closed-form Solution)。
但是对于多个数据点(𝑛 ≫ 2)的情况，这时很有可能不存在解析解，我们只能借助数值方法去优化(Optimize)出一个近似的数值解(Numerical Solution)。
为什么叫作优化？这是因为计算机的计算速度非常快，我们可以借助强大的计算能力去多次“搜索”和“试错”，从而一步步降低误差ℒ。
最简单的优化方法就是暴力搜索或随机试验，比如要找出最合适的𝑤∗和𝑏∗，我们就可以从(部分)实数空间中随机采样任意的𝑤和𝑏，
并计算出对应模型的误差值ℒ，然后从测试过的{ℒ}中挑出最好的ℒ∗，它所对应的𝑤和𝑏就可以作为我们要找的最优𝑤∗和𝑏∗。

这种算法固然简单直接，但是面对大规模、高维度数据的优化问题时计算效率极低，基本不可行。
梯度下降算法(Gradient Descent)是神经网络训练中最常用的优化算法，配合强大的图形处理芯片GPU(Graphics Processing Unit)的并行加速能力，
非常适合优化海量数据的神经网络模型，自然也适合优化我们这里的神经元线性模型。这里先简单地应用梯度下降算法，用于解决神经元模型预测的问题。
由于梯度下降算法是深度学习的核心算法，我们将在第 7 章非常详尽地推导梯度下降算法在神经网络中的应用，这里先给读者第一印象。

我们在高中时代学过导数(Derivative)的概念，如果要求解一个函数的极大、极小值，
可以简单地令导数函数为 0，求出对应的自变量点(称为驻点)，再检验驻点类型即可。以函数𝑓(𝑥) = 𝑥2 ∙ 𝑠𝑖𝑛 (𝑥)为例，
我们绘制出函数及其导数在𝑥 ∈ [−1 ,1 ]区间曲线，其中蓝色实线为𝑓(𝑥)，黄色虚线为d𝑓d(𝑥𝑥)。可以看出，函数导数(虚线)为0的点即为
𝑓(𝑥)的驻点，函数的极大值和极小值点均出现在驻点中。

函数的梯度(Gradient)定义为函数对各个自变量的偏导数(Partial Derivative)组成的向量。
考虑 3 维函数𝑧 = 𝑓(𝑥, 𝑦)，函数对自变量𝑥的偏导数记为𝜕𝑧/𝜕𝑥，函数对自变量𝑦的偏导数记为𝜕z/𝜕y,则梯度∇𝑓为向量(𝜕𝑧/𝜕𝑥，𝜕z/𝜕y)。
我们通过一个具体的函数来感受梯度的性质，如图所示，𝑓(𝑥, 𝑦) = −((cos𝑥)^2 + (cos𝑦)^2)2，图中𝑥𝑦平面的红色箭头的长度表示梯度向量的模，
箭头的方向表示梯度向量的方向。可以看到，箭头的方向总是指向当前位置函数值增速最大的方向，函数曲面越陡峭，箭头的长度也就越长，梯度的模也越大。

通过上面的例子，我们能直观地感受到，函数在各处的梯度方向∇𝑓总是指向函数值增大的方向，那么梯度的反方向−∇𝑓应指向函数值减少的方向。
利用这一性质，我们只需要按照
                 𝒙′ = 𝒙 − 𝜂 ∙ ∇𝑓  (2.1)
来迭代更新𝒙′，就能获得越来越小的函数值，其中𝜂用来缩放梯度向量，一般设置为某较小的值，如 0.01、0.001 等。
特别地，对于一维函数，上述向量形式可以退化成标量形式：
                𝑥′ = 𝑥 − 𝜂 ∙ d𝑦/d𝑥
通过上式迭代更新𝑥′若干次，这样得到的𝑥′处的函数值𝑦′，总是更有可能比在𝑥处的函数值𝑦小。

通过式(2.1)方式优化参数的方法称为梯度下降算法，它通过循环计算函数的梯度∇𝑓并更新待优化参数𝜃，从而得到函数𝑓获得极小值时
参数𝜃的最优数值解。需要注意的是，在深度学习中，一般𝒙表示模型输入，模型的待优化参数一般用𝜃、𝑤、𝑏等符号表示。
现在我们将利用速学的梯度下降算法来求解𝑤∗和𝑏∗参数。这里要最小化的是均方差误差函数ℒ：
ℒ = 1/𝑛∑(𝑤𝑥(𝑖) + 𝑏 − 𝑦(𝑖))^2
需要优化的模型参数是𝑤和𝑏，因此我们按照
𝑤′ = 𝑤 − 𝜂 𝜕ℒ/𝜕𝑤
𝑏′ = 𝑏 − 𝜂 𝜕ℒ/𝜕𝑏
方式循环更新参数。
"""
