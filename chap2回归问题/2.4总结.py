# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/13 16:33
"""
简单回顾一下我们的探索之路：
首先假设𝑛个输入的生物神经元的数学模型为线性模型之后，只采样𝑛 + 1个数据点就可以估计线性模型的参数𝒘和𝑏。
引入观测误差后，通过梯度下降算法，我们可以采样多组数据点循环优化得到𝒘和𝑏的数值解。

如果我们换一个角度来看待这个问题，它其实可以理解为一组连续值(向量)的预测问题。
给定数据集𝔻，我们需要从𝔻中学习到数据的真实模型，从而预测未见过的样本的输出值。
在假定模型的类型后，学习过程就变成了搜索模型参数的问题，比如我们假设神经元为线性模型，
那么训练过程即为搜索线性模型的𝒘和𝑏参数的过程。训练完成后，利用学到的模型，对于任意的新输入𝒙，
我们就可以使用学习模型输出值作为真实值的近似。从这个角度来看，它就是一个连续值的预测问题。

在现实生活中，连续值预测问题是非常常见的，比如股价的走势预测、天气预报中温度和湿度等的预测、年龄的预测、交通流量的预测等。
对于预测值是连续的实数范围，或者属于某一段连续的实数区间，我们把这种问题称为回归(Regression)问题。特别地，如果使用
线性模型去逼近真实模型，那么我们把这一类方法叫做线性回归(Linear Regression，简称 LR)，线性回归是回归问题中的一种具体的实现。

除了连续值预测问题以外，是不是还有离散值预测问题呢？比如说硬币正反面的预测，它的预测值𝑦只可能有正面或反面两种可能；
再比如说给定一张图片，这张图片中物体的类别也只可能是像猫、狗、天空之类的离散类别值。对于这一类问题，
我们把它称为分类(Classification)问题。
"""