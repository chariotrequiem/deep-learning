# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/26 17:07
"""
15.4.1迁移学习原理
迁移学习(Transfer Learning)是机器学习的一个研究方向，主要研究如何将任务 A 上面学习到的知识迁移到任务 B 上，以提高在任务 B 上的泛化性能。
例如任务 A 为猫狗分类问题，需要训练一个分类器能够较好的分辨猫和狗的样本图片，任务 B 为牛羊分类问题。
可以发现，任务 A 和任务 B 存在大量的共享知识，比如这些动物都可以从毛发、体型、形 态、发色等方面进行辨别。因此在任务 A 训练获得的分类器已经掌握了这部份知识，
在训练任务 B 的分类器时，可以不从零开始训练，而是在任务 A 上获得的知识的基础上面进行训练或微调(Fine-tuning)，这和“站在巨人的肩膀上”思想非常类似。
通过迁移任务 A 上学习的知识，在任务 B 上训练分类器可以使用更少的样本和更少的训练代价，并且获得不错的泛化能力。

我们介绍一种比较简单，但是非常常用的迁移学习方法：网络微调技术。对于卷积神经网络，一般认为它能够逐层提取特征，越末层的网络的抽象特征提取能力越强，
输出层一般使用与类别数相同输出节点的全连接层，作为分类网络的概率分布预测。对于相似的任务 A 和 B，如果它们的特征提取方法是相近的，
则网络的前面数层可以重用，网络后面的数层可以根据具体的任务设定从零开始训练。

如图 15.6 所示，左边的网络在任务 A 上面训练，学习到任务 A 的知识，迁移到任务B 时，可以重用网络模型的前面数层的参数，并将后面数层替换为新的网络，
并从零开始训练。我们把在任务 A 上面训练好的模型叫做预训练模型，
对于图片分类来说，在 ImageNet 数据集上面预训练的模型是一个较好的选择。


15.4.2迁移学习实战
我们在 DenseNet121 的基础上，使用在 ImageNet 数据集上预训练好的模型参数初始化DenseNet121 网络，并去除最后一个全连接层，
追加新的分类子网络，最后一层的输出节点数设置为 5。代码如下：

# 加载 DenseNet 网络模型，并去掉最后一层全连接层，最后一个池化层设置为 max pooling
# 并使用预训练的参数初始化
net = keras.applications.DenseNet121(weights='imagenet', include_top=False,pooling='max')
# 设计为不参与优化，即 DenseNet 这部分参数固定不动
net.trainable = False
newnet = keras.Sequential([
    net, # 去掉最后一层的 DenseNet121
    layers.Dense(1024, activation='relu'), # 追加全连接层
    layers.BatchNormalization(), # 追加 BN 层
    layers.Dropout(rate=0.5), # 追加 Dropout 层，防止过拟合
    layers.Dense(5) # 根据宝可梦数据的任务，设置最后一层输出节点数为 5
])
newnet.build(input_shape=(4,224,224,3))
newnet.summary()

上述代码在创建 DenseNet121 时，通过设置 weights='imagenet'参数可以返回预训练的DenseNet121 模型对象，
并通过 Sequential 容器将重用的网络层与新的子分类网络重新封装为一个新模型 newnet。在微调阶段，
可以通过设置 net.trainable = False 来固定 DenseNet121部分网络的参数，即 DenseNet121 部分网络不需要更新参数，
从而只需要训练新添加的子分类网络部分，大大减少了实际参与训练的参数量。当然也可以通过设置 net.trainable = True，
像正常的网络一样训练全部参数量。即使如此，由于重用部分网络已经学习到良好的参数状态，网络依然可以快速收敛到较好性能。

基于预训练的 DenseNet121 网络模型，我们将训练准确率、验证准确率和测试准确率绘制为曲线图，如图 15.7 所示。和从零开始训练相比，
借助于迁移学习，网络只需要少量样本即可训练到较好的性能，提升十分显著。
"""