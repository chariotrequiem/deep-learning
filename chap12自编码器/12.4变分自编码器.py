# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/25 10:42
"""
基本的自编码器本质上是学习输入𝒙和隐藏变量𝒛之间映射关系，它是一个判别模型(Discriminative model)，并不是生成模型(Generative model)。
那么能不能将自编码器调整为生成模型，方便地生成样本呢？

给定隐藏变量的分布P(𝒛)，如果可以学习到条件概率分布P(𝒙|𝒛)，则通过对联合概率分布P(𝒙, 𝒛) = P(𝒙|𝒛)P(𝒛)进行采样，生成不同的样本。
变分自编码器(Variational Auto-Encoders，简称 VAE)就可以实现此目的，如图 12.11 所示。如果从神经网络的角度来理解的话，VAE 和前面的自编码器一样，
非常直观好理解；但是 VAE 的理论推导稍复杂，接下来我们先从神经网络的角度去阐述 VAE，再从概率角度去推导 VAE。

从神经网络的角度来看，VAE 相对于自编码器模型，同样具有编码器和解码器两个子网络。解码器接受输入𝒙，输出为隐变量𝒛；解码器负责将隐变量𝒛解码为重建的𝒙 。
不同的是，VAE 模型对隐变量𝒛的分布有显式地约束，希望隐变量𝒛符合预设的先验分布P(𝒛)。因此，在损失函数的设计上，除了原有的重建误差项外，
还添加了隐变量𝒛分布的约束项。

12.4.1VAE原理
从概率的角度，我们假设任何数据集都采样自某个分布𝑝(𝒙|𝒛)，𝒛是隐藏变量，代表了某种内部特征，比如手写数字的图片𝒙，
𝒛可以表示字体的大小、书写风格、加粗、斜体等设定，它符合某个先验分布𝑝(𝒛)，在给定具体隐藏变量𝒛的情况下，
我们可以从学到了分布𝑝(𝒙|𝒛)中采样一系列的生成样本，这些样本都具有𝒛所表示的共性。

通常可以假设𝑝(𝒛)符合已知的分布，比如𝒩(0,1)。在𝑝(𝒛)已知的条件下，我们的目的就是希望能学会生成概率模型𝑝(𝒙|𝒛)。
这里可以采用最大似然估计(Maximum Likelihood Estimation)方法：一个好的模型，应该拥有很大的概率生成真实的样本𝒙 ∈ 𝔻。
如果我们的生成模型𝑝(𝒙|𝒛)是用𝜃来参数化，那么我们的神经网络的优化目标是：
                                        max 𝑝 (𝒙) = ∫𝒛 𝑝(𝒙|𝒛)𝑝(𝒛)𝑑𝒛
很遗憾的是，由于𝒛是连续变量，上述积分没法转换为离散形式，导致很难直接优化。

换一个思路，利用变分推断(Variational Inference)的思想，我们通过分布𝑞 (𝒛|𝒙)来逼近𝑝(𝒛|𝒙)，即需要最小化𝑞 (𝒛|𝒙)与𝑝(𝒛|𝒙)之间的距离：
                                    min 𝔻𝐾𝐿 (𝑞 (𝒛|𝒙)||𝑝(𝒛|𝒙))
其中 KL 散度𝔻𝐾𝐿是一种衡量分布𝑞和𝑝之间的差距的度量，定义为：
                             𝔻𝐾𝐿(𝑞||𝑝) = ∫𝑞(𝒙) log (𝑞(𝒙) /𝑝(𝒙) )𝑑𝒙
严格地说，距离一般是对称的，而 KL 散度并不对称。将 KL 散度展开为
𝔻𝐾𝐿 (𝑞 (𝒛|𝒙)||𝑝(𝒛|𝒙)) = ∫𝒛 𝑞 (𝒛|𝒙) log (𝑞(𝒙) /𝑝(𝒙) )𝑑𝒛
利用性质
                                        𝑝(𝒛|𝒙) ∙ 𝑝(𝒙) = 𝑝(𝒙, 𝒛)
可以得到
                            𝔻𝐾𝐿 (𝑞 (𝒛|𝒙)||𝑝(𝒛|𝒙)) = ∫𝒛 𝑞 (𝒛|𝒙) log [(𝑞(𝒛|𝒙) 𝑝(𝒙)) / 𝑝(𝒙, 𝒛) ] 𝑑𝒛
                            = ∫ 𝑞 (𝒛|𝒙) log[𝑞 (𝒛|𝒙) / 𝑝(𝒙, 𝒛)] d𝒛 + ∫𝒛𝑞 (𝒛|𝒙) log 𝑝 (𝒙)𝑑𝒛

"""