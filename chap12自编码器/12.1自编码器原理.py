# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/24 21:46
"""
让我们来考虑有监督学习中神经网络的功能：
                    𝒐 = 𝑓𝜃(𝒙), 𝒙 ∈ 𝑅𝑑in, 𝒐 ∈ 𝑅𝑑out
𝑑in是输入的特征向量长度，𝑑out是网络输出的向量长度。对于分类问题，网络模型通过把长度为𝑑in输入特征向量𝒙变换到长度为𝑑out的输出向量𝒐，
这个过程可以看成是特征降维的过程，把原始的高维输入向量𝒙变换到低维的变量𝒐。特征降维(Dimensionality Reduction)在机器学习中有广泛的应用，
比如文件压缩(Compression)、数据预处理(Preprocessing)等。最常见的降维算法有主成分分析法(Principal components analysis，简称 PCA)，
通过对协方差矩阵进行特征分解而得到数据的主要成分，但是 PCA 本质上是一种线性变换，提取特征的能力极为有限。

那么能不能利用神经网络的强大非线性表达能力去学习到低维的数据表示呢？问题的关键在于，训练神经网络一般需要一个显式的标签数据(或监督信号)，
但是无监督的数据没有额外的标注信息，只有数据𝒙本身。

于是，我们尝试着利用数据𝒙本身作为监督信号来指导网络的训练，即希望神经网络能够学习到映射𝑓𝜃: 𝒙 → 𝒙。我们把网络𝑓𝜃切分为两个部分，
前面的子网络尝试学习映射关系:𝑔𝜃1: 𝒙 → 𝒛，后面的子网络尝试学习映射关系ℎ𝜃2: 𝒛 → 𝒙，如图 12.1 所示。
我们把𝑔𝜃1看成一个数据编码(Encode)的过程，把高维度的输入𝒙编码成低维度的隐变量𝒛(Latent Variable，或隐藏变量)，称为 Encoder 网络(编码器)；
ℎ𝜃2看成数据解码(Decode)的过程，把编码过后的输入𝒛解码为高维度的𝒙，称为 Decoder 网络(解码器)。

编码器和解码器共同完成了输入数据𝒙的编码和解码过程，我们把整个网络模型𝑓𝜃叫做自动编码器(Auto-Encoder)，简称自编码器。
如果使用深层神经网络来参数化𝑔𝜃1和ℎ𝜃2函数，则 称为深度自编码器(Deep Auto-encoder)，如图 12.2 所示。

自编码器能够将输入变换到隐藏向量𝒛，并通过解码器重建(Reconstruct，或恢复)出𝒙 。我们希望解码器的输出能够完美地或者近似恢复出原来的输入，
即𝒙 ≈ 𝒙，那么，自编码器的优化目标可以写成：
                                    Minimize ℒ = dist(𝒙, 𝒙 )
                                        𝒙 = ℎ𝜃2(𝑔𝜃1(𝒙))
其中dist(𝒙, 𝒙 )表示 𝒙和𝒙 的距离度量，称为重建误差函数。最常见的度量方法有欧氏距离(Euclidean distance)的平方，计算方法如下：
                                    ℒ = ∑(𝑥𝑖 − 𝑥𝑖)2
它和均方误差原理上是等价的。自编码器网络和普通的神经网络并没有本质的区别，只不过训练的监督信号由标签𝒚变成了自身𝒙。
借助于深层神经网络的非线性特征提取能力，自编码器可以获得良好的数据表示，相对于 PCA 等线性方法，自编码器性能更加优秀，
甚至可以更加完美的恢复出输入𝒙。

在图 12.3(a)中，第 1 行是随机采样自测试集的真实 MNIST 手写数字图片，第 2、3、 4 行分别是基于长度为 30 的隐藏向量，
使用自编码器、Logistic PCA 和标准 PCA 算法恢复出的重建样本图片；在图 12.3(b)中，第 1 行为真实的人像图片，
第 2、3 行分别是基于长度为 30 的隐藏向量，使用自编码器和标准 PCA 算法恢复出的重建样本。
可以看到，使用深层神经网络的自编码器重建出图片相对清晰，还原度较高，而 PCA 算法重建出的图片较模糊。
"""