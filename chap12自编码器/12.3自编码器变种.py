# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/25 9:52
"""
一般而言，自编码器网络的训练较为稳定，但是由于损失函数是直接度量重建样本与真实样本的底层特征之间的距离，
而不是评价重建样本的逼真度和多样性等抽象指标，因此在某些任务上效果一般，如图片重建，容易出现重建图片边缘模糊，
逼真度相对真实图片仍有不小差距。为了尝试让自编码器学习到数据的真实分布，产生了一系列的自编码器变种网络。
下面将介绍几种典型的自编码器变种模型。

12.3.1Denoising Auto-Encoder
为了防止神经网络记忆住输入数据的底层特征，Denoising Auto-Encoders 给输入数据添加随机的噪声扰动，
如给输入𝒙添加采样自高斯分布的噪声𝜀：
                                        𝑥̃ = 𝑥 + 𝜀, 𝜀~𝒩(0, var)
添加噪声后，网络需要从𝒙̃学习到数据的真实隐藏变量 z，并还原出原始的输入𝒙，如图12.9 所示。
模型的优化目标为：
                                    𝜃∗ = argmin dist(ℎ𝜃2(𝑔𝜃1(𝒙̃)), 𝒙)

12.3.2Dropout Auto-Encoder
自编码器网络同样面临过拟合的风险，Dropout Auto-Encoder 通过随机断开网络的连接来减少网络的表达能力，防止过拟合。
Dropout Auto-Encoder 实现非常简单，通过在网络层中插入 Dropout 层即可实现网络连接的随机断开。

12.3.3Adversarial Auto-Encoder
为了能够方便地从某个已知的先验分布中𝑝(𝒛)采样隐藏变量𝒛，方便利用𝑝(𝒛)来重建输 入，
对抗自编码器(Adversarial Auto-Encoder)利用额外的判别器网络(Discriminator，简称 D网络)来判定降维的隐藏变量𝒛是否采样自先验分布𝑝(𝒛)，
如图 12.10 所示。判别器网络的输出为一个属于[0,1]区间的变量，表征隐藏向量是否采样自先验分布𝑝(𝒛)：
所有采样自先验分布𝑝(𝒛)的𝒛标注为真，采样自编码器的条件概率𝑞(𝒛|𝒙)的𝒛标注为假。
通过这种方式训练，除了可以重建样本，还可以约束条件概率分布𝑞(𝒛|𝒙)逼近先验分布𝑝(𝒛)。

对抗自编码器是从下一章要介绍的生成对抗网络算法衍生而来，在学习完对抗生成网络后可以加深对对抗自编码器的理解
"""