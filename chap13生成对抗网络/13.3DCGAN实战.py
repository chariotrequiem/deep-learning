# å½“å‰ç‰ˆæœ¬ ï¼š python3.7.11
# å¼€å‘æ—¶é—´ ï¼š 2021/9/26 14:47
"""
æœ¬èŠ‚æˆ‘ä»¬æ¥å®Œæˆä¸€ä¸ªäºŒæ¬¡å…ƒåŠ¨æ¼«å¤´åƒå›¾ç‰‡ç”Ÿæˆå®æˆ˜ï¼Œå‚è€ƒ DCGAN [2]çš„ç½‘ç»œç»“æ„ï¼Œå…¶ä¸­åˆ¤åˆ«å™¨ D åˆ©ç”¨æ™®é€šå·ç§¯å±‚å®ç°ï¼Œç”Ÿæˆå™¨ G åˆ©ç”¨è½¬ç½®å·ç§¯å±‚å®ç°ï¼Œå¦‚å›¾ 13.6 æ‰€ç¤ºã€‚

13.3.1åŠ¨æ¼«å›¾ç‰‡æ•°æ®é›†
è¿™é‡Œä½¿ç”¨çš„æ˜¯ä¸€ç»„äºŒæ¬¡å…ƒåŠ¨æ¼«å¤´åƒçš„æ•°æ®é›†â‘¡ï¼Œå…± 51223 å¼ å›¾ç‰‡ï¼Œæ— æ ‡æ³¨ä¿¡æ¯ï¼Œå›¾ç‰‡ä¸» ä½“å·²è£å‰ªã€å¯¹é½å¹¶ç»Ÿä¸€ç¼©æ”¾åˆ°96 Ã— 96å¤§å°ï¼Œéƒ¨åˆ†æ ·ç‰‡å¦‚å›¾ 13.7 æ‰€ç¤º
å¯¹äºè‡ªå®šä¹‰çš„æ•°æ®é›†ï¼Œéœ€è¦è‡ªè¡Œå®Œæˆæ•°æ®çš„åŠ è½½å’Œé¢„å¤„ç†å·¥ä½œï¼Œæˆ‘ä»¬è¿™é‡Œèšç„¦åœ¨ GANç®—æ³•æœ¬èº«ï¼Œåç»­è‡ªå®šä¹‰æ•°æ®é›†ä¸€ç« ä¼šè¯¦ç»†ä»‹ç»å¦‚ä½•åŠ è½½è‡ªå·±çš„æ•°æ®é›†ï¼Œ
è¿™é‡Œç›´æ¥é€šè¿‡é¢„ç¼–å†™å¥½çš„ make_anime_dataset å‡½æ•°è¿”å›å·²ç»å¤„ç†å¥½çš„æ•°æ®é›†å¯¹è±¡ã€‚ä»£ç å¦‚ä¸‹ï¼š
# æ•°æ®é›†è·¯å¾„ï¼Œä» https://pan.baidu.com/s/1eSifHcA æå–ç ï¼šg5qa ä¸‹è½½è§£å‹
img_path = glob.glob(r'C:\Users\z390\Downloads\faces\*.jpg')
# æ„å»ºæ•°æ®é›†å¯¹è±¡ï¼Œè¿”å›æ•°æ®é›† Dataset ç±»å’Œå›¾ç‰‡å¤§å°
dataset, img_shape, _ = make_anime_dataset(img_path, batch_size, resize=64)

å…¶ä¸­ dataset å¯¹è±¡å°±æ˜¯ tf.data.Dataset ç±»å®ä¾‹ï¼Œå·²ç»å®Œæˆäº†éšæœºæ‰“æ•£ã€é¢„å¤„ç†å’Œæ‰¹é‡åŒ–ç­‰æ“ä½œï¼Œå¯ä»¥ç›´æ¥è¿­ä»£è·å¾—æ ·æœ¬æ‰¹ï¼Œimg_shape æ˜¯é¢„å¤„ç†åçš„å›¾ç‰‡å¤§å°ã€‚

13.3.2ç”Ÿæˆå™¨
ç”Ÿæˆç½‘ç»œ G ç”± 5 ä¸ªè½¬ç½®å·ç§¯å±‚å•å…ƒå †å è€Œæˆï¼Œå®ç°ç‰¹å¾å›¾é«˜å®½çš„å±‚å±‚æ”¾å¤§ï¼Œç‰¹å¾å›¾é€šé“æ•°çš„å±‚å±‚å‡å°‘ã€‚
é¦–å…ˆå°†é•¿åº¦ä¸º 100 çš„éšè—å‘é‡ğ’›é€šè¿‡ Reshape æ“ä½œè°ƒæ•´ä¸º[ğ‘, 1,1,100]çš„ 4ç»´å¼ é‡ï¼Œå¹¶ä¾åºé€šè¿‡è½¬ç½®å·ç§¯å±‚ï¼Œæ”¾å¤§é«˜å®½ç»´åº¦ï¼Œå‡å°‘é€šé“æ•°ç»´åº¦ï¼Œ
æœ€åå¾—åˆ°é«˜å®½ä¸º 64ï¼Œé€šé“æ•°ä¸º 3 çš„å½©è‰²å›¾ç‰‡ã€‚æ¯ä¸ªå·ç§¯å±‚ä¸­é—´æ’å…¥ BN å±‚æ¥æé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå·ç§¯å±‚é€‰æ‹©ä¸ä½¿ç”¨åç½®å‘é‡ã€‚
ç”Ÿæˆå™¨çš„ç±»ä»£ç å®ç°å¦‚ä¸‹ï¼š
class Generator(keras.Model):
    # ç”Ÿæˆå™¨ç½‘ç»œç±»
    def __init__(self):
        super(Generator, self).__init__()
        filter = 64
        # è½¬ç½®å·ç§¯å±‚ 1,è¾“å‡º channel ä¸º filter*8,æ ¸å¤§å° 4,æ­¥é•¿ 1,ä¸ä½¿ç”¨ padding,ä¸ä½¿ç”¨åç½®
        self.conv1 = layers.Conv2DTranspose(filter*8, 4,1, 'valid', use_bias=False)
        self.bn1 = layers.BatchNormalization()
        # è½¬ç½®å·ç§¯å±‚ 2
        self.conv2 = layers.Conv2DTranspose(filter*4, 4,2, 'same', use_bias=False)
        self.bn2 = layers.BatchNormalization()
        # è½¬ç½®å·ç§¯å±‚ 3
        self.conv3 = layers.Conv2DTranspose(filter*2, 4,2, 'same', use_bias=False)
        self.bn3 = layers.BatchNormalization()
        # è½¬ç½®å·ç§¯å±‚ 4
        self.conv4 = layers.Conv2DTranspose(filter*1, 4,2, 'same', use_bias=False)
        self.bn4 = layers.BatchNormalization()
        # è½¬ç½®å·ç§¯å±‚ 5
        self.conv5 = layers.Conv2DTranspose(3, 4,2, 'same', use_bias=False)

    ç”Ÿæˆç½‘ç»œ G çš„å‰å‘ä¼ æ’­è¿‡ç¨‹å®ç°å¦‚ä¸‹ï¼š
    def call(self, inputs, training=None):
        x = inputs  # [z, 100]
        # Reshape æˆ 4D å¼ é‡ï¼Œæ–¹ä¾¿åç»­è½¬ç½®å·ç§¯è¿ç®—:(b, 1, 1, 100)
        x = tf.reshape(x, (x.shape[0], 1, 1, x.shape[1]))
        x = tf.nn.relu(x) # æ¿€æ´»å‡½æ•°
        # è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(b, 4, 4, 512)
        x = tf.nn.relu(self.bn1(self.conv1(x), training=training))
        # è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(b, 8, 8, 256)
        x = tf.nn.relu(self.bn2(self.conv2(x), training=training))
        # è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(b, 16, 16, 128)
        x = tf.nn.relu(self.bn3(self.conv3(x), training=training))
        # è½¬ç½®å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(b, 32, 32, 64)
        x = tf.nn.relu(self.bn4(self.conv4(x), training=training))
        # è½¬ç½®å·ç§¯-æ¿€æ´»å‡½æ•°:(b, 64, 64, 3)
        x = self.conv5(x)
        x = tf.tanh(x) # è¾“å‡º x èŒƒå›´-1~1,ä¸é¢„å¤„ç†ä¸€è‡´

        return x
ç”Ÿæˆç½‘ç»œçš„è¾“å‡ºå¤§å°ä¸º[ğ‘, 64,64,3]çš„å›¾ç‰‡å¼ é‡ï¼Œæ•°å€¼èŒƒå›´ä¸ºâˆ’1~1ã€‚

13.3.3åˆ¤åˆ«å™¨
åˆ¤åˆ«ç½‘ç»œ D ä¸æ™®é€šçš„åˆ†ç±»ç½‘ç»œç›¸åŒï¼Œæ¥å—å¤§å°ä¸º[ğ‘, 64,64,3]çš„å›¾ç‰‡å¼ é‡ï¼Œè¿ç»­é€šè¿‡ 5ä¸ªå·ç§¯å±‚å®ç°ç‰¹å¾çš„å±‚å±‚æå–ï¼Œå·ç§¯å±‚æœ€ç»ˆè¾“å‡ºå¤§å°ä¸º[ğ‘, 2,2,1024]ï¼Œ
å†é€šè¿‡æ± åŒ–å±‚GlobalAveragePooling2D å°†ç‰¹å¾å¤§å°è½¬æ¢ä¸º[ğ‘, 1024]ï¼Œæœ€åé€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚è·å¾—äºŒåˆ†ç±»ä»»åŠ¡çš„æ¦‚ç‡ã€‚
åˆ¤åˆ«ç½‘ç»œ D ç±»çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š
class Discriminator(keras.Model):
    # åˆ¤åˆ«å™¨ç±»
    def __init__(self):
        super(Discriminator, self).__init__()
        filter = 64
        # å·ç§¯å±‚ 1
        self.conv1 = layers.Conv2D(filter, 4, 2, 'valid', use_bias=False)
        self.bn1 = layers.BatchNormalization()
        # å·ç§¯å±‚ 2
        self.conv2 = layers.Conv2D(filter*2, 4, 2, 'valid', use_bias=False)
        self.bn2 = layers.BatchNormalization()
        # å·ç§¯å±‚ 3
        self.conv3 = layers.Conv2D(filter*4, 4, 2, 'valid', use_bias=False)
        self.bn3 = layers.BatchNormalization()
        # å·ç§¯å±‚ 4
        self.conv4 = layers.Conv2D(filter*8, 3, 1, 'valid', use_bias=False)
        self.bn4 = layers.BatchNormalization()
        # å·ç§¯å±‚ 5
        self.conv5 = layers.Conv2D(filter*16, 3, 1, 'valid', use_bias=False)
        self.bn5 = layers.BatchNormalization()
        # å…¨å±€æ± åŒ–å±‚
        self.pool = layers.GlobalAveragePooling2D()
        # ç‰¹å¾æ‰“å¹³å±‚
        self.flatten = layers.Flatten()
        # 2 åˆ†ç±»å…¨è¿æ¥å±‚
        self.fc = layers.Dense(1)
åˆ¤åˆ«å™¨ D çš„å‰å‘è®¡ç®—è¿‡ç¨‹å®ç°å¦‚ä¸‹ï¼š
    def call(self, inputs, training=None):
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 31, 31, 64)
        x = tf.nn.leaky_relu(self.bn1(self.conv1(inputs), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 14, 14, 128)
        x = tf.nn.leaky_relu(self.bn2(self.conv2(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 6, 6, 256)
        x = tf.nn.leaky_relu(self.bn3(self.conv3(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 4, 4, 512)
        x = tf.nn.leaky_relu(self.bn4(self.conv4(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 2, 2, 1024)
        x = tf.nn.leaky_relu(self.bn5(self.conv5(x), training=training))
        # å·ç§¯-BN-æ¿€æ´»å‡½æ•°:(4, 1024)
        x = self.pool(x)
        # æ‰“å¹³
        x = self.flatten(x)
        # è¾“å‡ºï¼Œ[b, 1024] => [b, 1]
        logits = self.fc(x)

        return logits
åˆ¤åˆ«å™¨çš„è¾“å‡ºå¤§å°ä¸º[ğ‘, 1]ï¼Œç±»å†…éƒ¨æ²¡æœ‰ä½¿ç”¨ Sigmoid æ¿€æ´»å‡½æ•°ï¼Œé€šè¿‡ Sigmoid æ¿€æ´»å‡½æ•°åå¯è·å¾—ğ‘ä¸ªæ ·æœ¬å±äºçœŸå®æ ·æœ¬çš„æ¦‚ç‡ ã€‚


13.3.4è®­ç»ƒå’Œå¯è§†åŒ–
åˆ¤åˆ«ç½‘ç»œ æ ¹æ®å¼(13-1)ï¼Œåˆ¤åˆ«ç½‘ç»œçš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–â„’(ğ·, ğº)å‡½æ•°ï¼Œä½¿å¾—çœŸå®æ ·æœ¬é¢„æµ‹ä¸ºçœŸçš„æ¦‚ç‡æ¥è¿‘äº 1ï¼Œç”Ÿæˆæ ·æœ¬é¢„æµ‹ä¸ºçœŸçš„æ¦‚ç‡æ¥è¿‘äº 0ã€‚
æˆ‘ä»¬å°†åˆ¤æ–­å™¨çš„è¯¯å·®å‡½æ•°å®ç°åœ¨ d_loss_fn å‡½æ•°ä¸­ï¼Œå°†æ‰€æœ‰çœŸå®æ ·æœ¬æ ‡æ³¨ä¸º 1ï¼Œæ‰€æœ‰ç”Ÿæˆæ ·æœ¬æ ‡æ³¨ä¸º 0ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–å¯¹åº”çš„äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å®ç°æœ€å¤§åŒ–â„’(ğ·,ğº)å‡½æ•°ã€‚
d_loss_fn å‡½æ•°å®ç°å¦‚ä¸‹ï¼š
def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):
    # è®¡ç®—åˆ¤åˆ«å™¨çš„è¯¯å·®å‡½æ•°
    # é‡‡æ ·ç”Ÿæˆå›¾ç‰‡
    fake_image = generator(batch_z, is_training)
    # åˆ¤å®šç”Ÿæˆå›¾ç‰‡
    d_fake_logits = discriminator(fake_image, is_training)
    # åˆ¤å®šçœŸå®å›¾ç‰‡
    d_real_logits = discriminator(batch_x, is_training)
    # çœŸå®å›¾ç‰‡ä¸ 1 ä¹‹é—´çš„è¯¯å·®
    d_loss_real = celoss_ones(d_real_logits)
    # ç”Ÿæˆå›¾ç‰‡ä¸ 0 ä¹‹é—´çš„è¯¯å·®
    d_loss_fake = celoss_zeros(d_fake_logits)
    # åˆå¹¶è¯¯å·®
    loss = d_loss_fake + d_loss_real

    return loss
å…¶ä¸­ celoss_ones å‡½æ•°è®¡ç®—å½“å‰é¢„æµ‹æ¦‚ç‡ä¸æ ‡ç­¾ 1 ä¹‹é—´çš„äº¤å‰ç†µæŸå¤±ï¼Œä»£ç å¦‚ä¸‹ï¼š
def celoss_ones(logits):
    # è®¡ç®—å±äºä¸æ ‡ç­¾ä¸º 1 çš„äº¤å‰ç†µ
    y = tf.ones_like(logits)
    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)
    return tf.reduce_mean(loss)

celoss_zeros å‡½æ•°è®¡ç®—å½“å‰é¢„æµ‹æ¦‚ç‡ä¸æ ‡ç­¾ 0 ä¹‹é—´çš„äº¤å‰ç†µæŸå¤±ï¼Œä»£ç å¦‚ä¸‹ï¼š
def celoss_zeros(logits):
    # è®¡ç®—å±äºä¸ä¾¿ç­¾ä¸º 0 çš„äº¤å‰ç†µ
    y = tf.zeros_like(logits)
    loss = keras.losses.binary_crossentropy(y, logits, from_logits=True)
    return tf.reduce_mean(loss)

ç”Ÿæˆç½‘ç»œ çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å°åŒ–â„’(ğ·, ğº)ç›®æ ‡å‡½æ•°ï¼Œç”±äºçœŸå®æ ·æœ¬ä¸ç”Ÿæˆå™¨æ— å…³ï¼Œå› æ­¤è¯¯å·®å‡½æ•°åªéœ€è¦è€ƒè™‘æœ€å°åŒ–ğ”¼ğ’›~ğ‘ğ‘§(âˆ™)log (1 âˆ’ ğ·ğœƒ(ğºğœ™(ğ’›)))é¡¹å³å¯ã€‚
å¯ä»¥é€šè¿‡å°†ç”Ÿæˆçš„æ ·æœ¬æ ‡æ³¨ä¸º 1ï¼Œæœ€å°åŒ–æ­¤æ—¶çš„äº¤å‰ç†µè¯¯å·®ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨åå‘ä¼ æ’­è¯¯å·®çš„è¿‡ç¨‹ä¸­ï¼Œåˆ¤åˆ«å™¨ä¹Ÿå‚ä¸äº†è®¡ç®—å›¾çš„æ„å»ºï¼Œ
ä½†æ˜¯æ­¤é˜¶æ®µåªéœ€è¦æ›´æ–°ç”Ÿæˆå™¨ç½‘ç»œå‚æ•°ï¼Œè€Œä¸æ›´æ–°åˆ¤åˆ«å™¨çš„ç½‘ç»œå‚æ•°ã€‚
ç”Ÿæˆå™¨çš„è¯¯å·®å‡½æ•°ä»£ç å¦‚ä¸‹ï¼š
def g_loss_fn(generator, discriminator, batch_z, is_training):
    # é‡‡æ ·ç”Ÿæˆå›¾ç‰‡
    fake_image = generator(batch_z, is_training)
    # åœ¨è®­ç»ƒç”Ÿæˆç½‘ç»œæ—¶ï¼Œéœ€è¦è¿«ä½¿ç”Ÿæˆå›¾ç‰‡åˆ¤å®šä¸ºçœŸ
    d_fake_logits = discriminator(fake_image, is_training)
    # è®¡ç®—ç”Ÿæˆå›¾ç‰‡ä¸ 1 ä¹‹é—´çš„è¯¯å·®
    loss = celoss_ones(d_fake_logits)

    return loss

ç½‘ç»œè®­ç»ƒ åœ¨æ¯ä¸ª Epochï¼Œé¦–å…ˆä»å…ˆéªŒåˆ†å¸ƒğ‘ (âˆ™)ä¸­éšæœºé‡‡æ ·éšè—å‘é‡ï¼Œä»çœŸå®æ•°æ®é›†ä¸­éšæœºé‡‡æ ·çœŸå®å›¾ç‰‡ï¼Œé€šè¿‡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨è®¡ç®—åˆ¤åˆ«å™¨ç½‘ç»œçš„æŸå¤±ï¼Œ
å¹¶ä¼˜åŒ–åˆ¤åˆ«å™¨ç½‘ç»œå‚æ•°ğœƒã€‚åœ¨è®­ç»ƒç”Ÿæˆå™¨æ—¶ï¼Œéœ€è¦å€ŸåŠ©äºåˆ¤åˆ«å™¨æ¥è®¡ç®—è¯¯å·®ï¼Œä½†æ˜¯åªè®¡ç®—ç”Ÿæˆå™¨çš„æ¢¯åº¦ä¿¡æ¯å¹¶æ›´æ–°ğœ™ã€‚è¿™é‡Œè®¾å®šåˆ¤åˆ«å™¨è®­ç»ƒğ‘˜ = 5æ¬¡åï¼Œç”Ÿæˆå™¨è®­ç»ƒä¸€æ¬¡ã€‚

é¦–å…ˆåˆ›å»ºç”Ÿæˆç½‘ç»œå’Œåˆ¤åˆ«ç½‘ç»œï¼Œå¹¶åˆ†åˆ«åˆ›å»ºå¯¹åº”çš„ä¼˜åŒ–å™¨ã€‚
ä»£ç å¦‚ä¸‹ï¼š
generator = Generator() # åˆ›å»ºç”Ÿæˆå™¨
generator.build(input_shape = (4, z_dim))
discriminator = Discriminator() # åˆ›å»ºåˆ¤åˆ«å™¨
discriminator.build(input_shape=(4, 64, 64, 3))
# åˆ†åˆ«ä¸ºç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨åˆ›å»ºä¼˜åŒ–å™¨
g_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1= 0.5)
d_optimizer = keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.5)

ä¸»è®­ç»ƒéƒ¨åˆ†ä»£ç å®ç°å¦‚ä¸‹ï¼š
for epoch in range(epochs): # è®­ç»ƒ epochs æ¬¡
    # 1. è®­ç»ƒåˆ¤åˆ«å™¨
    for _ in range(5):
        # é‡‡æ ·éšè—å‘é‡
        batch_z = tf.random.normal([batch_size, z_dim])
        batch_x = next(db_iter) # é‡‡æ ·çœŸå®å›¾ç‰‡
        # åˆ¤åˆ«å™¨å‰å‘è®¡ç®—
        with tf.GradientTape() as tape:
            d_loss = d_loss_fn(generator, discriminator, batch_z, batch_x, is_training)
            grads = tape.gradient(d_loss, discriminator.trainable_variables)
            d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))

    # 2. è®­ç»ƒç”Ÿæˆå™¨
    # é‡‡æ ·éšè—å‘é‡
    batch_z = tf.random.normal([batch_size, z_dim])
    batch_x = next(db_iter) # é‡‡æ ·çœŸå®å›¾ç‰‡
    # ç”Ÿæˆå™¨å‰å‘è®¡ç®—
    with tf.GradientTape() as tape:
        g_loss = g_loss_fn(generator, discriminator, batch_z, is_training)
    grads = tape.gradient(g_loss, generator.trainable_variables)
    g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))

æ¯é—´éš” 100 ä¸ª Epochï¼Œè¿›è¡Œä¸€æ¬¡å›¾ç‰‡ç”Ÿæˆæµ‹è¯•ã€‚é€šè¿‡ä»å…ˆéªŒåˆ†å¸ƒä¸­éšæœºé‡‡æ ·éšå‘é‡ï¼Œé€å…¥ç”Ÿæˆå™¨è·å¾—ç”Ÿæˆå›¾ç‰‡ï¼Œå¹¶ä¿å­˜ä¸ºæ–‡ä»¶ã€‚
å¦‚å›¾ 13.8 æ‰€ç¤ºï¼Œå±•ç¤ºäº† DCGAN æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„ç”Ÿæˆå›¾ç‰‡æ ·ä¾‹ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ï¼Œå¤§éƒ¨åˆ†å›¾ç‰‡ä¸»ä½“æ˜ç¡®ï¼Œè‰²å½©é€¼çœŸï¼Œå›¾ç‰‡å¤šæ ·æ€§è¾ƒä¸°å¯Œï¼Œ
å›¾ç‰‡æ•ˆæœè¾ƒä¸ºè´´è¿‘æ•°æ®é›†ä¸­çœŸå®çš„å›¾ç‰‡ã€‚åŒæ—¶ä¹Ÿèƒ½å‘ç°ä»æœ‰å°‘é‡ç”Ÿæˆå›¾ç‰‡æŸåï¼Œæ— æ³•é€šè¿‡äººçœ¼è¾¨è¯†å›¾ç‰‡ä¸»ä½“ã€‚


"""