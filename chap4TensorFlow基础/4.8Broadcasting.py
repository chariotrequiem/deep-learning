# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/14 21:15
"""
Broadcasting 称为广播机制(或自动扩展机制)，它是一种轻量级的张量复制手段，在逻辑上扩展张量数据的形状，
但是只会在需要时才会执行实际存储复制操作。对于大部分场景，Broadcasting 机制都能通过优化手段避免实际复制数据而完成逻辑运算，
从而相对于tf.tile 函数，减少了大量计算代价。

对于所有长度为 1 的维度，Broadcasting 的效果和 tf.tile 一样，都能在此维度上逻辑复制数据若干份，
区别在于 tf.tile 会创建一个新的张量，执行复制 IO 操作，并保存复制后的张量数据，而 Broadcasting 并不会立即复制数据，
它会在逻辑上改变张量的形状，使得视图上变成了复制后的形状。
Broadcasting 会通过深度学习框架的优化手段避免实际复制数据而完成逻辑运算，至于怎么实现的用户不必关心，对于用户来说，
Broadcasting 和 tf.tile 复制的最终效果是一样的，操作对用户透明，但是 Broadcasting 机制节省了大量计算资源，
建议在运算过程中尽可能地利用 Broadcasting 机制提高计算效率。

继续考虑上述的𝒀 = 𝑿@𝑾 + 𝒃的例子，𝑿@𝑾的 shape 为[2,3]，𝒃的 shape 为[3]，我们可以通过结合 tf.expand_dims 和 tf.tile
手动完成复制数据操作，将𝒃变换为[2,3]，然后与𝑿@𝑾完成相加运算。但实际上，直接将 shape 为[2,3]与[3]的𝒃相加也是合法的，
例如：
import tensorflow as tf
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '/gpu:0'

x = tf.random.normal([2, 4])
w = tf.random.normal([4, 3])
b = tf.random.normal([3])
y = x@w+b  # 不同 shape 的张量直接相加
print(y)
上述加法并没有发生逻辑错误，那么它是怎么实现的呢？这是因为它自动调用 Broadcasting函数 tf.broadcast_to(x, new_shape)，
将两者 shape 扩张为相同的[2,3]，即上式可以等效为：
y = x@w + tf.broadcast_to(b,[2,3]) # 手动扩展，并相加
也就是说，操作符+在遇到 shape 不一致的 2 个张量时，会自动考虑将 2 个张量自动扩展到一致的 shape，
然后再调用 tf.add 完成张量相加运算，这也就解释了我们之前一直存在的困惑。通过自动调用 tf.broadcast_to(b, [2,3])的 Broadcasting 机制，
既实现了增加维度、复制数据的目的，又避免实际复制数据的昂贵计算代价，同时使书写更加简洁高效。

那么有了 Broadcasting 机制后，所有 shape 不一致的张量是不是都可以直接完成运算？显然，所有的运算都需要在正确逻辑下进行，
Broadcasting 机制并不会扰乱正常的计算逻辑，它只会针对于最常见的场景自动完成增加维度并复制数据的功能，提高开发效率和运行效率。
这种最常见的场景是什么呢？这就要说到 Broadcasting 设计的核心思想。

Broadcasting 机制的核心思想是普适性，即同一份数据能普遍适合于其他位置。在验证普适性之前，需要先将张量 shape 靠右对齐，
然后进行普适性判断：对于长度为 1 的维度，默认这个数据普遍适合于当前维度的其他位置；对于不存在的维度，
则在增加新维度后默认当前数据也是普适于新维度的，从而可以扩展为更多维度数、任意长度的张量形状。

考虑 shape 为[ w, 1]的张量𝑨，需要扩展为 shape：[𝑏, ℎ, w, 𝑐]，如图 4.7 所示，第一行为欲扩展的shape，第二行为现有的shape：
                      b      h     w    c
                     <---------    w    1 -> 长度为1，默认数据相同
                空维度，默认存在此维度且数据相同

首先将 2 个 shape 靠右对齐，对于通道维度𝑐，张量的现长度为 1，则默认此数据同样适合当前维度的其他位置，
将数据在逻辑上复制𝑐 − 1份，长度变为 c；对于不存在的𝑏和ℎ维度，则自动插入新维度，新维度长度为 1，
同时默认当前的数据普适于新维度的其他位置，即对于其它的图片、其它的行来说，与当前的这一行的数据完全一致。
这样将数据𝑏 和ℎ维度的长度自动扩展为𝑏和ℎ，如图 4.8 所示。(书p87)

通过 tf.broadcast_to(x, new_shape)函数可以显式地执行自动扩展功能，将现有 shape 扩张为 new_shape，实现如下：
In [87]:
A = tf.random.normal([32,1]) # 创建矩阵
tf.broadcast_to(A, [2,32,32,3]) # 扩展为 4D 张量
Out[87]:
<tf.Tensor: id=13, shape=(2, 32, 32, 3), dtype=float32, numpy=
array([[[[-1.7571245 , -1.7571245 , -1.7571245 ],
 [ 1.580159 , 1.580159 , 1.580159 ],
 [-1.5324328 , -1.5324328 , -1.5324328 ],...

可以看到，在普适性原则的指导下，Broadcasting 机制变得直观、好理解，它的设计是非常符合人的思维模式。
我们来考虑不满足普适性原则的例子，如下图 4.9 所示。
                      b      h     w    c
                                   w    2 -> 长度为2,不具备普适性
在c维度上，张量已经有2个特征数据，新shape对应维度的长度为c(c ≠ 2， 如c = 3)，那么当前维度上的这两个特征无法普适到其他位置，
故不满足普适性原则，无法应用Broadcasting 机制，将会触发错误，例如：
In [88]:
A = tf.random.normal([32,2])
tf.broadcast_to(A, [2,32,32,4]) # 不符合 Broadcasting 条件
Out[88]:
InvalidArgumentError: Incompatible shapes: [32,2] vs. [2,32,32,4]
[Op:BroadcastTo]

在进行张量运算时，有些运算在处理不同 shape 的张量时，会隐式地自动调用Broadcasting 机制，如+，-，*，/等运算等，
将参与运算的张量 Broadcasting 成一个公共shape，再进行相应的计算。
如图 4.10 所示，演示了 3 种不同 shape 下的张量𝑨、𝑩相加的例子：

简单测试一下基本运算符的自动 Broadcasting 机制，例如：
a = tf.random.normal([2,32,32,1])
b = tf.random.normal([32,32])
a+b,a-b,a*b,a/b # 测试加减乘除运算的 Broadcasting 机制
这些运算都能 Broadcasting 成[2,32,32,32]的公共 shape，再进行运算。熟练掌握并运用Broadcasting 机制可以让代码更简洁，计算效率更高。
"""
