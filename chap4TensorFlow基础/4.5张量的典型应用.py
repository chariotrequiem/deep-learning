# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/14 15:13
"""
在介绍完张量的相关属性和创建方式后，下面将介绍每种维度数下张量的典型应用，
让读者在看到每种张量后，能够直观的联想到它主要的物理意义和用途，对后续张量的维度变换等一系列抽象操作的学习打下基础。
本节在介绍典型应用时不可避免地会体积后续将要学习的网络模型或算法，学习时不需要完全理解，有初步印象即可。

4.5.1标量
在TensorFlow中，标量最容易理解，他就是一个简单的数字，维度数为0，shape为[]。
标量的一些典型用途是误差值的表示、各种测量指标的表示，比如准确度(Accuracy,简称为acc)，精度(Precision)和召回率等(Recall)等。
考虑某个模型的训练曲线，横坐标为训练步数Step，纵坐标分别为Loss per Query Image误差变化趋势和准确度Accuracy变化趋势曲线，
其中损失值和准确度均由张量计算产生，类型为标量，可以直接可视化为曲线图
以均方差误差函数为例，经过tf.keras.losses.mse(或tf.keras.losses.MSE， 两者相同功能)返回每个样本上的误差值，
最后取误差的均值作为当前Batch的误差，它是一个标量。
In [41]:
out = tf.random.uniform([4,10]) #随机模拟网络输出
y = tf.constant([2,3,2,0]) # 随机构造样本真实标签
y = tf.one_hot(y, depth=10) # one-hot 编码
loss = tf.keras.losses.mse(y, out) # 计算每个样本的 MSE
loss = tf.reduce_mean(loss) # 平均 MSE,loss 应是标量
print(loss)
Out[41]:
tf.Tensor(0.19950335, shape=(), dtype=float32)


4.5.2向量
向量是一种非常常见的数据载体，如在全连接层和卷积神经网络中，偏置量b就使用向量来表示。
如图 4.2 所示，每个全连接层的输出节点都添加了一个偏置值，把所有输出节点的偏置表示成向量形式：𝒃 = [𝑏1, 𝑏2]T。
考虑到两个输出节点的网络层，我们创建长度为2的偏置向量b，并累加在每个输出节点上。
In [42]:
# z=wx,模拟获得激活函数的输入 z z = tf.random.normal([4,2])
b = tf.zeros([2]) # 创建偏置向量
z = z + b # 累加上偏置向量
Out[42]:
<tf.Tensor: id=245, shape=(4, 2), dtype=float32, numpy=
array([[ 0.6941646 , 0.4764454 ],
       [-0.34862405, -0.26460952],
       [ 1.5081744 , -0.6493869 ],
       [-0.26224667, -0.78742725]], dtype=float32)>
注意此处shape为[4, 2]的z和shaoe为[2]的b张量可以直接相加，这是为什么呢？我们将在 Broadcasting 一节为大家揭秘

通过高层接口类Dense()方式创建的网络层，张量w和b存储在类的内部，由类自动创建并管理。可以通过全连接层的bias成员变量查看偏置变量，
例如创建输入节点数为4，输出节点数为3的线性层网络，那么它的偏置向量b的长度应该为3，实现如下：
In [43]:
fc = layers.Dense(3) # 创建一层 Wx+b，输出节点为 3 # 通过 build 函数创建 W,b 张量，输入节点为 4
fc.build(input_shape=(2,4))
fc.bias # 查看偏置向量
Out[43]:
<tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>
可以看到，类的偏置成员 bias 为长度为 3 的向量，初始化为全 0，这也是偏置𝒃的默认初始化方案。
同时偏置向量𝒃的类型为 Variable，这是因为𝑾和𝒃都是待优化参数。


4.5.3矩阵
矩阵也是非常常见的张量类型，比如全连接层的批量输入张量X的形状为[b, din]，其中b表示输入样本的个数，即Batch Size，
din表示输入特征的长度。例如特征长度为4，一共包含2个样本的输入可以表示为矩阵：
x = tf.random.normal([2,4]) # 2 个样本，特征长度为 4 的张量
令全连接层的输出节点数为 3，则它的权值张量𝑾的 shape 为[4,3]，我们利用张量𝑿、𝑾和向量𝒃可以直接实现一个网络层，代码如下:
In [44]:
w = tf.ones([4,3]) # 定义 W 张量
b = tf.zeros([3]) # 定义 b 张量
o = x@w+b # X@W+b 运算
Out[44]:
<tf.Tensor: id=291, shape=(2, 3), dtype=float32, numpy=array([[ 2.3506963, 2.3506963, 2.3506963],
                                                            [-1.1724043, -1.1724043, -1.1724043]], dtype=float32)>
其中X和W张量均是矩阵，上述代码实现了一个线性变换的网络层，激活函数为空。
一般地，𝜎(𝑿@𝑾 + 𝒃)网络层称为全连接层，在 TensorFlow 中可以通过 Dense 类直接实现，
特别地，当激活函数𝜎为空时，全连接层也称为线性层。
我们通过Dense类创建输入4个节点，输出3个节点的网络层，并通过全连接层的kernel成员名查看其权值矩阵W：
In [45]:
fc = layers.Dense(3) # 定义全连接层的输出节点为 3
fc.build(input_shape=(2,4)) # 定义全连接层的输入节点为 4
fc.kernel # 查看权值矩阵 W
Out[45]:
<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy = array([[ 0.06468129, -0.5146048 , -0.12036425],
                                                                   [ 0.71618867, -0.01442951, -0.5891943 ],
                                                                   [-0.03011459, 0.578704 , 0.7245046 ],
                                                                   [ 0.73894167, -0.21171576, 0.4820758 ]], dtype=float32)>


4.5.4三维张量
三维张量的一个典型应用就是表示序列信号，它的格式是：
                        𝑿 = [𝑏, sequence len, feature len]
其中𝑏表示序列信号的数量，sequence len 表示序列信号在时间维度上的采样点数或步数， feature len 表示每个点的特征长度。

考虑自然语言处理(Natural Language Processing，简称 NLP)中句子的表示，如评价句子的是否为正面情绪的情感分类任务网络，
如图 4.3 所示。为了能够方便字符串被神经网络处理，一般将单词通过嵌入层(Embedding Layer)编码为固定长度的向量，
比如“a”编码为某个长度 3 的向量，那么 2 个等长(单词数量为 5)的句子序列可以表示为 shape 为[2,5,3] 的 3 维张量，
其中 2 表示句子个数，5 表示单词数量，3 表示单词向量的长度。我们通过IMDB 数据集来演示如何表示句子，代码如下：

In [46]: # 自动加载 IMDB 电影评价数据集
(x_train,y_train),(x_test,y_test)=keras.datasets.imdb.load_data(num_words=10000) # 将句子填充、截断为等长 80 个单词的句子
x_train = keras.preprocessing.sequence.pad_sequences(x_train,maxlen=80)
x_train.shape
Out [46]: (25000, 80)
可以看到 x_train 张量的 shape 为[25000,80]，其中 25000 表示句子个数，80 表示每个句子共 80 个单词，每个单词使用数字编码方式表示。

我们通过 layers.Embedding 层将数字编码的单词转换为长度为 100 个词向量：
In [47]: # 创建词向量 Embedding 层类
embedding=layers.Embedding(10000, 100) # 将数字编码的单词转换为词向量
out = embedding(x_train)
out.shape
Out[47]: TensorShape([25000, 80, 100])
可以看到，经过 Embedding 层编码后，句子张量的 shape 变为[25000,80,100]，其中 100 表示每个单词编码为长度是 100 的向量。

对于特征长度为 1 的序列信号，比如商品价格在 60 天内的变化曲线，只需要一个标量即可表示商品的价格，
因此 2 件商品的价格变化趋势可以使用 shape 为[2,60]的张量表示。为了方便统一格式，也将价格变化趋势表达为 shape 为 [2,60,1]的张量，
其中的 1 表示特征长度为 1。


4.5.5四维张量
这里只讨论三、四维张量，大于四维的张量一般应用的比较少，如在元学习中(Meta Learning)中会采用五维的张量表示方法，
理解方法与三、四维张量类似，不再赘述。、

四维张量在卷积神经网络中应用的非常广泛，它用于保存特征图(Feature maps)数据，格式一般定义为
                                        [b, h, w, c]
其中b表示输入样本的数量，h/w分别表示特征图的高/宽，c表示特征图的通道数，部分深度学习框架也会使用[b, c, h, w]格式的特征图张量，
例如PyTorch。图片数据是特征图的一种，对于含有RGB3个通道的彩色图片，每张包含了h行w列像素点，每个点需要3个数值表示RGB通道的颜色强度
因此一张图片可以表示为[h, w, 3]。
神经网络一般并行计算多个输入以提高计算效率，故b张图片的张量可表示为[b, h, w, 3]，例如：
In [48]:
# 创建 32x32 的彩色图片输入，个数为 4 x = tf.random.normal([4,32,32,3])
# 创建卷积神经网络
layer = layers.Conv2D(16,kernel_size=3)
out = layer(x) # 前向计算
out.shape # 输出大小
Out[48]: TensorShape([4, 30, 30, 16])
其中卷积核张量也是 4 维张量，可以通过 kernel 成员变量访问：
In [49]: layer.kernel.shape # 访问卷积核张量
Out[49]: TensorShape([3, 3, 3, 16])
"""