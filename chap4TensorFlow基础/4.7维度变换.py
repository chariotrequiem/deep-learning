# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/14 17:53
"""
在神经网络中，维度变换是最核心的张量操作，通过维度变换可以将数据任意地切换形式，满足不同场合的运算需求。
那么为什么需要维度变换呢？考虑线性层的批量形式：
              𝒀 = 𝑿@𝑾 + 𝒃
其中，假设X包含了2个样本，每个样本的特征长度是4，X的shape为[2, 4]。线性层的输出为3个结点，即W的shape定义为[4, 3],
偏置b的shape定义为[3]。那么𝑿@𝑾的运算结果张量shape为[2, 3]，需要叠加上shape为[3]的偏置b。不同shape的；两个张量则么能直接相加呢？

回顾设计偏置的初衷，我们给每个层的每个输出节点添加一个偏置，这个偏置数据是对所有的样本都是共享的，换言之，
每个样本都应该累加上同样的偏置向量b，
因此，对于 2 个样本的输入𝑿，我们需要将 shape 为[3]的偏置
𝒃 = [𝑏1
     𝑏2
     𝑏3]
按样本数量复制 1 份，变成如下矩阵形式𝑩′：
                            𝑩′ = [𝑏1 𝑏2 𝑏3
                                    𝑏1 𝑏2 𝑏3 ]
通过与𝐗′ = 𝐗@𝐖
𝐗′ = [X11'   X12'   X13'
       X21'   X22'   X23']
相加，此时X′与𝐵′shape 相同，满足矩阵相加的数学条件：
𝐘 = 𝐗′ + 𝐁′ = [X11'   X12'   X13'    +    [b1,  b2,   b3
                  X21'   X22'   X23']         b1,  b2,   b3]

通过这种方式，既满足了数学上矩阵相加要求shape一致的条件，又达到了给每个输入样本的输出节点共享偏置向量的逻辑。
为了实现这种运算方式，我们将偏置向量b插入一个新的维度，并把它定义为Batch维度，然后在Batch维度将数据复制一份，
得到变换后的B'，新的shape为[2, 3]。这一系列的操作就是维度变换操作

算法的每个模块对于数据张量的格式有不同的逻辑需求，当现有的数据格式不满足算法要求时，需要通过维度变换将数据调整为正确的格式。
这就是维度变换的功能。

基本的维度变换操作函数包含了改变视图reshape、插入新维度expand_dims，删除维度squeeze、交换维度transpose、复制数据tile等函数。


4.7.1改变视图
在介绍视图reshape操作之前，先来认识一下张量的存储(Storage)和视图(View)的概念。张量的视图就是我们理解张量的方式，
比如shape为[2,4,4,3]的张量A，我们从逻辑上可以理解为2张图片，每张图片4行4列，每个位置有RGB3个通道的数据；
张量的存储体现在张量在内存上保存为一段连续的内存区域，对于同样的存储，我们可以有不同的理解方式，
比如上述张量A，我们可以在不改变张量的存储下，将张量A理解为2个样本，每个样本的特征为长度48的向量。
同一个存储从不同的角度观察数据，可以产生不同的视图，这就是存储与视图的关系。视图的产生是非常灵活的，但需要保证是合理。

我们通过tf.range()模拟生成一个向量数据，并通过tf.reshape视图改变函数产生不同的视图，例如：
In [67]: x=tf.range(96) # 生成向量
x=tf.reshape(x,[2,4,4,3]) # 改变 x 的视图，获得 4D 张量，存储并未改变
Out[67]: # 可以观察到数据仍然是 0~95 的顺序，可见数据并未改变，改变的是数据的结构
<tf.Tensor: id=11, shape=(2, 4, 4, 3), dtype=int32, numpy=
array([[[[ 0, 1, 2],
         [ 3, 4, 5],
         [ 6, 7, 8],
         [ 9, 10, 11]],…

在存储数据时，内存并不支持这个维度层级概念，只能以平铺方式按序写入内存，因此这种层级关系需要人为管理，
也就是说，每个张量的存储数据需要人为跟踪。为了方便表达，我们把张量shape列表相对靠左侧的维度叫做大维度，
shape列表中相对靠右的维度叫做小维度，比如[2,4,4,3]的张量中，图片数量维数与通道数量维数相比，图片数量叫做大维度，
通道数叫做小维度。在优先写入小维度的设定下，上述张量的内存布局为
                    1 2 3 4 5 6 7 8 9 … … … 93 94 95

数据在创建时按着初始的维度顺序写入，改变张量的视图仅仅是改变了张量的理解方式，并不需要改变张量的存储顺序，
这在一定程度上是从计算效率考虑的，大量数据的写入操作会消耗较多的计算资源。由于存储时数据只有平坦结构，
与数据的逻辑结构是分离的，因此如果新的逻辑结构不需要改变数据的存储方式，就可以节省大量计算资源，这也是改变视图操作的优势。
改变视图操作在提供便捷性的同时，也会带来很多逻辑隐患，这主要的原因是改变视图操作的默认前提是存储不需要改变，
否则改变视图操作就是非法的。我们先介绍合法的视图变换操作，再介绍不合法的视图变换。

例如，张量A按着初始视图[b, h, w, c]写入的内存布局，我们改变A的理解方式，他可以有如下多种合法的理解方式：
❑ [𝑏, ℎ ∙w , 𝑐] 张量理解为𝑏张图片，ℎ ∙ 个像素点，𝑐个通道
❑ [𝑏, ℎ, w∙ 𝑐] 张量理解为𝑏张图片，ℎ行，每行的特征长度为 w∙ 𝑐
❑ [𝑏, ℎ ∙ w ∙ 𝑐] 张量理解为𝑏张图片，每张图片的特征长度为ℎ ∙ w ∙ 𝑐
上述新视图的存储都不需要改变，因此是合法的。
从语法上讲，视图变换只需要满足新视图的元素总量与存储区域大小相等即可，即新视图的元素数量等于：
                                     𝑏 ∙ ℎ ∙ w∙ c
正是由于视图的设计的语法约束很少，完全由用户定义，使得在改变视图时容易出现逻辑隐患。

现在来考虑不合法的视图变换。例如，如果定义新视图为[𝑏, w, ℎ, 𝑐]，[𝑏, 𝑐, ℎ ∗ w]或者[𝑏, 𝑐, ℎ, w]等时，
张量的存储顺序需要改变，如果不同步更新张量的存储顺序，那么恢复出的数据将与新视图不一致，从而导致数据错乱。
这需要用户理解数据，才能判断操作是否合法。我们会在“交换维度”一节介绍如何改变张量的存储。

一种正确使用视图变换操作的技巧就是跟踪存储的维度顺序。
例如根据“图片数量-行 -列-通道”初始视图保存的张量，存储也是按照“图片数量-行-列-通道”的顺序写入的。
如果按着“图片数量-像素-通道”的方式恢复视图，并没有与“图片数量-行-列-通道”相悖，因此能得到合法的数据。
但是如果按着“图片数量-通道-像素”的方式恢复数据，由于内存布局是按着“图片数量-行-列-通道”的顺序，
视图维度顺序与存储维度顺序相悖，提取的数据将是错乱的。

改变视图是神经网络中非常常见的操作，可以通过串联多个 reshape 操作来实现复杂逻辑，但是在通过 reshape 改变视图时，
必须始终记住张量的存储顺序，新视图的维度顺序不能与存储顺序相悖，否则需要通过交换维度操作将存储顺序同步过来。
举个例子，对 于 shape 为[4,32,32,3]的图片数据，通过 reshape 操作将 shape 调整为[4,1024,3]，此时视图的维度顺序为𝑏 − pixel − 𝑐，
张量的存储顺序为[𝑏, ℎ, w , 𝑐]。可以将[4,1024,3]恢复为
❑ [𝑏, ℎ, w , 𝑐] = [4,32,32,3]时，新视图的维度顺序与存储顺序无冲突，可以恢复出无逻辑问题的数据。
❑ [𝑏, w , ℎ, 𝑐] = [4,32,32,3]时，新视图的维度顺序与存储顺序冲突。
❑ [ℎ ∙ w ∙ 𝑐, 𝑏] = [3072,4]时，新视图的维度顺序与存储顺序冲突。

在 TensorFlow 中，可以通过张量的 ndim 和 shape 成员属性获得张量的维度数和形状：
In [68]: x.ndim,x.shape # 获取张量的维度数和形状列表
Out[68]:(4, TensorShape([2, 4, 4, 3]))

通过 tf.reshape(x, new_shape)，可以将张量的视图任意地合法改变，例如：
In [69]: tf.reshape(x,[2,-1])
Out[69]:<tf.Tensor: id=520, shape=(2, 48), dtype=int32, numpy=
array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31,…
 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]])>
其中的参数−1表示当前轴上长度需要根据张量总元素不变的法则自动推导，从而方便用户书写。比如，上面的−1可以推导为
                          2 ∙ 4 ∙ 4 ∙ 3 /2 = 48
再次改变数据的视图为[2, 4, 12],实现如下：
In [70]: tf.reshape(x,[2,4,12])
Out[70]:<tf.Tensor: id=523, shape=(2, 4, 12), dtype=int32, numpy=
array([[[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],…
 [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]],
 [[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], …
 [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]]])>

再次改变数据的视图为[2, 16, 3]，实现如下;
In [71]: tf.reshape(x,[2,-1,3])
Out[71]:<tf.Tensor: id=526, shape=(2, 16, 3), dtype=int32, numpy=
array([[[ 0, 1, 2], …
        [45, 46, 47]],
      [[48, 49, 50],…
       [93, 94, 95]]])>

通过上述的一系列连续变换视图操作时需要意识到，张量的存储顺序始终没有改变，数据在内存中仍然是按着初始写入的数据0，1，2，...，95保存的。




4.7.2增、删维度
增加维度
增加一个长度为 1 的维度相当于给原有的数据添加一个新维度的概念，维度长度为 1，故数据并不需要改变，仅仅是改变数据的理解方式，
因此它其实可以理解为改变视图的一种特殊方式。
考虑一个具体例子，一张28 × 28大小的灰度图片的数据保存为 shape 为[28,28]的张量，在末尾给张量增加一新维度，定义为通道数维度，
此时张量的 shape 变为[28,28,1]，实现如下：
In [72]: # 产生矩阵
x = tf.random.uniform([28,28],maxval=10,dtype=tf.int32)
Out[72]:
<tf.Tensor: id=11, shape=(28, 28), dtype=int32, numpy=
array([[6, 2, 0, 0, 6, 7, 3, 3, 6, 2, 6, 2, 9, 3, 0, 3, 2, 8, 1, 3, 6, 2,3, 9, 3, 6, 1, 7],…

通过 tf.expand_dims(x, axis)可在指定的 axis 轴前可以插入一个新的维度：
In [73]: x = tf.expand_dims(x,axis=2) # axis=2 表示宽维度后面的一个维度
Out[73]:
<tf.Tensor: id=13, shape=(28, 28, 1), dtype=int32, numpy=
array([[[6],
 [2],
 [0],
 [0],
 [6],
 [7],
 [3],…
可以看到，插入一个新维度后，数据的存储顺序并没有改变，依然按着6,2,0,0,6,7,⋯的顺序保存，仅仅是在插入一个新的维度后，改变了数据的视图。

同样的方法，我们可以在最前面插入一个新的维度，并命名为图片数量维度，长度为1，此时张量的shape变为[1,28,28,1]，实现如下：
In [74]: x = tf.expand_dims(x,axis=0) # 高维度之前插入新维度
Out[74]:
<tf.Tensor: id=15, shape=(1, 28, 28, 1), dtype=int32, numpy=
array([[[[6],
         [2],
         [0],
         [0],
         [6],
         [7],
         [3],…

需要注意的是，tf.expand_dims 的 axis 为正时，表示在当前维度之前插入一个新维度；为负时，表示当前维度之后插入一个新的维度。
以[𝑏, ℎ, , 𝑐]张量为例，不同 axis 参数的实际插入位置如图 4.6 所示：
                                    0  1  2  3  4
                                    [b, c, h, w]
                                   -5 -4 -3 -2 -1


删除维度
删除维度是增加维度的逆操作，与增加维度一样，删除维度只能增加长度为1的维度，也不会改变张量的存储。
继续考虑增加维度后 shape 为[1,28,28,1]的例子，如果希望将图片数量维度删除，可以通过 tf.squeeze(x, axis)函数，
axis 参数为待删除的维度的索引号，例如，图片数量的维度轴 axis=0：
In [75]: x = tf.squeeze(x, axis=0) # 删除图片数量维度
Out[75]:
<tf.Tensor: id=586, shape=(28, 28, 1), dtype=int32, numpy=
array([[[8],
 [2],
 [2],
 [0],…
继续删除通道数维度，由于已经删除了图片数量维度，此时的 x 的 shape 为[28,28,1]，因此删除通道数维度时指定 axis=2，实现如下：
In [76]: x = tf.squeeze(x, axis=2) # 删除图片通道数维度
Out[76]:
<tf.Tensor: id=588, shape=(28, 28), dtype=int32, numpy=
array([[8, 2, 2, 0, 7, 0, 1, 4, 9, 1, 7, 4, 8, 2, 7, 4, 8, 2, 9, 8, 8, 0,9, 9, 7, 5, 9, 7],
       [3, 4, 9, 9, 0, 6, 5, 7, 1, 9, 9, 1, 2, 7, 2, 7, 5, 3, 3, 7, 2, 4, 5, 2, 7, 3, 8, 0],…

如果不指定维度参数axis，即tf.squeeze(x)，那么他会默认删除所有长度为1的维度，例如：
In [77]:
x = tf.random.uniform([1,28,28,1],maxval=10,dtype=tf.int32)
tf.squeeze(x) # 删除所有长度为 1 的维度
Out[77]:
<tf.Tensor: id=594, shape=(28, 28), dtype=int32, numpy=
array([[9, 1, 4, 6, 4, 9, 0, 0, 1, 4, 0, 8, 5, 2, 5, 0, 0, 8, 9, 4, 5, 0,1, 1, 4, 3, 9, 9],…

建议使用 tf.squeeze()时逐一指定需要删除的维度参数，防止 TensorFlow 意外删除某些长度为 1 的维度，导致计算结果不合法。



4.7.3交换维度
改变视图、增删维度都不会影响张量的存储。在实现算法逻辑时，在保持维度顺序不变的条件下，仅仅改变张量的理解方式是不够的，
有时需要直接调整的存储顺序，即交换维度(Transpose)。通过交换维度操作，改变了张量的存储顺序，同时也改变了张量的视图。

交换维度操作是非常常见的，比如在 TensorFlow 中，图片张量的默认存储格式是通道后行格式：[𝑏, ℎ, w , 𝑐]，
但是部分库的图片格式是通道先行格式：[𝑏, 𝑐, ℎ, w]，因此需要完成[𝑏, ℎ, w , 𝑐]到[𝑏, 𝑐, ℎ, w]维度交换运算，
此时若简单的使用改变视图函数 reshape，则新视图的存储方式需要改变，因此使用改变视图函数是不合法的。
我们以[𝑏, ℎ, w, 𝑐]转换到[𝑏, 𝑐, ℎ, w]为例，介绍如何使用 tf.transpose(x, perm)函数完成维度交换操作，其中参数 perm
表示新维度的顺序 List。考虑图片张量 shape 为[2,32,32,3]，“图片数量、行、列、通道数”的维度索引分别为 0、1、2、3，
如果需要交换为[𝑏, 𝑐, ℎ, w]格式，则新维度的排序为“图片数量、通道数、行、列”，对应的索引号为[0,3,1,2]，
因此参数 perm 需设置为[0,3,1,2]，实现如下：
In [78]: x = tf.random.normal([2,32,32,3])
tf.transpose(x,perm=[0,3,1,2]) # 交换维度
Out[78]:
<tf.Tensor: id=603, shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[-1.93072677e+00, -4.80163872e-01, -8.85614634e-01, ...,
 1.49124235e-01, 1.16427064e+00, -1.47740364e+00],
 [-1.94761145e+00, 7.26879001e-01, -4.41877693e-01, ...

如果希望将[b, h, w, c]交换为[b, w, h, c]，即将高、宽维度互换，则新维度索引为[0, 2, 1, 3]，实现如下：
In [79]:
x = tf.random.normal([2,32,32,3])
tf.transpose(x,perm=[0,2,1,3]) # 交换维度
Out[79]:
<tf.Tensor: id=612, shape=(2, 32, 32, 3), dtype=float32, numpy=
array([[[[ 2.1266546 , -0.64206547, 0.01311932],
 [ 0.918484 , 0.9528751 , 1.1346699 ],
 ...,

需要注意的是，通过 tf.transpose 完成维度交换后，张量的存储顺序已经改变，视图也随之改变，后续的所有操作必须基于新的存续顺序和视图进行。
相对于改变视图操作，维度交换操作的计算代价更高。



4.7.4复制数据
当通过增加维度操作插入新维度后，可能希望在新的维度上面复制若干份数据，满足后续格式的算法要求。
考虑𝒀 = 𝑿@𝑾 + 𝒃的例子，偏置𝒃插入样本数的新维度后，需要在新维度上复制 Batch Size 份数据，将shape变为与𝑿@𝑾一致后，才能完成张量相加运算。
可以通过 tf.tile(x, multiples)函数完成数据在指定维度上的复制操作，multiples 分别指定了每个维度上面的复制倍数，
对应位置为 1 表明不复制，为 2 表明新长度为原来长度的2 倍，即数据复制一份，以此类推。

以输入为[2,4]，输出为 3 个节点线性变换层为例，偏置𝒃定义为：
                        𝒃 = [𝑏1
                             𝑏2
                             𝑏3]
通过 tf.expand_dims(b, axis=0)插入新维度，变成矩阵：
                         𝑩 = [𝑏1 𝑏2 𝑏3]
此时𝑩的 shape 变为[1,3]，我们需要在 axis=0 图片数量维度上根据输入样本的数量复制若干次，这里的 Batch Size 为 2，即复制一份，变成：
                             𝑩 = [𝑏1 𝑏2 𝑏3
                                  𝑏1 𝑏2 𝑏3]
通过 tf.tile(b, multiples=[2,1])即可在 axis=0 维度复制 1 次，在 axis=1 维度不复制。首先插入新的维度，实现如下：
In [80]:
b = tf.constant([1,2]) # 创建向量 b
b = tf.expand_dims(b, axis=0) # 插入新维度，变成矩阵
b
Out[80]:
<tf.Tensor: id=645, shape=(1, 2), dtype=int32, numpy=array([[1, 2]])>
在 Batch 维度上复制数据 1 份，实现如下：
In [81]: b = tf.tile(b, multiples=[2,1]) # 样本维度上复制一份
Out[81]:
<tf.Tensor: id=648, shape=(2, 2), dtype=int32, numpy=
array([[1, 2], [1, 2]])>
此时𝑩的 shape 变为[2,3]，可以直接与𝑿@𝑾进行相加运算。实际上，上述插入维度和复制数据的步骤并不需要我们手动执行，
TensorFlow 会自动完成，这就是自动扩展功能。

考虑另一个例子，输入 x 为 2 行 2 列的矩阵，实现如下：
In [82]: x = tf.range(4)
x=tf.reshape(x,[2,2]) # 创建 2 行 2 列矩阵
Out[82]:
<tf.Tensor: id=655, shape=(2, 2), dtype=int32, numpy= array([[0, 1], [2, 3]])>
首先在列维度复制1份数据
In [83]: x = tf.tile(x,multiples=[1,2]) # 列维度复制一份
Out[83]:
<tf.Tensor: id=658, shape=(2, 4), dtype=int32, numpy=array([[0, 1, 0, 1],
                                                            [2, 3, 2, 3]])>
然后在行维度复制1份数据，实现如下;
In [84]: x = tf.tile(x,multiples=[2,1]) # 行维度复制一份
Out[84]:
<tf.Tensor: id=672, shape=(4, 4), dtype=int32, numpy=array([[0, 1, 0, 1],
                                                           [2, 3, 2, 3],
                                                           [0, 1, 0, 1],
                                                          [2, 3, 2, 3]])>
经过 2 个维度上的复制运算后,可以看到数据的变化过程，shape也变为原来的2倍。这个例子比较直观地帮助我们理解数据复制的过程。
tf.tile 会创建一个新的张量来保存复制后的张量，由于复制操作涉及大量数据的读写 IO 运算，计算代价相对较高。
神经网络中不同 shape 之间的张量运算操作十分频繁，那么有没有轻量级的复制操作呢？这就是接下来要介绍的 Broadcasting 操作。

"""
import tensorflow as tf
x = tf.random.normal([2, 32, 32, 3])
tf.transpose(x, perm=[0, 3, 1, 2])
y = tf.transpose(x, perm=[0, 2, 1, 3])
print(y)


