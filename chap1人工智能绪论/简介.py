# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/13 10:47
"""
1.1.2机器学习
机器学习可以分为有监督学习(Supervised Learning)、无监督学习(Unsupervised Learning)和强化学习(Reinforcement Learning，RL)
  有监督学习
有监督学习的数据集包含了样本𝒙与样本的标签𝒚，算法模型需要学习到映射关系𝑓𝜃: 𝒙 → 𝒚，其中𝑓𝜃代表模型函数，𝜃为模型的参数。
在训练时，通过计算模型的预测值𝑓𝜃(𝒙)与真实标签𝒚之间的误差来优化网络参数𝜃，使得网络下一次能够预测更精准。
常见的有监督学习有线性回归、逻辑回归、支持向量机、随机森林等。

  无监督学习
收集带标签的数据往往代价较为昂贵，对于只有样本𝒙的数据集，算法需要自行发现数据的模态，这种方式叫作无监督学习。
无监督学习中有一类算法将自身作为监督信号，即模型需要学习的映射为𝑓𝜃: 𝒙 → 𝒙，称为自监督学习(Self-supervised Learning)。
在训练时，通过计算模型的预测值𝑓𝜃(𝒙)与自身𝒙之间的误差来优化网络参数𝜃。
常见的无监督学习算法有自编码器、生成对抗网络等。

  强化学习
也称为增强学习，通过与环境进行交互来学习解决问题的策略的一类算法。
与有监督、无监督学习不同，强化学习问题并没有明确的“正确的”动作监督信号，算法需要与环境进行交互，
获取环境反馈的滞后的奖励信号，因此并不能通过计算动作与“正确动作”之间的误差来优化网络。
常见的强化学习算法有 DQN，PPO 等。

1.1.3神经网络与深度学习
  神经网络算法是一类基于神经网络从数据中学习的算法，它仍然属于机器学习的范畴。
受限于计算能力和数据量，早期的神经网络层数较浅，一般在 1~4 层左右，网络表达能力有限。
随着计算能力的提升和大数据时代的到来，高度并行化的 GPU 和海量数据让大规模神经网络的训练成为可能。

一般将利用深层神经网络实现的算法称作深度学习，本质上神经网络和深度学习可认为是相同的。
简单来比较一下深度学习算法与其它算法的特点。
传统的机器学习算法一般会人为设计具有一定通用性的特征检测方法，如 SIFT、HOG 特征，这些特征能够适合某一类的任务，
具有一定的通用性，但是如何设计特征方法，以及特征方法的优劣性是问题的关键。

神经网络的出现，使得人为设计特征这一部分工作可以通过神经网络让机器自动学习完成，不需要人类干预。
但是浅层的神经网络的特征提取能力较为有限，而深层的神经网络擅长提取高层、抽象的特征，因此具有更好的性能表现。

1.2神经网络发展史
我们将神经网络的发展历程大致分为浅层神经网络阶段和深度学习阶段，以2006年为分割点。
2006年以前，深度学习以神经网络和连接主义名义发展，历经了两次兴盛和两次寒冬；
2006年，Geoffrey Hinton 首次将深层神经网络命名为深度学习，自此开启了深度学习的第三次复兴之路。

1.3深度学习特点
与传统的机器学习算法、浅层神经网络相比，现代的深度学习算法通常具有如下特点。
1.3.1 数据量
早期的机器学习算法比较简单，容易快速训练，需要的数据集规模也比较小，如 1936年由英国统计学家Ronald Fisher
收集整理的鸢尾花卉数据集Iris共包含3个类别花卉，每个类别50个样本。随着计算机技术的发展，设计的算法越来越复杂，
对数据量的需求也随之增大。1998年由 Yann LeCun收集整理的MNIST手写数字图片数据集共包含0~9共10类数字，
每个类别多达7000张图片。随着神经网络的兴起，尤其是深度学习，网络层数一般较深，模型的参数量可达百万、千万甚至十亿个，
为了防止过拟合，需要的数据集的规模通常也是巨大的。现代社交媒体的流行也让收集海量数据成为可能，
如2010年发布的ImageNet数据集收录了共14197122张图片，整个数据集的压缩文件大小就有154GB。
尽管深度学习对数据集需求较高，收集数据，尤其是收集带标签的数据，往往是代价昂贵的。
数据集的形成通常需要手动采集、爬取原始数据，并清洗掉无效样本，再通过人类智能去标注数据样本，
因此不可避免地引入主观偏差和随机误差。研究数据量需求较少的算法模型是非常有用的一个方向。
1.3.2 计算力
计算能力的提升是第三次人工智能复兴的一个重要因素。实际上，现代深度学习的基础理论在1980年代就已经被提出，
但直到2012年，基于两块GTX580 GPU训练的AlexNet发布后，深度学习的真正潜力才得以发挥。传统的机器学习算法并不像神经网络
这样对数据量和计算能力有严苛的要求，通常在CPU上串行训练即可得到满意结果。但是深度学习非常依赖并行加速计算设备，
目前的大部分神经网络均使用 NVIDIA GPU 和 Google TPU 等并行加速芯片训练模型参数。如围棋程序 AlphaGo Zero在64块GPU上从
零开始训练了40天才得以超越所有的AlphaGo历史版本；自动网络结构搜索算法使用了800块GPU 同时训练才能优化出较好的网络结构。
1.3.3 网络规模
早期的感知机模型和多层神经网络层数只有1层或者2~4层，网络参数量也在数万左右。
随着深度学习的兴起和计算能力的提升，AlexNet(8层)、VGG16(16层)、 GoogLeNet(22层)、ResNet50(50层)、DenseNet121(121层)等模
型相继被提出，同时输入图片的大小也从28×28逐渐增大，变成224×224、299×299等，这些变化使得网络的总参数量可达到千万级别。

网络规模的增大，使得神经网络的容量也相应增大，从而能够学习到复杂的数据模态，模型的性能也会随之提升；
另一方面，网络规模的增大，意味着更容易出现过拟合现象，训练需要的数据集和计算代价也会变大。

1.3.4 通用智能
过去，为了提升某项任务上的算法性能，往往需要利用先验知识手动设计相应的特征，以帮助算法更好地收敛到最优解。
这类特征提取方法往往是与具体任务场景强相关的，一旦场景发生了变动，这些依靠人工设计的特征和先验设定无法自适应新场景，
往往需要重新设计算法模型，模型的通用性不强。设计一种像人脑一样可以自动学习、自我调整的通用智能机制一直是人类的共同愿
景。从目前来看，深度学习是最接近通用智能的算法之一。在计算机视觉领域，过去需要针对具体的任务设计特征、添加先验假设的做法，
已经被深度学习算法抛弃了，目前在图片识别、目标检测、语义分割等方向，几乎都是基于深度学习端到端地训练，获得的模型性能好，
适应性强；在 Atria 游戏平台上，DeepMind 设计的 DQN 算法模型可以在相同的算法、模型结构和超参数的设定下，
在 49 个游戏上获得人类相当的游戏水平，呈现出一定程度的通用智能。

1.4深度学习应用
深度学习算法已经广泛应用到人们生活的角角落落，例如手机中的语音助手、汽车上的智能辅助驾驶、人脸支付等。
我们将从计算机视觉、自然语言处理和强化学习3个领域入手，为大家介绍深度学习的一些主流应用。

1.4.1 计算机视觉
图片识别(Image Classification)是常见的分类问题。神经网络的输入为图片数据，输出值为当前样本属于每个类别的概率分布。
通常选取概率值最大的类别作为样本的预测类别。图片识别是最早成功应用深度学习的任务之一，经典的网络模型有VGG系列、
Inception系列、ResNet系列等。
目标检测(Object Detection)是指通过算法自动检测出图片中常见物体的大致位置，通常用边界框(Bounding box)表示，
并分类出边界框中物体的类别信息。常见的目标检测算法有 RCNN、Fast RCNN、Faster RCNN、Mask RCNN、SSD、YOLO系列等。
语义分割(Semantic Segmentation) 是通过算法自动分割并识别出图片中的内容，可以将语义分割理解为每个像素点的分类问题，
分析每个像素点的物体的类别信息。常见的语义分割模型有 FCN、U-net、SegNet、DeepLab 系列等。
视频理解(Video Understanding) 随着深度学习在 2D 图片的相关任务上取得较好的效果，
具有时间维度信息的3D视频理解任务受到越来越多的关注。常见的视频理解任务有视频分类、行为检测、视频主体抽取等。
常用的模型有 C3D、TSN、DOVF、TS_LSTM等。
图片生成(Image Generation)通过学习真实图片的分布，并从学习到的分布中采样而获得逼真度较高的生成图片。目前常见的生成模型有
VAE系列、GAN系列等。其中GAN系列算法近年来取得了巨大的进展，最新GAN模型产生的图片效果达到了肉眼难辨真伪的程度。
除了上述应用，深度学习还在其它方向上取得了不俗的效果，比如艺术风格迁移、超分辨率、图片去燥/去雾、灰度图片着色等一系列
非常实用酷炫的任务，限于篇幅，不再赘述。

1.4.2 自然语言处理
机器翻译(Machine Translation) 过去的机器翻译算法通常是基于统计机器翻译模型，这也是2016年前Google翻译系统采用的技术。
2016 年 11 月，Google基于Seq2Seq模型上线了 Google 神经机器翻译系统(GNMT)，首次实现了源语言到目标语言的直译技术，
在多项任务上获得了50~90%的效果提升。常用的机器翻译模型有Seq2Seq、BERT、GPT、GPT-2 等，其中OpenAI提出的GPT-2模型参数量
高达 15 亿个，甚至发布之初以技术安全考虑为由拒绝开源 GPT-2 模型。
聊天机器人(Chatbot) 聊天机器人也是自然语言处理的一项主流任务，机器自动学习与人类对话，对于人类的简单诉求提供满意
的自动回复，提高客户的服务效率和服务质量等。常应用在咨询系统、娱乐系统、智能家居等中。

1.4.3 强化学习
虚拟游戏相对于真实环境，虚拟游戏平台既可以训练、测试强化学习算法，又可以避免无关因素干扰，同时也能将实验代价降到最低。
目前常用的虚拟游戏平台有 OpenAI Gym、OpenAI Universe、OpenAI Roboschool、DeepMind OpenSpiel、MuJoCo 等，常用的强化学习
算法有 DQN、A3C、A2C、PPO 等。在围棋领域，DeepMind AlaphGo 程序已经超越人类围棋专家；在Dota2和星际争霸游戏上，
OpenAI 和 DeepMind 开发的智能程序也在限制规则下战胜了职业队伍。
机器人(Robotics) 在真实环境中，机器人的控制也取得了一定的进展。如 UC Berkeley实验室在机器人领域的
Imitation Learning、Meta Learning、Few-shot Learning 等方向上取得了不少进展。美国波士顿动力公司在机器人应用中取得喜人的
成就，其制造的机器人在复杂地形行走、多智能体协作等任务上表现良好。
自动驾驶(Autonomous Driving) 被认为是强化学习短期内能技术落地的一个应用方向，很多公司投入大量资源在自动驾驶上，
如百度、Uber、Google 无人车等，其中百度的无人巴士“阿波龙”已经在北京、雄安、武汉等地展开试运营。

1.5深度学习框架
工欲善其事，必先利其器。在了解了深度学习相关知识后，我们来挑选一下实现深度学习算法所使用的工具吧。
1.5.1 主流框架
❑ Theano 是最早的深度学习框架之一，由 Yoshua Bengio 和 Ian Goodfellow 等人开发，是一个基于 Python 语言、定位底层运算的
计算库，Theano 同时支持 GPU 和 CPU 运算。由于Theano开发效率较低，模型编译时间较长，同时开发人员转投TensorFlow等原因，
Theano 目前已经停止维护。
❑ Scikit-learn 是一个完整的面向机器学习算法的计算库，内建了常见的传统机器学习算法支持，文档和案例也较为丰富，
但是 Scikit-learn 并不是专门面向神经网络而设计的，不支持 GPU 加速，对神经网络相关层的实现也较欠缺。
❑ Caffe 由华人贾扬清在2013 年开发，主要面向使用卷积神经网络的应用场合，并不适合其它类型的神经网络的应用。
Caffe 的主要开发语言是 C++，也提供Python语言等接口，支持 GPU 和 CPU。由于开发时间较早，在业界的知名度较高，2017 年
Facebook 推出了 Caffe 的升级版本Cafffe2，Caffe2目前已经融入到PyTorch库中。
❑ Torch 是一个非常优秀的科学计算库，基于较冷门的编程语言Lua开发。Torch灵活性较高，容易实现自定义网络层，
这也是 PyTorch继承获得的优良基因。但是由于 Lua语言使用人群较少，Torch一直未能获得主流应用。
❑ MXNet由华人陈天奇和李沐等人开发，是亚马逊公司的官方深度学习框架。采用了命令式编程和符号式编程混合方式，灵活性高，
运行速度快，文档和案例也较为丰富。
❑ PyTorch 是Facebook基于原Torch框架推出的采用Python作为主要开发语言的深度学习框架。PyTorch借鉴了Chainer的设计风格，
采用命令式编程，使得搭建网络和调试网络非常方便。尽管 PyTorch在2017年才发布，但是由于精良紧凑的接口设计，
PyTorch 在学术界获得了广泛好评。在 PyTorch 1.0 版本后，原来的 PyTorch 与 Caffe2进行了合并，弥补了 PyTorch在工业部署
方面的不足。总的来说，PyTorch是一个非常优秀的深度学习框架。
❑ Keras 是一个基于 Theano 和 TensorFlow 等框架提供的底层运算而实现的高层框架，提供了大量快速训练、测试网络的高层接口。
对于常见应用来说，使用 Keras 开发效率非常高。但是由于没有底层实现，需要对底层框架进行抽象，运行效率不高，灵活性一般。
❑ TensorFlow 是 Google 于 2015 年发布的深度学习框架，最初版本只支持符号式编程。得益于发布时间较早，以及Google在
深度学习领域的影响力，TensorFlow很快成为最流行的深度学习框架。但是由于TensorFlow接口设计频繁变动，功能设计重复冗余，
符号式编程开发和调试非常困难等问题，TensorFlow 1.x 版本一度被业界诟病。2019年，Google 推出 TensorFlow 2 正式版本，
将以动态图优先模式运行，从而能够避免TensorFlow 1.x 版本的诸多缺陷，已获得业界的广泛认可。

目前来看，TensorFlow 和 PyTorch 框架是业界使用最为广泛的两个深度学习框架，TensorFlow 在工业界拥有完备的解决方案和用户基础，
PyTorch 得益于其精简灵活的接口设计，可以快速搭建和调试网络模型，在学术界获得好评如潮。TensorFlow 2 发布后，弥补了
TensorFlow 在上手难度方面的不足，使得用户既能轻松上手 TensorFlow 框架，又能无缝部署网络模型至工业系统。
本书以 TensorFlow 2.0 版本作为主要框架，实现深度学习算法。

这里特别介绍 TensorFlow 与 Keras 之间的联系与区别。Keras 可以理解为一套高层 API的设计规范，Keras本身对这套规范有官方的实现，
在 TensorFlow 中也实现了这套规范，称为 tf.keras 模块，并且 tf.keras 将作为 TensorFlow 2 版本的唯一高层接口，避免出现接口
重复冗余的问题。如无特别说明，本书中 Keras 均指代 tf.keras。

1.5.2 TensorFlow 2 与 1.x
TensorFlow 2 是一个与 TensorFlow 1.x 使用体验完全不同的框架，TensorFlow 2 不兼容TensorFlow 1.x 的代码，
同时在编程风格、函数接口设计等上也大相径庭，TensorFlow 1.x的代码需要依赖人工的方式迁移，自动化迁移方式并不靠谱。
Google 即将停止更新TensorFlow 1.x，不建议学习 TensorFlow 1.x 版本。
TensorFlow 2 支持动态图优先模式，在计算时可以同时获得计算图与数值结果，可以代码中调试并实时打印数据，搭建网络也像搭积木
一样，层层堆叠，非常符合软件开发思维。

可以看到，计算过程非常简洁，没有多余的计算步骤。
这种运算时同时创建计算图𝑐 = 𝑎 + 𝑏和数值结果6.0 = 2.0 + 4.0的方式叫做命令式编程，也称为动态图模式。
TensorFlow 2 和 PyTorch 都是采用动态图(优先)模式开发，调试方便，所见即所得。一般来说，动态图模式开发效率高，
但是运行效率可能不如静态图模式。TensorFlow 2 也支持通过 tf.function 将动态图优先模式的代码转化为静态图模式，实现开发和
运行效率的双赢。
"""
import timeit

import tensorflow as tf

"""# 创建输出张量，并赋初始值
a = tf.constant(2.)
b = tf.constant(4.)
# 直接计算，并打印程序
print('a+b=', a+b)"""

"""
1.5.3功能演示
深度学习的核心是算法的设计思想，深度学习框架只是我们实现算法的工具。下面我们将演示TensorFlow深度学习框架的三大核心功能，
从而帮助我们理解框架在算法设计中扮演的角色。
a) 加速计算
神经网络本质上由大量的矩阵相乘、矩阵相加等基本数学运算构成，TensorFlow 的重要功能就是利用GPU方便地实现并行计算加速功能。
为了演示 GPU 的加速效果，我们通过完成多次矩阵𝑨和矩阵𝑩的矩阵相乘运算，并测量其平均运算时间来比对。其中矩阵𝑨的
shape 为[1, 𝑛]，矩阵𝑩的 shape 为[𝑛, 1]，通过调节𝑛即可控制矩阵的大小。
首先我们分别创建使用 CPU 和 GPU 环境运算的 2 个矩阵，代码如下：
"""
# 创建在CPU环境上运算的2个矩阵
with tf.device('/cpu:0'):
    cpu_a = tf.random.normal([1, 10000000])
    cpu_b = tf.random.normal([10000000, 1])
    print(cpu_a.device, cpu_b.device)

# 创建使用GPU环境运算的2个矩阵
with tf.device('/gpu:0'):
    gpu_a = tf.random.normal([1, 10000000])
    gpu_b = tf.random.normal([10000000, 1])
    print(gpu_a.device, gpu_b.device)

"""接下来实现 CPU 和 GPU 运算的函数，并通过 timeit.timeit()函数来测量两个函数的运算时间。
需要注意的是，第一次计算时一般需要完成额外的环境初始化工作，因此这段时间不能计算在内。
我们通过热身环节将这段时间去除，再测量运算时间，代码如下："""


def cpu_run():  # cpu运算函数
    with tf.device('/cpu:0'):
        c = tf.matmul(cpu_a, cpu_b)
    return c


def gpu_run():  # GPU 运算函数
    with tf.device('/gpu:0'):
        c = tf.matmul(gpu_a, gpu_b)
    return c


# 第一次计算需要热身，避免将初始化时间结算在内
cpu_time = timeit.timeit(cpu_run, number=10)
gpu_time = timeit.timeit(gpu_run, number=10)
print('warmup:', cpu_time, gpu_time)
# 正式计算 10 次，取平均时间
cpu_time = timeit.timeit(cpu_run, number=10)
gpu_time = timeit.timeit(gpu_run, number=10)
print('run time:', cpu_time, gpu_time)
"""
b) 自动梯度
在使用 TensorFlow构建前向计算过程的时候，除了能够获得数值结果，TensorFlow还会自动构建计算图，
通过 TensorFlow提供的自动求导的功能，可以不需要手动推导，即可计算输出对网络参数的偏导数。
考虑如下函数的表达式：y = 𝑎𝑤^2 + 𝑏𝑤 + 𝑐
输出对于变量𝑤的导数关系为：dy/d𝑤 = 2𝑎𝑤 + 𝑏
考虑在(𝑎, 𝑏, 𝑐, 𝑤) = (1,2,3,4)处的导数，代入上式可得d𝑦/d𝑤 = 2 ∙ 1 ∙ 4 + 2 = 10。
我们通过手动推导的方式计算出导数值为10。
通过TensorFlow的方式，我们可以不需要手动推导导数的表达式，直接给出函数的表达式，即可由TensorFlow自动求导，代码实现如下："""
# 创建4个张量， 并赋值
a = tf.constant(1.)
b = tf.constant(2.)
c = tf.constant(3.)
w = tf.constant(4.)

with tf.GradientTape() as tape:  # 构建梯度环境
    tape.watch([w])  # 将w加入梯度跟踪列表
    # 构建计算过程，函数表达式
    y = a * w ** 2 + b * w + c
# 自动求导
[dy_dw] = tape.gradient(y, [w])
print(dy_dw)  # 打印出导数

"""
c)常用神经网络接口
TensorFlow除了提供底层的矩阵相乘、相加等数学函数，还内建了常用神经网络运算函数、常用网络层、网络训练、模型保存与加载、
网络部署等一系列深度学习系统的便捷功能。使用TensorFlow开发，可以方便地利用这些功能完成常用业务流程，高效稳定。
"""


