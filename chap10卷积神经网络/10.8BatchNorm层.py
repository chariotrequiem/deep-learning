# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/23 9:53
"""
卷积神经网络的出现，网络参数量大大减低，使得几十层的深层网络成为可能。然而，在残差网络出现之前，网络的加深使得网络训练变得非常不稳定，
甚至出现网络长时间不更新甚至不收敛的现象，同时网络对超参数比较敏感，超参数的微量扰动也会导致网络的训练轨迹完全改变。

2015 年，Google 研究人员 Sergey Ioffe 等提出了一种参数标准化(Normalize)的手段，并基于参数标准化设计了 Batch Nomalization(简写为 BatchNorm，或 BN)层 [6]。
BN 层的提出，使得网络的超参数的设定更加自由，比如更大的学习率、更随意的网络初始化等，同时网络的收敛速度更快，性能也更好。
BN 层提出后便广泛地应用在各种深度网络模型上，卷积层、BN 层、ReLU 层、池化层一度成为网络模型的标配单元块，
通过堆叠 Conv-BN-ReLU-Pooling 方式往往可以获得不错的模型性能。

首先我们来探索，为什么需要对网络中的数据进行标准化操作？这个问题很难从理论层面解释透彻，即使是 BN 层的作者给出的解释也未必让所有人信服。
与其纠结其缘由，不如通过具体问题来感受数据标准化后的好处。

考虑 Sigmoid 激活函数和它的梯度分布，如下图 10.39 所示，Sigmoid 函数在𝑥 ∈ [−2 2 ]区间的导数值在 [0.1, 0.25]区间分布；
当𝑥 > 2 或𝑥 < −2时，Sigmoid 函数的导数变得很小，逼近于 0，从而容易出现梯度弥散现象。
为了避免因为输入较大或者较小而导致Sigmoid 函数出现梯度弥散现象，将函数输入𝑥标准化映射到 0 附近的一段较小区间将变得非常重要，
可以从图 10.39 看到，通过标准化重映射后，值被映射在 0 附近，此处的导数值不至于过小，从而不容易出现梯度弥散现象。
这是使用标准化手段受益的一个例子。

我们再看另一个例子。考虑 2 个输入节点的线性模型，如图 10.40(a)所示：
                            ℒ = 𝑎 = 𝑥1𝑤1 + 𝑥2𝑤2 + b
讨论如下 2 种输入分布下的优化问题：
❑ 输入𝑥1 ∈ [1, 10]，𝑥2 ∈ [1, 10]
❑ 输入𝑥1 ∈ [1, 10]，𝑥2 ∈ [100, 1000]
由于模型相对简单，可以绘制出 2 种𝑥1、𝑥2下，函数的损失等高线图，图 10.40(b)是𝑥1 ∈ [1, 10]、𝑥2 ∈ [100, 1000]时的某条优化轨迹线示意，
图 10.40(c)是𝑥1 ∈ [1, 10]、𝑥2 ∈ [1, 10]时的某条优化轨迹线示意，图中的圆环中心即为全局极值点.

考虑到
                    𝜕ℒ / 𝜕𝑤1 = 𝑥1
                    𝜕ℒ / 𝜕𝑤2 = 𝑥2
当𝑥1、𝑥2输入分布相近时，𝜕ℒ/𝜕𝑤1、𝜕ℒ/𝜕𝑤2偏导数值相当，函数的优化轨迹如图 10.40(c)所示；
当𝑥1、𝑥2输入分布差距较大时，比如𝑥1 ≪ 𝑥2，则
                            𝜕ℒ / 𝜕𝑤1 ≪ 𝜕ℒ / 𝜕𝑤2
损失函数等势线在𝑤2轴更加陡峭，某条可能的优化轨迹如图 10.40(b)所示。对比 2 条优化轨迹线可以观察到，
𝑥1、𝑥2分布相近时图 10.40(c)中收敛更加快速，优化轨迹更理想。

通过上述的 2 个例子，我们能够经验性归纳出：网络层输入𝑥分布相近，并且分布在较小范围内(如0附近)，更有利于函数的优化。
那么如何保证输入x的分布相近呢？数据标准化可以实现此目的，通过数据标准化操作可以将数据数据𝑥映射到𝑥̂:
                                𝑥̂ = 𝑥 − 𝜇𝑟 / √𝜎𝑟2 + 𝜖
其中𝜇𝑟、𝜎𝑟2来自统计的所有数据的均值和方差，𝜖是为防止出现除 0 错误而设置的较小数字，如 1e − 8。

在基于 Batch 的训练阶段，如何获取每个网络层所有输入的统计数据𝜇𝑟、𝜎𝑟2呢？考虑Batch 内部的均值𝜇𝐵和方差𝜎𝐵2：
                                    𝜇𝐵 = 1 / 𝑚∑𝑥𝑖
                                𝜎𝐵2 = 1/𝑚∑(𝑥𝑖 − 𝜇𝐵)2
可以视为近似于𝜇𝑟、𝜎𝑟2，其中𝑚为 Batch 样本数。因此，在训练阶段，通过
                            𝑥̂train = 𝑥train − 𝜇𝐵 /√(𝜎𝐵2 + 𝜖)
标准化输入，并记录每个 Batch 的统计数据𝜇𝐵、𝜎𝐵2，用于统计真实的全局𝜇𝑟、𝜎𝑟2

在测试阶段，根据记录的每个 Batch 的𝜇𝐵、𝜎𝐵2估计出所有训练数据的𝜇𝑟、𝜎𝑟2，按着
                                 𝑥̂test = 𝑥test − 𝜇r /√(𝜎r2 + 𝜖)
将每层的输入标准化。

上述的标准化运算并没有引入额外的待优化变量，𝜇𝑟、𝜎𝑟2和𝜇𝐵、𝜎𝐵2均由统计得到，不需要参与梯度更新。
实际上，为了提高BN层的表达能力，BN 层作者引入了“scale and shift”技巧，将𝑥̂变量再次映射变换：
                                  𝑥̃ = 𝑥̂ ∙ 𝛾 + 𝛽
其中𝛾参数实现对标准化后的𝑥̂再次进行缩放，𝛽参数实现对标准化的𝑥̂进行平移，不同的是，𝛾、𝛽参数均由反向传播算法自动优化，
实现网络层“按需”缩放平移数据的分布的目的。
下面我们来学习在 TensorFlow 中实现的 BN 层的方法。


10.8.1前向传播
我们将 BN 层的输入记为𝑥，输出记为𝑥̃。分训练阶段和测试阶段来讨论前向传播过程。
训练阶段：首先计算当前 Batch 的𝜇𝐵、𝜎𝐵2，根据
                        𝑥̃train = (𝑥train − 𝜇𝐵) √(𝜎𝐵2 + 𝜖) ∙ 𝛾 + 𝛽
计算BN层的输出。
同时按照
                        𝜇𝑟 ← momentum ∙ 𝜇𝑟 + (1 − momentum) ∙ 𝜇𝐵
                        𝜎𝑟2 ← momentum ∙ 𝜎𝑟2 + (1 − momentum) ∙ 𝜎𝐵2
迭代更新全局训练数据的统计值𝜇𝑟和𝜎𝑟2，其中 momentum 是需要设置一个超参数，用于平衡𝜇𝑟、𝜎𝑟2的更新幅度：
当momentum = 0时，𝜇𝑟和𝜎𝑟2直接被设置为最新一个 Batch 的𝜇𝐵 和𝜎𝐵2；
当momentum = 1时，𝜇𝑟和𝜎𝑟2保持不变，忽略最新一个 Batch 的𝜇𝐵和𝜎𝐵2，
在TensorFlow 中，momentum 默认设置为 0.99。

测试阶段：BN 层根据
                        𝑥̃test = (𝑥test − 𝜇𝑟 )√(𝜎𝑟2 + 𝜖) ∗ 𝛾 + 𝛽
计算输出𝑥̃𝑡𝑒𝑠𝑡，其中𝜇𝑟、𝜎𝑟2、𝛾、𝛽均来自训练阶段统计或优化的结果，在测试阶段直接使用，并不会更新这些参数。



10.8.2反向更新
在训练模式下的反向更新阶段，反向传播算法根据损失ℒ求解梯度𝜕ℒ/𝜕𝛾和𝜕ℒ/𝜕𝛽，并按着梯度更新法则自动优化𝛾、𝛽参数。

需要注意的是，对于 2D 特征图输入𝑿: [b ℎ 𝑤 𝑐] ，BN 层并不是计算每个点的𝜇𝐵、 𝜎𝐵2，而是在通道轴𝑐上面统计每个通道上面所有数据的𝜇𝐵、𝜎𝐵2，
因此𝜇𝐵、𝜎𝐵2是每个通道上所有其它维度的均值和方差。以 shape 为 [100 32 32 3 ]的输入为例，在通道轴𝑐上面的均值计算如下：
# 构造输入
x=tf.random.normal([100,32,32,3])
# 将其他维度合并，仅保留通道维度
x=tf.reshape(x,[-1,3])
# 计算其他维度的均值
ub=tf.reduce_mean(x,axis=0)
ub
Out[7]: # 通道维度的均值
<tf.Tensor: id=62, shape=(3,), dtype=float32, numpy=array([-0.00222636, -
0.00049868, -0.00180082], dtype=float32)>
数据有𝑐个通道数，则有𝑐个均值产生。

除了在𝑐轴上面统计数据𝜇𝐵、𝜎𝐵2的方式，我们也很容易将其推广至其它维度计算均值的方式，如图 10.41 所示：
❑ Layer Norm：统计每个样本的所有特征的均值和方差
❑ Instance Norm：统计每个样本的每个通道上特征的均值和方差
❑ Group Norm：将𝑐通道分成若干组，统计每个样本的通道组内的特征均值和方差

上面提到的 Normalization 方法均由独立的几篇论文提出，并在某些应用上验证了其相当或者优于 BatchNorm 算法的效果。
由此可见，深度学习算法研究并非难于上青天，只要多思考、多锻炼算法工程能力，人人都有机会发表创新性成果。



10.8.3BN层实现
在 TensorFlow 中，通过 layers.BatchNormalization()类可以非常方便地实现 BN 层：
# 创建 BN 层
layer=layers.BatchNormalization()
与全连接层、卷积层不同，BN 层的训练阶段和测试阶段的行为不同，需要通过设置training 标志位来区分训练模式还是测试模式。

以 LeNet-5 的网络模型为例，在卷积层后添加 BN 层，代码如下：
network = Sequential([ # 网络容器
layers.Conv2D(6,kernel_size=3,strides=1),
# 插入 BN 层
layers.BatchNormalization(),
layers.MaxPooling2D(pool_size=2,strides=2),
layers.ReLU(),
layers.Conv2D(16,kernel_size=3,strides=1),
# 插入 BN 层
layers.BatchNormalization(),
layers.MaxPooling2D(pool_size=2,strides=2),
layers.ReLU(),
layers.Flatten(),
layers.Dense(120, activation='relu'),
# 此处也可以插入 BN 层
layers.Dense(84, activation='relu'),
# 此处也可以插入 BN 层
layers.Dense(10)])

在训练阶段，需要设置网络的参数 training=True 以区分 BN 层是训练还是测试模型，代码如下：
with tf.GradientTape() as tape:
    # 插入通道维度
    x = tf.expand_dims(x,axis=3)
    # 前向计算，设置计算模式，[b, 784] => [b, 10]
    out = network(x, training=True)
在测试阶段，需要设置 training=False，避免 BN 层采用错误的行为，代码如下：
for x,y in db_test: # 遍历测试集
    # 插入通道维度
    x = tf.expand_dims(x,axis=3) # 前向计算，测试模式
    out = network(x, training=False)
"""