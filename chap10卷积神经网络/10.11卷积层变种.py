# 当前版本 ： python3.7.11
# 开发时间 ： 2021/9/23 17:13
"""
卷积神经网络的研究产生了各种各样优秀的网络模型，还提出了各种卷积层的变种，本节将重点介绍数种典型的卷积层变种。

10.11.1空洞卷积
普通的卷积层为了减少网络的参数量，卷积核的设计通常选择较小的 × 和3 × 3感受野大小。小卷积核使得网络提取特征时的感受野区域有限，
但是增大感受野的区域又会增加网络的参数量和计算代价，因此需要权衡设计。

空洞卷积(Dilated/Atrous Convolution)的提出较好地解决这个问题，空洞卷积在普通卷积的感受野上增加一个 Dilation Rate 参数，
用于控制感受野区域的采样步长，如下图10.51 所示：当感受野的采样步长 Dilation Rate 为 1 时，每个感受野采样点之间的距离为1，
此时的空洞卷积退化为普通的卷积；当 Dilation Rate 为 2 时，感受野每 2 个单元采样一个点，如图 10.51 中间的绿色方框中绿色格子所示，
每个采样格子之间的距离为 2；同样的方法，图 10.51 右边的 Dilation Rate 为 3，采样步长为 3。尽管 Dilation Rate 的增大会使得感受野区域增大，
但是实际参与运算的点数仍然保持不变。

以输入为单通道的 7× 7张量，单个3 × 3卷积核为例，如下图 10.52 所示。在初始位置，感受野从最上、最右位置开始采样，
每隔一个点采样一次，共采集 9 个数据点，如图10.52 中绿色方框所示。这 9 个数据点与卷积核相乘运算，写入输出张量的对应位置。

卷积核窗口按着步长为𝑠 = 向右移动一个单位，如图 10.53 所示，同样进行隔点采样，共采样 9 个数据点，与卷积核完成相乘累加运算，
写入输出张量对应位置，直至卷积核移动至最下方、最右边位置。需要注意区分的是，卷积核窗口的移动步长𝑠和感受野区域的采样步长 Dilation Rate 是不同的概念。

空洞卷积在不增加网络参数的条件下，提供了更大的感受野窗口。但是在使用空洞卷积设置网络模型时，需要精心设计 Dilation Rate 参数来避免出现网格效应，
同时较大的Dilation Rate 参数并不利于小物体的检测、语义分割等任务。

在 TensorFlow 中，可以通过设置 layers.Conv2D()类的 dilation_rate 参数来选择使用普通卷积还是空洞卷积。例如：

In [8]:
x = tf.random.normal([1,7,7,1]) # 模拟输入
# 空洞卷积，1 个 3x3 的卷积核
layer = layers.Conv2D(1,kernel_size=3,strides=1,dilation_rate=2)
out = layer(x) # 前向计算
out.shape
Out[8]: TensorShape([1, 3, 3, 1])
当 dilation_rate 参数设置为默认值 1 时，使用普通卷积方式进行运算；当 dilation_rate 参数大于 1 时，采样空洞卷积方式进行计算。


10.11.2转置卷积
转置卷积(Transposed Convolution，或 Fractionally Strided Convolution，部分资料也称之为反卷积/Deconvolution，
实际上反卷积在数学上定义为卷积的逆过程，但转置卷积并不能恢复出原卷积的输入，因此称为反卷积并不妥当)通过在输入之间填充大量的 padding 来
实现输出高宽大于输入高宽的效果，从而实现向上采样的目的，如图 10.54 所示。我们先介绍转置卷积的计算过程，再介绍转置卷积与普通卷积的联系。

为了简化讨论，我们此处只讨论输入ℎ = 𝑤，即输入高宽相等的情况。

𝒐 + 𝟐𝒑 − 𝒌为𝒔倍数
考虑输入为2 × 2的单通道特征图，转置卷积核为3 × 3大小，步长𝑠 = 2，填充𝑝 =0的例子。
首先在输入数据点之间均匀插入𝑠 − 1个空白数据点，得到3 × 3的矩阵，如图 10.55第 2 个矩阵所示，
根据填充量在3 × 3矩阵周围填充相应𝑘 − 𝑝 − 1= 3 − 0 − 1= 2行/列，此时输入张量的高宽为 7× 7，如图 10.55 中第 3 个矩阵所示。

在7 × 7的输入张量上，进行3 × 3卷积核，步长𝑠′ = 1 ，填充𝑝 = 0的普通卷积运算(注意，此阶段的普通卷积的步长𝑠′始终为 1，与转置卷积的步长𝑠不同)，
根据普通卷积的输出计算公式，得到输出大小为： 𝑜 = [𝑖 + 2 ∗𝑠′𝑝 − 𝑘] / s' + 1 = [7 + 2 * 0 - 3 ] / 1 + 1 = 5
5 × 5大小的输出。我们直接按照此计算流程给出最终转置卷积输出与输入关系。在𝑜 + 2𝑝 − 𝑘为 s 倍数时，满足关系
                            𝑜 = (𝑖 − 1)𝑠 + 𝑘 − 2𝑝

转置卷积并不是普通卷积的逆过程，但是二者之间有一定的联系，同时转置卷积也是基于普通卷积实现的。在相同的设定下，
输入𝒙经过普通卷积运算后得到𝒐 = Conv(𝒙)，我们将𝒐送入转置卷积运算后，得到𝒙′ = ConvTranspose(𝒐)，其中𝒙′ ≠ 𝒙，
但是𝒙′与𝒙形状相同。我们可以用输入为 5× 5，步长𝑠 = 2，填充𝑝 = 0，3 × 3卷积核的普通卷积运算进行验证演示，如下图 10.56 所示。

可以看到，将转置卷积的输出 × 在同设定条件下送入普通卷积，可以得到2 × 2的输出，此大小恰好就是转置卷积的输入大小，
同时我们也观察到，输出的2 × 2矩阵并不是转置卷积输入的2 × 2矩阵。转置卷积与普通卷积并不是互为逆过程，不能恢复出对方的输入内容，
仅能恢复出等大小的张量。因此称之为反卷积并不贴切。

基于 TensorFlow 实现上述例子的转置卷积运算，代码如下：
In [8]:
# 创建 X 矩阵，高宽为 5x5
x = tf.range(25)+1 # Reshape 为合法维度的张量
x = tf.reshape(x,[1,5,5,1])
x = tf.cast(x, tf.float32)
# 创建固定内容的卷积核矩阵
w = tf.constant([[-1,2,-3.],[4,-5,6],[-7,8,-9]])
# 调整为合法维度的张量
w = tf.expand_dims(w,axis=2)
w = tf.expand_dims(w,axis=3) # 进行普通卷积运算
out = tf.nn.conv2d(x,w,strides=2,padding='VALID')
out
Out[9]: # 输出的高宽为 2x2
<tf.Tensor: id=14, shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[ -67.],
        [ -77.]],
        [[-117.],
        [-127.]]]], dtype=float32)>

现在我们将普通卷积的输出作为转置卷积的输入，验证转置卷积的输出是否为 5 × 5，代码如下:
In [10]:
# 普通卷积的输出作为转置卷积的输入，进行转置卷积运算
xx = tf.nn.conv2d_transpose(out, w, strides=2,
padding='VALID',
output_shape=[1,5,5,1])
Out[10]: # 输出的高宽为 5x5
<tf.Tensor: id=117, shape=(5, 5), dtype=float32, numpy=
array([[ 67., -134., 278., -154., 231.],
[ -268., 335., -710., 385., -462.],
[ 586., -770., 1620., -870., 1074.],
[ -468., 585., -1210., 635., -762.],
[ 819., -936., 1942., -1016., 1143.]], dtype=float32)>

可以看到，转置卷积能够恢复出同大小的普通卷积的输入，但转置卷积的输出并不等同于普通卷积的输入。


𝒐 + 𝟐𝒑 − 𝒌不为𝒔倍数
让我们更加深入地分析卷积运算中输入与输出大小关系的一个细节。考虑卷积运算的输出表达式：
                        𝑜 = ⌊(𝑖 + 2 ∗𝑝 − 𝑘) / s⌋ + 1
当步长s > 时，⌊(𝑖 + 2 ∗𝑝 − 𝑘) / s⌋ 向下取整运算使得出现多种不同输入尺寸𝑖对应到相同的输出尺寸𝑜上。举个例子，
考虑输入大小为6 × 6，卷积核大小为3 × 3，步长为 1 的卷积运算，代码如下：
In [11]:
x = tf.random.normal([1,6,6,1])
# 6x6 的输入经过普通卷积
out = tf.nn.conv2d(x,w,strides=2,padding='VALID')
out.shape
x = tf.random.normal([1,6,6,1])...
Out[12]: # 输出的高宽同样为 2x2，与输入为 5x5 时一样
<tf.Tensor: id=21, shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[ 20.438847 ],
[ 19.160788 ]],
[[ 0.8098897],
[-28.30303 ]]]], dtype=float32)>

此种情况也能获得2 × 2大小的卷积输出，与图 10.56 中可以获得相同大小的输出。因此，不同输入大小的卷积运算可能获得相同大小的输出。
考虑到卷积与转置卷积输入输出大小关系互换，从转置卷积的角度来说，输入尺寸𝑖经过转置卷积运算后，可能获得不同的输出𝑜大小。
因此通过在图 10.55 中填充𝑎行、𝑎列来实现不同大小的输出𝑜，从而恢复普通卷积不同大小的输入的情况，其中𝑎关系为：
                                    𝑎 = (𝑜 + 2𝑝 − 𝑘)%𝑠
此时转置卷积的输出变为：
                                𝑜 = (𝑖 − 1)𝑠 + 𝑘 − 2𝑝 + 𝑎
在 TensorFlow 中间，不需要手动指定𝑎参数，只需要指定输出尺寸即可，TensorFlow会自动推导需要填充的行列数𝑎，前提是输出尺寸合法。
例如：
In [13]:
# 恢复出 6x6 大小
xx = tf.nn.conv2d_transpose(out, w, strides=2,
padding='VALID',
output_shape=[1,6,6,1])
xx
Out[13]:
<tf.Tensor: id=23, shape=(1, 6, 6, 1), dtype=float32, numpy=
array([[[[ -20.438847 ],
[ 40.877693 ],
[ -80.477325 ],
[ 38.321575 ],
[ -57.48236 ],
[ 0. ]],...
通过改变参数 output_shape=[1,5,5,1]也可以获得高宽为 × 的张量。



矩阵角度
转置卷积的转置是指卷积核矩阵𝑾产生的稀疏矩阵𝑾′在计算过程中需要先转置𝑾′𝐓， 再进行矩阵相乘运算，而普通卷积并没有转置𝑾′的步骤。
这也是它被称为转置卷积的名字由来。

考虑普通 Conv2d 运算：𝑿和𝑾，需要根据 strides 将卷积核在行、列方向循环移动获取参与运算的感受野的数据，
串行计算每个窗口处的“相乘累加”值，计算效率极低。为了加速运算，在数学上可以将卷积核𝑾根据 strides 重排成稀疏矩阵𝑾′，
再通过𝑾′@𝑿′一次完成运算(实际上，𝑾′矩阵过于稀疏，导致很多无用的 0 乘运算，很多深度学习框架也不是通过这种方式实现的)。

以 4 行 4 列的输入𝑿，高宽为3 × 3，步长为 1，无 padding 的卷积核𝑾的卷积运算为 例，首先将𝑿打平成𝑿′，如图 10.57 所示。

然后将卷积核𝑾转换成稀疏矩阵𝑾′，如图 10.58 所示。
此时通过一次矩阵相乘即可实现普通卷积运算：
                                𝑶′ = 𝑾′@𝑿′
如果给定𝑶，希望能够生成与𝑿同形状大小的张量，怎么实现呢？将𝑾′转置后与图 10.57方法重排后的𝑶′完成矩阵相乘即可：
                                𝑿′ = 𝑾′T@𝑶′
得到的𝑿′通过 Reshape 操作变为与原来的输入𝑿尺寸一致，但是内容不同。例如𝑶′的 shape 为 [4, 1]，𝑾′T的 shape [16, 4]为 ，
矩阵相乘得到𝑿′的 shape 为 [16, 1]，Reshape 后即可产生[4, 4]形状的张量。由于转置卷积在矩阵运算时，
需要将𝑾′转置后才能与转置卷积的输入𝑶′矩阵相乘，故称为转置卷积。

转置卷积具有“放大特征图”的功能，在生成对抗网络、语义分割等中得到了广泛应用，如 DCGAN [12]中的生成器通过堆叠转置卷积层实现逐层“放大”特征图，
最后获得十分逼真的生成图片。


转置卷积实现
在 TensorFlow 中，可以通过 nn.conv2d_transpose 实现转置卷积运算
nn.conv2d 完成普通卷积运算。注意转置卷积的卷积核的定义格式为 [𝑘 𝑘 cout cin]
例如：
In [14]:
# 创建 4x4 大小的输入
x = tf.range(16)+1
x = tf.reshape(x,[1,4,4,1])
x = tf.cast(x, tf.float32)
# 创建 3x3 卷积核
w = tf.constant([[-1,2,-3.],[4,-5,6],[-7,8,-9]])
w = tf.expand_dims(w,axis=2)
w = tf.expand_dims(w,axis=3) # 普通卷积运算
out = tf.nn.conv2d(x,w,strides=1,padding='VALID')
Out[14]:
<tf.Tensor: id=42, shape=(2, 2), dtype=float32, numpy=
array([[-56., -61.],
 [-76., -81.]], dtype=float32)>
在保持 strides=1，padding=’VALID’，卷积核不变的情况下，我们通过卷积核 w 与输出 out 的转置卷积运算尝试恢复与输入 x 相同大小的高宽张量，
代码如下：
In [15]: # 恢复 4x4 大小的输入
xx = tf.nn.conv2d_transpose(out, w, strides=1, padding='VALID',
output_shape=[1,4,4,1])
tf.squeeze(xx)
Out[15]:
<tf.Tensor: id=44, shape=(4, 4), dtype=float32, numpy=
array([[ 56., -51., 46., 183.],
 [-148., -35., 35., -123.],
 [ 88., 35., -35., 63.],
 [ 532., -41., 36., 729.]], dtype=float32)>
可以看到，转置卷积生成了 × 的特征图，但特征图的数据与输入 x 并不相同。
在使用 tf.nn.conv2d_transpose 进行转置卷积运算时，需要额外手动设置输出的高宽。
tf.nn.conv2d_transpose 并不支持自定义 padding 设置，只能设置为 VALID 或者 SAME。
当设置 padding=’VALID’时，输出大小表达为：
                                    𝑜 = (𝑖 − 1)𝑠 + 𝑘
当设置 padding=’SAME’时，输出大小表达为：
                                        𝑜 = 𝑖 ∙ 𝑠
如果读者对转置卷积的原理细节暂时无法理解，可以牢记上述两个表达式即可。例如，2 × 2的转置卷积输入与3 × 3的卷积核运算，
strides=1，padding=’VALID’时，输出大小为：
                                ℎ′ = 𝑤′ = (2 − 1) ∙ 1 + 3 = 4
2 × 2的转置卷积输入与3 × 3的卷积核运算，strides=3，padding=’SAME’时，输出大小为：
                                    ℎ′ = 𝑤′ = 2 ∙ 3 = 6
转置卷积也可以和其他层一样，通过 layers.Conv2DTranspose 类创建一个转置卷积层，然后调用实例即可完成前向计算
In [16]: # 创建转置卷积类
layer = layers.Conv2DTranspose(1,kernel_size=3,strides=1,padding='VALID')
xx2 = layer(out) # 通过转置卷积层
xx2
Out[16]:
<tf.Tensor: id=130, shape=(1, 4, 4, 1), dtype=float32, numpy=
array([[[[ 9.7032385 ],
[ 5.485071 ],
[ -1.6490463 ],
[ 1.6279562 ]],...



10.11.3分离卷积
这里以深度可分离卷积(Depth-wise Separable Convolution)为例。普通卷积在对多通道输入进行运算时，
卷积核的每个通道与输入的每个通道分别进行卷积运算，得到多通道的特征图，再对应元素相加产生单个卷积核的最终输出，如图 10.60 所示。

分离卷积的计算流程则不同，卷积核的每个通道与输入的每个通道进行卷积运算，得到多个通道的中间特征，如图 10.61 所示。
这个多通道的中间特征张量接下来进行多个1 × 1卷积核的普通卷积运算，得到多个高宽不变的输出，这些输出在通道轴上面进行拼接，
从而产生最终的分离卷积层的输出。可以看到，分离卷积层包含了两步卷积运算，第一步卷积运算是单个卷积核，第二个卷积运算包含了多个卷积核。

那么采用分离卷积有什么优势呢？一个很明显的优势在于，同样的输入和输出，采用Separable Convolution 的参数量约是普通卷积的1/3。
考虑上图中的普通卷积和分离卷积的例子。普通卷积的参数量是
                                3 ∙ 3 ∙ 3 ∙ 4 = 108
分离卷积的第一部分参数量是
                                3 ∙ 3 ∙ 3 ∙ = 27
第二部分参数量是
                                1 ∙ 1 ∙ 3 ∙ 4 = 12
分离卷积的总参数量只有39，但是却能实现普通卷积同样的输入输出尺寸变换。
分离卷积在 Xception 和 MobileNets 等对计算代价敏感的领域中得到了大量应用。
"""